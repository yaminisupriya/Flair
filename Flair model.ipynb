{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dccc5e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import flair, torch\n",
    "flair.device = torch.device('cpu') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d99103a2",
   "metadata": {},
   "source": [
    "The default behavior is that the model gets put on GPU if available and runs on CPU if there is no GPU.\n",
    "The flair.device parameter gets called all over the code to move models and tensor to the device on which flair is run.\n",
    "Change this behavior, for instance to direct it to run on CPU even if you have a GPU available, you need to run this code before instantiating your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "129a6313",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Labels</th>\n",
       "      <th>PII</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Candidate economic character present money dau...</td>\n",
       "      <td>Address</td>\n",
       "      <td>Apt. 026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Film range sound. People age Apt. 476 that.</td>\n",
       "      <td>Address</td>\n",
       "      <td>Apt. 476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Back want myself class certain. Tree pretty ca...</td>\n",
       "      <td>Address</td>\n",
       "      <td>Suite 492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Bring guy 81627 Kimberly Squares Washingtonber...</td>\n",
       "      <td>Address</td>\n",
       "      <td>81627 Kimberly Squares Washingtonberg, RI 13540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>52013 Jason Vista Lake Kathleen, PA 89168 May ...</td>\n",
       "      <td>Address</td>\n",
       "      <td>52013 Jason Vista Lake Kathleen, PA 89168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>Speech national especially available own black...</td>\n",
       "      <td>Address</td>\n",
       "      <td>Apt. 646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>Forward 8278 Torres Branch Apt. 177 Robertsbur...</td>\n",
       "      <td>Address</td>\n",
       "      <td>8278 Torres Branch Apt. 177 Robertsburgh, NJ 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>Test artist person billion. Trouble staff indu...</td>\n",
       "      <td>Address</td>\n",
       "      <td>4058 Gordon Fields South Charlestown, NJ 40537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>Group think step increase answer know. Agreeme...</td>\n",
       "      <td>Address</td>\n",
       "      <td>Apt. 212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>Candidate economic character present money dau...</td>\n",
       "      <td>CreditCardNumber</td>\n",
       "      <td>2254280030993205</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>101 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Text            Labels  \\\n",
       "0    Candidate economic character present money dau...           Address   \n",
       "1          Film range sound. People age Apt. 476 that.           Address   \n",
       "2    Back want myself class certain. Tree pretty ca...           Address   \n",
       "3    Bring guy 81627 Kimberly Squares Washingtonber...           Address   \n",
       "4    52013 Jason Vista Lake Kathleen, PA 89168 May ...           Address   \n",
       "..                                                 ...               ...   \n",
       "96   Speech national especially available own black...           Address   \n",
       "97   Forward 8278 Torres Branch Apt. 177 Robertsbur...           Address   \n",
       "98   Test artist person billion. Trouble staff indu...           Address   \n",
       "99   Group think step increase answer know. Agreeme...           Address   \n",
       "100  Candidate economic character present money dau...  CreditCardNumber   \n",
       "\n",
       "                                                   PII  \n",
       "0                                             Apt. 026  \n",
       "1                                             Apt. 476  \n",
       "2                                            Suite 492  \n",
       "3      81627 Kimberly Squares Washingtonberg, RI 13540  \n",
       "4            52013 Jason Vista Lake Kathleen, PA 89168  \n",
       "..                                                 ...  \n",
       "96                                            Apt. 646  \n",
       "97   8278 Torres Branch Apt. 177 Robertsburgh, NJ 0...  \n",
       "98      4058 Gordon Fields South Charlestown, NJ 40537  \n",
       "99                                            Apt. 212  \n",
       "100                                   2254280030993205  \n",
       "\n",
       "[101 rows x 3 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "train = pd.read_excel('PII_Train_Large_Data_Test_Data.xlsx',sheet_name=1)\n",
    "train.columns = train[0:1].values[-1]\n",
    "train = train.drop(labels=0, axis=0).reset_index(drop=True)\n",
    "train.head(101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1755a47b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Labels</th>\n",
       "      <th>PII</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Candidate economic character present money dau...</td>\n",
       "      <td>Address</td>\n",
       "      <td>Apt. 026</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text   Labels       PII\n",
       "0  Candidate economic character present money dau...  Address  Apt. 026"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "538249f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Flair library requires train data to be in this format.\n",
    "# converting pII column to string. bcz some are integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dc8ae184",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['PII'] = train['PII'].astype('str')\n",
    "train['Labels'] = train['Labels'].apply(str.upper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8fa7eb9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>annotation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Candidate economic character present money dau...</td>\n",
       "      <td>[(Apt. 026, ADDRESS)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Film range sound. People age Apt. 476 that.</td>\n",
       "      <td>[(Apt. 476, ADDRESS)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Back want myself class certain. Tree pretty ca...</td>\n",
       "      <td>[(Suite 492, ADDRESS)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Bring guy 81627 Kimberly Squares Washingtonber...</td>\n",
       "      <td>[(81627 Kimberly Squares Washingtonberg, RI 13...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>52013 Jason Vista Lake Kathleen, PA 89168 May ...</td>\n",
       "      <td>[(52013 Jason Vista Lake Kathleen, PA 89168, A...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>695</th>\n",
       "      <td>Coach he west magazine against 510-81-5182 bea...</td>\n",
       "      <td>[(510-81-5182, SSN)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>696</th>\n",
       "      <td>Speech national especially 471 33 3655 availab...</td>\n",
       "      <td>[(471 33 3655, SSN)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>697</th>\n",
       "      <td>Forward listen step this community financial m...</td>\n",
       "      <td>[(008-52-3159, SSN)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>698</th>\n",
       "      <td>Test artist person billion. Trouble staff indu...</td>\n",
       "      <td>[(818-43-7502, SSN)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>699</th>\n",
       "      <td>Group think step increase 587-05-0700 answer k...</td>\n",
       "      <td>[(587-05-0700, SSN)]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>700 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text  \\\n",
       "0    Candidate economic character present money dau...   \n",
       "1          Film range sound. People age Apt. 476 that.   \n",
       "2    Back want myself class certain. Tree pretty ca...   \n",
       "3    Bring guy 81627 Kimberly Squares Washingtonber...   \n",
       "4    52013 Jason Vista Lake Kathleen, PA 89168 May ...   \n",
       "..                                                 ...   \n",
       "695  Coach he west magazine against 510-81-5182 bea...   \n",
       "696  Speech national especially 471 33 3655 availab...   \n",
       "697  Forward listen step this community financial m...   \n",
       "698  Test artist person billion. Trouble staff indu...   \n",
       "699  Group think step increase 587-05-0700 answer k...   \n",
       "\n",
       "                                            annotation  \n",
       "0                                [(Apt. 026, ADDRESS)]  \n",
       "1                                [(Apt. 476, ADDRESS)]  \n",
       "2                               [(Suite 492, ADDRESS)]  \n",
       "3    [(81627 Kimberly Squares Washingtonberg, RI 13...  \n",
       "4    [(52013 Jason Vista Lake Kathleen, PA 89168, A...  \n",
       "..                                                 ...  \n",
       "695                               [(510-81-5182, SSN)]  \n",
       "696                               [(471 33 3655, SSN)]  \n",
       "697                               [(008-52-3159, SSN)]  \n",
       "698                               [(818-43-7502, SSN)]  \n",
       "699                               [(587-05-0700, SSN)]  \n",
       "\n",
       "[700 rows x 2 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cast Labels and PII to a tuple list by zip\n",
    "annotation = list(zip(train['PII'], train['Labels']))\n",
    "aaa = []\n",
    "for i in range(len(annotation)):\n",
    "    aaa.append([annotation[i]])\n",
    "train['annotation'] = aaa\n",
    "# we do not need Labels and PII columns anymore\n",
    "train = train.drop(columns=['Labels', 'PII'])\n",
    "train = train.loc[:699]\n",
    "train.columns = ['text', 'annotation']\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2b6a351d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Labels</th>\n",
       "      <th>PII</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Candidate economic character present money dau...</td>\n",
       "      <td>Address</td>\n",
       "      <td>Apt. 026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Film range sound. People age Apt. 476 that.</td>\n",
       "      <td>Address</td>\n",
       "      <td>Apt. 476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Back want myself class certain. Tree pretty ca...</td>\n",
       "      <td>Address</td>\n",
       "      <td>Suite 492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Bring guy 81627 Kimberly Squares Washingtonber...</td>\n",
       "      <td>Address</td>\n",
       "      <td>81627 Kimberly Squares Washingtonberg, RI 13540</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text   Labels  \\\n",
       "0  Candidate economic character present money dau...  Address   \n",
       "1        Film range sound. People age Apt. 476 that.  Address   \n",
       "2  Back want myself class certain. Tree pretty ca...  Address   \n",
       "3  Bring guy 81627 Kimberly Squares Washingtonber...  Address   \n",
       "\n",
       "                                               PII  \n",
       "0                                         Apt. 026  \n",
       "1                                         Apt. 476  \n",
       "2                                        Suite 492  \n",
       "3  81627 Kimberly Squares Washingtonberg, RI 13540  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "5ef46090",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>annotation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>680</th>\n",
       "      <td>Talk while American suddenly 587-05-0700 parti...</td>\n",
       "      <td>[(587-05-0700, SSN)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>681</th>\n",
       "      <td>Camera wall continue top us clearly hot includ...</td>\n",
       "      <td>[(896-11-9761, SSN)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>682</th>\n",
       "      <td>Course college still loss scene. Series 530 12...</td>\n",
       "      <td>[(530 12 8752, SSN)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>683</th>\n",
       "      <td>243 53 6315 Movement near value number able ab...</td>\n",
       "      <td>[(243 53 6315, SSN)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>684</th>\n",
       "      <td>Pull key other nor 166-43-2545 save perform. L...</td>\n",
       "      <td>[(166-43-2545, SSN)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>685</th>\n",
       "      <td>Ready off score foot market protect. 829 49 9139</td>\n",
       "      <td>[(829 49 9139, SSN)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>686</th>\n",
       "      <td>Up course education involve thousand. Treat sc...</td>\n",
       "      <td>[(211 72 2423, SSN)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>687</th>\n",
       "      <td>Huge develop environmental president their. Ma...</td>\n",
       "      <td>[(423 72 5067, SSN)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>688</th>\n",
       "      <td>Possible both develop 595-23-4635 claim. Call ...</td>\n",
       "      <td>[(595-23-4635, SSN)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>689</th>\n",
       "      <td>Describe question cover suggest 344 81 3132 ac...</td>\n",
       "      <td>[(344 81 3132, SSN)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>690</th>\n",
       "      <td>Per learn agree despite smile him foot. Wait s...</td>\n",
       "      <td>[(877-42-6395, SSN)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>691</th>\n",
       "      <td>766-65-9797 Director thousand manager give boa...</td>\n",
       "      <td>[(766-65-9797, SSN)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>692</th>\n",
       "      <td>Report author 319-69-3108 increase a receive w...</td>\n",
       "      <td>[(319-69-3108, SSN)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>693</th>\n",
       "      <td>Positive cause I arrive night site wind. Throu...</td>\n",
       "      <td>[(319-69-3108, SSN)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>694</th>\n",
       "      <td>Store effort campaign girl worker technology. ...</td>\n",
       "      <td>[(377 98 7558, SSN)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>695</th>\n",
       "      <td>Coach he west magazine against 510-81-5182 bea...</td>\n",
       "      <td>[(510-81-5182, SSN)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>696</th>\n",
       "      <td>Speech national especially 471 33 3655 availab...</td>\n",
       "      <td>[(471 33 3655, SSN)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>697</th>\n",
       "      <td>Forward listen step this community financial m...</td>\n",
       "      <td>[(008-52-3159, SSN)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>698</th>\n",
       "      <td>Test artist person billion. Trouble staff indu...</td>\n",
       "      <td>[(818-43-7502, SSN)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>699</th>\n",
       "      <td>Group think step increase 587-05-0700 answer k...</td>\n",
       "      <td>[(587-05-0700, SSN)]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text            annotation\n",
       "680  Talk while American suddenly 587-05-0700 parti...  [(587-05-0700, SSN)]\n",
       "681  Camera wall continue top us clearly hot includ...  [(896-11-9761, SSN)]\n",
       "682  Course college still loss scene. Series 530 12...  [(530 12 8752, SSN)]\n",
       "683  243 53 6315 Movement near value number able ab...  [(243 53 6315, SSN)]\n",
       "684  Pull key other nor 166-43-2545 save perform. L...  [(166-43-2545, SSN)]\n",
       "685   Ready off score foot market protect. 829 49 9139  [(829 49 9139, SSN)]\n",
       "686  Up course education involve thousand. Treat sc...  [(211 72 2423, SSN)]\n",
       "687  Huge develop environmental president their. Ma...  [(423 72 5067, SSN)]\n",
       "688  Possible both develop 595-23-4635 claim. Call ...  [(595-23-4635, SSN)]\n",
       "689  Describe question cover suggest 344 81 3132 ac...  [(344 81 3132, SSN)]\n",
       "690  Per learn agree despite smile him foot. Wait s...  [(877-42-6395, SSN)]\n",
       "691  766-65-9797 Director thousand manager give boa...  [(766-65-9797, SSN)]\n",
       "692  Report author 319-69-3108 increase a receive w...  [(319-69-3108, SSN)]\n",
       "693  Positive cause I arrive night site wind. Throu...  [(319-69-3108, SSN)]\n",
       "694  Store effort campaign girl worker technology. ...  [(377 98 7558, SSN)]\n",
       "695  Coach he west magazine against 510-81-5182 bea...  [(510-81-5182, SSN)]\n",
       "696  Speech national especially 471 33 3655 availab...  [(471 33 3655, SSN)]\n",
       "697  Forward listen step this community financial m...  [(008-52-3159, SSN)]\n",
       "698  Test artist person billion. Trouble staff indu...  [(818-43-7502, SSN)]\n",
       "699  Group think step increase 587-05-0700 answer k...  [(587-05-0700, SSN)]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.tail(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a7d45c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from difflib import SequenceMatcher\n",
    "import re\n",
    "import pickle\n",
    "\n",
    "def matcher(string, pattern):\n",
    "    '''\n",
    "    Return the start and end index of any pattern present in the text.\n",
    "    '''\n",
    "    match_list = []\n",
    "    pattern = pattern.strip()\n",
    "    seqMatch = SequenceMatcher(None, string, pattern, autojunk=False)\n",
    "    match = seqMatch.find_longest_match(0, len(string), 0, len(pattern))\n",
    "    if (match.size == len(pattern)):\n",
    "        start = match.a\n",
    "        end = match.a + match.size\n",
    "        match_tup = (start, end)\n",
    "        string = string.replace(pattern, \"X\" * len(pattern), 1)\n",
    "        match_list.append(match_tup)\n",
    "        \n",
    "    return match_list, string\n",
    "def mark_sentence(s, match_list):\n",
    "    '''\n",
    "    Marks all the entities in the sentence as per the BIO scheme. \n",
    "    '''\n",
    "    word_dict = {}\n",
    "    for word in s.split():\n",
    "        word_dict[word] = 'O'\n",
    "        \n",
    "    for start, end, e_type in match_list:\n",
    "        temp_str = s[start:end]\n",
    "        tmp_list = temp_str.split()\n",
    "        if len(tmp_list) > 1:\n",
    "            word_dict[tmp_list[0]] = 'B-' + e_type\n",
    "            for w in tmp_list[1:]:\n",
    "                word_dict[w] = 'I-' + e_type\n",
    "        else:\n",
    "            word_dict[temp_str] = 'B-' + e_type\n",
    "    return word_dict\n",
    "\n",
    "def clean(text):\n",
    "    '''\n",
    "    Just a helper fuction to add a space before the punctuations for better tokenization\n",
    "    '''\n",
    "    filters = [\"!\", \"#\", \"$\", \"%\", \"&\", \"(\", \")\", \"/\", \"*\", \".\", \":\", \";\", \"<\", \"=\", \">\", \"?\", \"@\", \"[\",\n",
    "               \"\\\\\", \"]\", \"_\", \"`\", \"{\", \"}\", \"~\", \"'\"]\n",
    "    for i in text:\n",
    "        if i in filters:\n",
    "            text = text.replace(i, \" \" + i)\n",
    "            \n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "51049484",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data(df, filepath):\n",
    "    '''\n",
    "    The function responsible for the creation of data in the said format.\n",
    "    '''\n",
    "    # here in the original code that I used the encoding was not specified which took me some time to figure\n",
    "    # out, and put the output files into required formats.\n",
    "    with open(filepath , 'w',encoding='utf-8') as f:\n",
    "        for text, annotation in zip(df.text, df.annotation):\n",
    "            #text = clean(text)\n",
    "            #text_ = text        \n",
    "            match_list = []\n",
    "            for i in annotation:\n",
    "                a, text_ = matcher(text, i[0])\n",
    "                match_list.append((a[0][0], a[0][1], i[1]))\n",
    "\n",
    "            d = mark_sentence(text, match_list)\n",
    "\n",
    "            for i in d.keys():\n",
    "                f.writelines(i + ' ' + d[i] +'\\n')\n",
    "            f.writelines('\\n')\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9bf4046e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    " \n",
    "    ## path to save the txt file. \n",
    "    ## run three times with train, test, and dev .txt filenames.\n",
    "    ## as shown in the below screen shot snippet. \n",
    "    filepath = 'C:/Users/Yamini/Documents/NER/dev.txt'\n",
    "    ## creating the file.\n",
    "    create_data(train, filepath)\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9a186ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading the Corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "780f73aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-01-13 09:13:18,360 Reading data from C:\\Users\\Yamini\\Documents\\NER\n",
      "2022-01-13 09:13:18,368 Train: C:\\Users\\Yamini\\Documents\\NER\\train.txt\n",
      "2022-01-13 09:13:18,368 Dev: C:\\Users\\Yamini\\Documents\\NER\\dev.txt\n",
      "2022-01-13 09:13:18,368 Test: C:\\Users\\Yamini\\Documents\\NER\\test.txt\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "from flair.data import Corpus\n",
    "from flair.datasets import ColumnCorpus\n",
    "\n",
    "# define columns\n",
    "columns = {0 : 'text', 1 : 'ner'}\n",
    "# directory where the data resides\n",
    "data_folder = 'C:/Users/Yamini/Documents/NER'\n",
    "# initializing the corpus\n",
    "corpus: Corpus = ColumnCorpus(data_folder, columns,\n",
    "                              train_file = 'train.txt',\n",
    "                              test_file = 'test.txt',\n",
    "                              dev_file = 'dev.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "71502216",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "700\n",
      "Candidate economic character present money daughter Apt. <B-ADDRESS> 026 <I-ADDRESS> world well. Open analysis center.\n"
     ]
    }
   ],
   "source": [
    "# lenght of our corpus and one example of tagging.\n",
    "print(len(corpus.train))\n",
    "print(corpus.train[0].to_tagged_string('ner'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3d5af5ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Yamini\\AppData\\Local\\Temp/ipykernel_17404/1387472010.py:4: DeprecationWarning: Call to deprecated method make_tag_dictionary. (Use 'make_label_dictionary' instead.) -- Deprecated since version 0.8.\n",
      "  tag_dictionary = corpus.make_tag_dictionary(tag_type=tag_type)\n"
     ]
    }
   ],
   "source": [
    "# tag to predict\n",
    "tag_type = 'ner'\n",
    "# make tag dictionary from the corpus\n",
    "tag_dictionary = corpus.make_tag_dictionary(tag_type=tag_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "38d89545",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{b'O': 0,\n",
       " b'B-ADDRESS': 1,\n",
       " b'I-ADDRESS': 2,\n",
       " b'B-CREDITCARDNUMBER': 3,\n",
       " b'B-EMAIL': 4,\n",
       " b'B-NAME': 5,\n",
       " b'I-NAME': 6,\n",
       " b'B-PHONE_NUMBER': 7,\n",
       " b'B-PLATES': 8,\n",
       " b'I-PLATES': 9,\n",
       " b'B-SSN': 10,\n",
       " b'I-SSN': 11,\n",
       " b'<START>': 12,\n",
       " b'<STOP>': 13}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# here are a list of avalibale tags that we created, which aligns with our PII on initial data.\n",
    "tag_dictionary.item2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "06d937f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize the embeddings\n",
    "from flair.embeddings import WordEmbeddings, StackedEmbeddings\n",
    "\n",
    "from typing import List\n",
    "embedding_types : List['TokenEmbeddings'] = [\n",
    "        WordEmbeddings('glove'),\n",
    "        ## other embeddings\n",
    "        WordEmbeddings('crawl'),\n",
    "        # ELMoEmbeddings() wanted to use this embedding since i read it improves the model accuracy significantly\n",
    "\n",
    "        ]\n",
    "embeddings : StackedEmbeddings = StackedEmbeddings(\n",
    "                                 embeddings=embedding_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8665f4dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here we initialize a Sequence Tagger. We activate Conditional Random Fields with use_crf=True flag. \n",
    "#On the backend training is being done by bi-directional LSTM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4d3a041f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SequenceTagger(\n",
      "  (embeddings): StackedEmbeddings(\n",
      "    (list_embedding_0): WordEmbeddings(\n",
      "      'glove'\n",
      "      (embedding): Embedding(400001, 100)\n",
      "    )\n",
      "    (list_embedding_1): WordEmbeddings(\n",
      "      'crawl'\n",
      "      (embedding): Embedding(1000001, 300)\n",
      "    )\n",
      "  )\n",
      "  (word_dropout): WordDropout(p=0.05)\n",
      "  (locked_dropout): LockedDropout(p=0.5)\n",
      "  (embedding2nn): Linear(in_features=400, out_features=400, bias=True)\n",
      "  (rnn): LSTM(400, 256, batch_first=True, bidirectional=True)\n",
      "  (linear): Linear(in_features=512, out_features=14, bias=True)\n",
      "  (beta): 1.0\n",
      "  (weights): None\n",
      "  (weight_tensor) None\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from flair.models import SequenceTagger\n",
    "\n",
    "tagger : SequenceTagger = SequenceTagger(hidden_size=256,\n",
    "                                       embeddings=embeddings,\n",
    "                                       tag_dictionary=tag_dictionary,\n",
    "                                       tag_type=tag_type,\n",
    "                                       use_crf=True)\n",
    "print(tagger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a2b3bcd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here we train our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b007a38f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-01-13 09:56:00,479 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-13 09:56:00,487 Model: \"SequenceTagger(\n",
      "  (embeddings): StackedEmbeddings(\n",
      "    (list_embedding_0): WordEmbeddings(\n",
      "      'glove'\n",
      "      (embedding): Embedding(400001, 100)\n",
      "    )\n",
      "    (list_embedding_1): WordEmbeddings(\n",
      "      'crawl'\n",
      "      (embedding): Embedding(1000001, 300)\n",
      "    )\n",
      "  )\n",
      "  (word_dropout): WordDropout(p=0.05)\n",
      "  (locked_dropout): LockedDropout(p=0.5)\n",
      "  (embedding2nn): Linear(in_features=400, out_features=400, bias=True)\n",
      "  (rnn): LSTM(400, 256, batch_first=True, bidirectional=True)\n",
      "  (linear): Linear(in_features=512, out_features=14, bias=True)\n",
      "  (beta): 1.0\n",
      "  (weights): None\n",
      "  (weight_tensor) None\n",
      ")\"\n",
      "2022-01-13 09:56:00,488 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-13 09:56:00,488 Corpus: \"Corpus: 700 train + 700 dev + 700 test sentences\"\n",
      "2022-01-13 09:56:00,488 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-13 09:56:00,488 Parameters:\n",
      "2022-01-13 09:56:00,488  - learning_rate: \"0.1\"\n",
      "2022-01-13 09:56:00,488  - mini_batch_size: \"32\"\n",
      "2022-01-13 09:56:00,488  - patience: \"3\"\n",
      "2022-01-13 09:56:00,488  - anneal_factor: \"0.5\"\n",
      "2022-01-13 09:56:00,488  - max_epochs: \"150\"\n",
      "2022-01-13 09:56:00,495  - shuffle: \"True\"\n",
      "2022-01-13 09:56:00,495  - train_with_dev: \"False\"\n",
      "2022-01-13 09:56:00,495  - batch_growth_annealing: \"False\"\n",
      "2022-01-13 09:56:00,495 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-13 09:56:00,495 Model training base path: \"resources\\taggers\\example-ner\"\n",
      "2022-01-13 09:56:00,495 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-13 09:56:00,503 Device: cpu\n",
      "2022-01-13 09:56:00,503 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-13 09:56:00,503 Embeddings storage mode: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Yamini\\anaconda3\\lib\\site-packages\\flair\\trainers\\trainer.py:64: UserWarning: There should be no best model saved at epoch 1 except there is a model from previous trainings in your training folder. All previous best models will be deleted.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-01-13 09:56:00,743 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-13 09:56:01,302 epoch 1 - iter 2/22 - loss 3.93779930 - samples/sec: 114.53 - lr: 0.100000\n",
      "2022-01-13 09:56:01,809 epoch 1 - iter 4/22 - loss 3.14496055 - samples/sec: 126.22 - lr: 0.100000\n",
      "2022-01-13 09:56:02,209 epoch 1 - iter 6/22 - loss 2.38903525 - samples/sec: 159.97 - lr: 0.100000\n",
      "2022-01-13 09:56:02,659 epoch 1 - iter 8/22 - loss 1.90877115 - samples/sec: 147.26 - lr: 0.100000\n",
      "2022-01-13 09:56:03,049 epoch 1 - iter 10/22 - loss 1.63348920 - samples/sec: 164.07 - lr: 0.100000\n",
      "2022-01-13 09:56:03,470 epoch 1 - iter 12/22 - loss 1.47632219 - samples/sec: 152.25 - lr: 0.100000\n",
      "2022-01-13 09:56:03,909 epoch 1 - iter 14/22 - loss 1.32397090 - samples/sec: 145.54 - lr: 0.100000\n",
      "2022-01-13 09:56:04,277 epoch 1 - iter 16/22 - loss 1.20476542 - samples/sec: 174.31 - lr: 0.100000\n",
      "2022-01-13 09:56:04,721 epoch 1 - iter 18/22 - loss 1.13077813 - samples/sec: 144.12 - lr: 0.100000\n",
      "2022-01-13 09:56:05,144 epoch 1 - iter 20/22 - loss 1.06380273 - samples/sec: 151.25 - lr: 0.100000\n",
      "2022-01-13 09:56:05,533 epoch 1 - iter 22/22 - loss 0.99741388 - samples/sec: 164.25 - lr: 0.100000\n",
      "2022-01-13 09:56:05,533 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-13 09:56:05,533 EPOCH 1 done: loss 0.9974 - lr 0.1000000\n",
      "2022-01-13 09:56:07,271 DEV : loss 0.4337974190711975 - f1-score (micro avg)  0.0454\n",
      "2022-01-13 09:56:07,318 BAD EPOCHS (no improvement): 0\n",
      "2022-01-13 09:56:07,318 saving best model\n",
      "2022-01-13 09:56:12,640 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-13 09:56:13,098 epoch 2 - iter 2/22 - loss 0.47332798 - samples/sec: 139.69 - lr: 0.100000\n",
      "2022-01-13 09:56:13,535 epoch 2 - iter 4/22 - loss 0.43566925 - samples/sec: 146.31 - lr: 0.100000\n",
      "2022-01-13 09:56:13,990 epoch 2 - iter 6/22 - loss 0.40767604 - samples/sec: 140.72 - lr: 0.100000\n",
      "2022-01-13 09:56:14,422 epoch 2 - iter 8/22 - loss 0.41007450 - samples/sec: 153.98 - lr: 0.100000\n",
      "2022-01-13 09:56:14,933 epoch 2 - iter 10/22 - loss 0.40099543 - samples/sec: 125.04 - lr: 0.100000\n",
      "2022-01-13 09:56:15,432 epoch 2 - iter 12/22 - loss 0.39138704 - samples/sec: 128.38 - lr: 0.100000\n",
      "2022-01-13 09:56:15,910 epoch 2 - iter 14/22 - loss 0.37843886 - samples/sec: 133.99 - lr: 0.100000\n",
      "2022-01-13 09:56:16,340 epoch 2 - iter 16/22 - loss 0.36806988 - samples/sec: 148.70 - lr: 0.100000\n",
      "2022-01-13 09:56:16,839 epoch 2 - iter 18/22 - loss 0.36066429 - samples/sec: 128.16 - lr: 0.100000\n",
      "2022-01-13 09:56:17,293 epoch 2 - iter 20/22 - loss 0.35702430 - samples/sec: 141.19 - lr: 0.100000\n",
      "2022-01-13 09:56:17,793 epoch 2 - iter 22/22 - loss 0.35299449 - samples/sec: 127.87 - lr: 0.100000\n",
      "2022-01-13 09:56:17,793 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-13 09:56:17,809 EPOCH 2 done: loss 0.3530 - lr 0.1000000\n",
      "2022-01-13 09:56:19,199 DEV : loss 0.21625113487243652 - f1-score (micro avg)  0.2702\n",
      "2022-01-13 09:56:19,239 BAD EPOCHS (no improvement): 0\n",
      "2022-01-13 09:56:19,239 saving best model\n",
      "2022-01-13 09:56:23,777 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-13 09:56:24,252 epoch 3 - iter 2/22 - loss 0.26543911 - samples/sec: 134.76 - lr: 0.100000\n",
      "2022-01-13 09:56:24,936 epoch 3 - iter 4/22 - loss 0.27319777 - samples/sec: 93.55 - lr: 0.100000\n",
      "2022-01-13 09:56:25,319 epoch 3 - iter 6/22 - loss 0.26830820 - samples/sec: 166.85 - lr: 0.100000\n",
      "2022-01-13 09:56:25,821 epoch 3 - iter 8/22 - loss 0.26486847 - samples/sec: 127.54 - lr: 0.100000\n",
      "2022-01-13 09:56:26,276 epoch 3 - iter 10/22 - loss 0.26524057 - samples/sec: 145.88 - lr: 0.100000\n",
      "2022-01-13 09:56:26,725 epoch 3 - iter 12/22 - loss 0.26423862 - samples/sec: 147.61 - lr: 0.100000\n",
      "2022-01-13 09:56:27,181 epoch 3 - iter 14/22 - loss 0.26512856 - samples/sec: 140.20 - lr: 0.100000\n",
      "2022-01-13 09:56:27,739 epoch 3 - iter 16/22 - loss 0.25855579 - samples/sec: 114.67 - lr: 0.100000\n",
      "2022-01-13 09:56:28,243 epoch 3 - iter 18/22 - loss 0.25620389 - samples/sec: 127.13 - lr: 0.100000\n",
      "2022-01-13 09:56:28,707 epoch 3 - iter 20/22 - loss 0.25419874 - samples/sec: 138.31 - lr: 0.100000\n",
      "2022-01-13 09:56:29,164 epoch 3 - iter 22/22 - loss 0.25229437 - samples/sec: 140.18 - lr: 0.100000\n",
      "2022-01-13 09:56:29,164 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-13 09:56:29,164 EPOCH 3 done: loss 0.2523 - lr 0.1000000\n",
      "2022-01-13 09:56:30,588 DEV : loss 0.16455477476119995 - f1-score (micro avg)  0.4472\n",
      "2022-01-13 09:56:30,619 BAD EPOCHS (no improvement): 0\n",
      "2022-01-13 09:56:30,619 saving best model\n",
      "2022-01-13 09:56:35,445 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-13 09:56:36,205 epoch 4 - iter 2/22 - loss 0.22355404 - samples/sec: 85.05 - lr: 0.100000\n",
      "2022-01-13 09:56:36,677 epoch 4 - iter 4/22 - loss 0.20581420 - samples/sec: 135.73 - lr: 0.100000\n",
      "2022-01-13 09:56:37,170 epoch 4 - iter 6/22 - loss 0.20820797 - samples/sec: 134.00 - lr: 0.100000\n",
      "2022-01-13 09:56:37,772 epoch 4 - iter 8/22 - loss 0.20396578 - samples/sec: 106.28 - lr: 0.100000\n",
      "2022-01-13 09:56:38,300 epoch 4 - iter 10/22 - loss 0.19957055 - samples/sec: 125.62 - lr: 0.100000\n",
      "2022-01-13 09:56:38,847 epoch 4 - iter 12/22 - loss 0.19823279 - samples/sec: 117.01 - lr: 0.100000\n",
      "2022-01-13 09:56:39,291 epoch 4 - iter 14/22 - loss 0.19761860 - samples/sec: 144.15 - lr: 0.100000\n",
      "2022-01-13 09:56:39,744 epoch 4 - iter 16/22 - loss 0.19791693 - samples/sec: 141.14 - lr: 0.100000\n",
      "2022-01-13 09:56:40,241 epoch 4 - iter 18/22 - loss 0.19647968 - samples/sec: 132.96 - lr: 0.100000\n",
      "2022-01-13 09:56:40,651 epoch 4 - iter 20/22 - loss 0.19636790 - samples/sec: 156.27 - lr: 0.100000\n",
      "2022-01-13 09:56:41,084 epoch 4 - iter 22/22 - loss 0.19731436 - samples/sec: 147.68 - lr: 0.100000\n",
      "2022-01-13 09:56:41,084 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-13 09:56:41,084 EPOCH 4 done: loss 0.1973 - lr 0.1000000\n",
      "2022-01-13 09:56:42,559 DEV : loss 0.1346154808998108 - f1-score (micro avg)  0.4879\n",
      "2022-01-13 09:56:42,590 BAD EPOCHS (no improvement): 0\n",
      "2022-01-13 09:56:42,606 saving best model\n",
      "2022-01-13 09:56:47,703 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-13 09:56:48,478 epoch 5 - iter 2/22 - loss 0.16675105 - samples/sec: 83.98 - lr: 0.100000\n",
      "2022-01-13 09:56:49,244 epoch 5 - iter 4/22 - loss 0.16774127 - samples/sec: 83.63 - lr: 0.100000\n",
      "2022-01-13 09:56:49,958 epoch 5 - iter 6/22 - loss 0.16964026 - samples/sec: 89.64 - lr: 0.100000\n",
      "2022-01-13 09:56:50,632 epoch 5 - iter 8/22 - loss 0.17801637 - samples/sec: 94.98 - lr: 0.100000\n",
      "2022-01-13 09:56:51,549 epoch 5 - iter 10/22 - loss 0.17842325 - samples/sec: 70.37 - lr: 0.100000\n",
      "2022-01-13 09:56:52,410 epoch 5 - iter 12/22 - loss 0.17307189 - samples/sec: 74.36 - lr: 0.100000\n",
      "2022-01-13 09:56:52,936 epoch 5 - iter 14/22 - loss 0.17590183 - samples/sec: 121.65 - lr: 0.100000\n",
      "2022-01-13 09:56:53,437 epoch 5 - iter 16/22 - loss 0.17532255 - samples/sec: 127.72 - lr: 0.100000\n",
      "2022-01-13 09:56:54,017 epoch 5 - iter 18/22 - loss 0.17499378 - samples/sec: 110.25 - lr: 0.100000\n",
      "2022-01-13 09:56:54,514 epoch 5 - iter 20/22 - loss 0.17526925 - samples/sec: 128.97 - lr: 0.100000\n",
      "2022-01-13 09:56:54,980 epoch 5 - iter 22/22 - loss 0.17292337 - samples/sec: 137.48 - lr: 0.100000\n",
      "2022-01-13 09:56:54,980 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-13 09:56:54,995 EPOCH 5 done: loss 0.1729 - lr 0.1000000\n",
      "2022-01-13 09:56:56,954 DEV : loss 0.14268307387828827 - f1-score (micro avg)  0.5538\n",
      "2022-01-13 09:56:56,987 BAD EPOCHS (no improvement): 0\n",
      "2022-01-13 09:56:56,995 saving best model\n",
      "2022-01-13 09:57:01,588 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-13 09:57:02,038 epoch 6 - iter 2/22 - loss 0.18498158 - samples/sec: 142.11 - lr: 0.100000\n",
      "2022-01-13 09:57:02,494 epoch 6 - iter 4/22 - loss 0.17716321 - samples/sec: 140.42 - lr: 0.100000\n",
      "2022-01-13 09:57:02,957 epoch 6 - iter 6/22 - loss 0.17657897 - samples/sec: 138.31 - lr: 0.100000\n",
      "2022-01-13 09:57:03,339 epoch 6 - iter 8/22 - loss 0.17272145 - samples/sec: 167.90 - lr: 0.100000\n",
      "2022-01-13 09:57:03,822 epoch 6 - iter 10/22 - loss 0.17205709 - samples/sec: 132.66 - lr: 0.100000\n",
      "2022-01-13 09:57:04,216 epoch 6 - iter 12/22 - loss 0.17551156 - samples/sec: 162.09 - lr: 0.100000\n",
      "2022-01-13 09:57:04,694 epoch 6 - iter 14/22 - loss 0.17190358 - samples/sec: 133.99 - lr: 0.100000\n",
      "2022-01-13 09:57:05,198 epoch 6 - iter 16/22 - loss 0.16664899 - samples/sec: 127.31 - lr: 0.100000\n",
      "2022-01-13 09:57:05,734 epoch 6 - iter 18/22 - loss 0.16564473 - samples/sec: 119.28 - lr: 0.100000\n",
      "2022-01-13 09:57:06,208 epoch 6 - iter 20/22 - loss 0.16291716 - samples/sec: 135.18 - lr: 0.100000\n",
      "2022-01-13 09:57:06,652 epoch 6 - iter 22/22 - loss 0.16234289 - samples/sec: 143.95 - lr: 0.100000\n",
      "2022-01-13 09:57:06,652 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-13 09:57:06,652 EPOCH 6 done: loss 0.1623 - lr 0.1000000\n",
      "2022-01-13 09:57:08,048 DEV : loss 0.10858266055583954 - f1-score (micro avg)  0.6123\n",
      "2022-01-13 09:57:08,097 BAD EPOCHS (no improvement): 0\n",
      "2022-01-13 09:57:08,097 saving best model\n",
      "2022-01-13 09:57:12,561 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-13 09:57:13,178 epoch 7 - iter 2/22 - loss 0.14596399 - samples/sec: 103.68 - lr: 0.100000\n",
      "2022-01-13 09:57:13,701 epoch 7 - iter 4/22 - loss 0.15036708 - samples/sec: 122.74 - lr: 0.100000\n",
      "2022-01-13 09:57:14,197 epoch 7 - iter 6/22 - loss 0.15103316 - samples/sec: 133.38 - lr: 0.100000\n",
      "2022-01-13 09:57:14,637 epoch 7 - iter 8/22 - loss 0.15960294 - samples/sec: 145.40 - lr: 0.100000\n",
      "2022-01-13 09:57:15,245 epoch 7 - iter 10/22 - loss 0.15573731 - samples/sec: 105.22 - lr: 0.100000\n",
      "2022-01-13 09:57:15,663 epoch 7 - iter 12/22 - loss 0.15223157 - samples/sec: 153.15 - lr: 0.100000\n",
      "2022-01-13 09:57:16,228 epoch 7 - iter 14/22 - loss 0.14854872 - samples/sec: 113.25 - lr: 0.100000\n",
      "2022-01-13 09:57:16,814 epoch 7 - iter 16/22 - loss 0.14927531 - samples/sec: 109.35 - lr: 0.100000\n",
      "2022-01-13 09:57:17,266 epoch 7 - iter 18/22 - loss 0.15010742 - samples/sec: 142.04 - lr: 0.100000\n",
      "2022-01-13 09:57:17,808 epoch 7 - iter 20/22 - loss 0.14854057 - samples/sec: 118.04 - lr: 0.100000\n",
      "2022-01-13 09:57:18,243 epoch 7 - iter 22/22 - loss 0.14874165 - samples/sec: 152.57 - lr: 0.100000\n",
      "2022-01-13 09:57:18,243 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-13 09:57:18,243 EPOCH 7 done: loss 0.1487 - lr 0.1000000\n",
      "2022-01-13 09:57:19,882 DEV : loss 0.1151956096291542 - f1-score (micro avg)  0.654\n",
      "2022-01-13 09:57:19,915 BAD EPOCHS (no improvement): 0\n",
      "2022-01-13 09:57:19,915 saving best model\n",
      "2022-01-13 09:57:24,815 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-13 09:57:25,258 epoch 8 - iter 2/22 - loss 0.15615501 - samples/sec: 149.55 - lr: 0.100000\n",
      "2022-01-13 09:57:25,821 epoch 8 - iter 4/22 - loss 0.13891857 - samples/sec: 113.65 - lr: 0.100000\n",
      "2022-01-13 09:57:26,339 epoch 8 - iter 6/22 - loss 0.14550862 - samples/sec: 123.67 - lr: 0.100000\n",
      "2022-01-13 09:57:26,832 epoch 8 - iter 8/22 - loss 0.14583840 - samples/sec: 129.82 - lr: 0.100000\n",
      "2022-01-13 09:57:27,269 epoch 8 - iter 10/22 - loss 0.14949325 - samples/sec: 146.31 - lr: 0.100000\n",
      "2022-01-13 09:57:27,764 epoch 8 - iter 12/22 - loss 0.14531901 - samples/sec: 129.48 - lr: 0.100000\n",
      "2022-01-13 09:57:28,310 epoch 8 - iter 14/22 - loss 0.14170468 - samples/sec: 117.15 - lr: 0.100000\n",
      "2022-01-13 09:57:28,849 epoch 8 - iter 16/22 - loss 0.13738461 - samples/sec: 119.10 - lr: 0.100000\n",
      "2022-01-13 09:57:29,363 epoch 8 - iter 18/22 - loss 0.13883462 - samples/sec: 125.00 - lr: 0.100000\n",
      "2022-01-13 09:57:29,890 epoch 8 - iter 20/22 - loss 0.13968210 - samples/sec: 121.30 - lr: 0.100000\n",
      "2022-01-13 09:57:30,353 epoch 8 - iter 22/22 - loss 0.14015439 - samples/sec: 138.24 - lr: 0.100000\n",
      "2022-01-13 09:57:30,353 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-13 09:57:30,353 EPOCH 8 done: loss 0.1402 - lr 0.1000000\n",
      "2022-01-13 09:57:31,842 DEV : loss 0.10840067267417908 - f1-score (micro avg)  0.6439\n",
      "2022-01-13 09:57:31,874 BAD EPOCHS (no improvement): 1\n",
      "2022-01-13 09:57:31,874 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-13 09:57:32,347 epoch 9 - iter 2/22 - loss 0.11047800 - samples/sec: 138.65 - lr: 0.100000\n",
      "2022-01-13 09:57:32,897 epoch 9 - iter 4/22 - loss 0.12320645 - samples/sec: 116.27 - lr: 0.100000\n",
      "2022-01-13 09:57:33,455 epoch 9 - iter 6/22 - loss 0.11970862 - samples/sec: 114.72 - lr: 0.100000\n",
      "2022-01-13 09:57:33,984 epoch 9 - iter 8/22 - loss 0.12679102 - samples/sec: 124.66 - lr: 0.100000\n",
      "2022-01-13 09:57:34,444 epoch 9 - iter 10/22 - loss 0.12899157 - samples/sec: 139.06 - lr: 0.100000\n",
      "2022-01-13 09:57:35,017 epoch 9 - iter 12/22 - loss 0.12994975 - samples/sec: 111.65 - lr: 0.100000\n",
      "2022-01-13 09:57:35,511 epoch 9 - iter 14/22 - loss 0.12895789 - samples/sec: 129.75 - lr: 0.100000\n",
      "2022-01-13 09:57:36,011 epoch 9 - iter 16/22 - loss 0.12917615 - samples/sec: 128.02 - lr: 0.100000\n",
      "2022-01-13 09:57:36,483 epoch 9 - iter 18/22 - loss 0.12823265 - samples/sec: 135.33 - lr: 0.100000\n",
      "2022-01-13 09:57:36,948 epoch 9 - iter 20/22 - loss 0.12933525 - samples/sec: 137.78 - lr: 0.100000\n",
      "2022-01-13 09:57:37,399 epoch 9 - iter 22/22 - loss 0.12992961 - samples/sec: 141.75 - lr: 0.100000\n",
      "2022-01-13 09:57:37,399 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-13 09:57:37,399 EPOCH 9 done: loss 0.1299 - lr 0.1000000\n",
      "2022-01-13 09:57:39,020 DEV : loss 0.07945432513952255 - f1-score (micro avg)  0.6667\n",
      "2022-01-13 09:57:39,067 BAD EPOCHS (no improvement): 0\n",
      "2022-01-13 09:57:39,067 saving best model\n",
      "2022-01-13 09:57:44,074 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-13 09:57:44,609 epoch 10 - iter 2/22 - loss 0.14373025 - samples/sec: 119.68 - lr: 0.100000\n",
      "2022-01-13 09:57:45,188 epoch 10 - iter 4/22 - loss 0.13769537 - samples/sec: 110.55 - lr: 0.100000\n",
      "2022-01-13 09:57:45,592 epoch 10 - iter 6/22 - loss 0.13763337 - samples/sec: 158.43 - lr: 0.100000\n",
      "2022-01-13 09:57:46,073 epoch 10 - iter 8/22 - loss 0.14010019 - samples/sec: 132.85 - lr: 0.100000\n",
      "2022-01-13 09:57:46,534 epoch 10 - iter 10/22 - loss 0.13537937 - samples/sec: 139.00 - lr: 0.100000\n",
      "2022-01-13 09:57:47,088 epoch 10 - iter 12/22 - loss 0.13087645 - samples/sec: 115.58 - lr: 0.100000\n",
      "2022-01-13 09:57:47,579 epoch 10 - iter 14/22 - loss 0.13262644 - samples/sec: 130.33 - lr: 0.100000\n",
      "2022-01-13 09:57:48,082 epoch 10 - iter 16/22 - loss 0.13044940 - samples/sec: 127.27 - lr: 0.100000\n",
      "2022-01-13 09:57:48,573 epoch 10 - iter 18/22 - loss 0.13040525 - samples/sec: 130.24 - lr: 0.100000\n",
      "2022-01-13 09:57:49,147 epoch 10 - iter 20/22 - loss 0.12994344 - samples/sec: 111.58 - lr: 0.100000\n",
      "2022-01-13 09:57:49,726 epoch 10 - iter 22/22 - loss 0.12996644 - samples/sec: 110.39 - lr: 0.100000\n",
      "2022-01-13 09:57:49,726 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-13 09:57:49,726 EPOCH 10 done: loss 0.1300 - lr 0.1000000\n",
      "2022-01-13 09:57:51,237 DEV : loss 0.10519221425056458 - f1-score (micro avg)  0.6616\n",
      "2022-01-13 09:57:51,281 BAD EPOCHS (no improvement): 1\n",
      "2022-01-13 09:57:51,281 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-13 09:57:51,819 epoch 11 - iter 2/22 - loss 0.13938817 - samples/sec: 119.55 - lr: 0.100000\n",
      "2022-01-13 09:57:52,328 epoch 11 - iter 4/22 - loss 0.12889612 - samples/sec: 125.64 - lr: 0.100000\n",
      "2022-01-13 09:57:52,983 epoch 11 - iter 6/22 - loss 0.13068747 - samples/sec: 97.80 - lr: 0.100000\n",
      "2022-01-13 09:57:53,509 epoch 11 - iter 8/22 - loss 0.12542504 - samples/sec: 121.64 - lr: 0.100000\n",
      "2022-01-13 09:57:53,965 epoch 11 - iter 10/22 - loss 0.12698495 - samples/sec: 140.39 - lr: 0.100000\n",
      "2022-01-13 09:57:54,423 epoch 11 - iter 12/22 - loss 0.12634104 - samples/sec: 139.63 - lr: 0.100000\n",
      "2022-01-13 09:57:55,075 epoch 11 - iter 14/22 - loss 0.12716296 - samples/sec: 98.15 - lr: 0.100000\n",
      "2022-01-13 09:57:55,646 epoch 11 - iter 16/22 - loss 0.12674567 - samples/sec: 113.79 - lr: 0.100000\n",
      "2022-01-13 09:57:56,428 epoch 11 - iter 18/22 - loss 0.12472056 - samples/sec: 81.85 - lr: 0.100000\n",
      "2022-01-13 09:57:56,973 epoch 11 - iter 20/22 - loss 0.12467756 - samples/sec: 117.28 - lr: 0.100000\n",
      "2022-01-13 09:57:57,359 epoch 11 - iter 22/22 - loss 0.12512383 - samples/sec: 165.78 - lr: 0.100000\n",
      "2022-01-13 09:57:57,359 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-13 09:57:57,375 EPOCH 11 done: loss 0.1251 - lr 0.1000000\n",
      "2022-01-13 09:57:59,039 DEV : loss 0.07131336629390717 - f1-score (micro avg)  0.6419\n",
      "2022-01-13 09:57:59,079 BAD EPOCHS (no improvement): 2\n",
      "2022-01-13 09:57:59,079 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-13 09:57:59,582 epoch 12 - iter 2/22 - loss 0.13506386 - samples/sec: 127.23 - lr: 0.100000\n",
      "2022-01-13 09:58:00,333 epoch 12 - iter 4/22 - loss 0.13005211 - samples/sec: 85.17 - lr: 0.100000\n",
      "2022-01-13 09:58:00,947 epoch 12 - iter 6/22 - loss 0.13555734 - samples/sec: 106.75 - lr: 0.100000\n",
      "2022-01-13 09:58:01,386 epoch 12 - iter 8/22 - loss 0.13167278 - samples/sec: 145.77 - lr: 0.100000\n",
      "2022-01-13 09:58:01,883 epoch 12 - iter 10/22 - loss 0.13089393 - samples/sec: 128.77 - lr: 0.100000\n",
      "2022-01-13 09:58:02,318 epoch 12 - iter 12/22 - loss 0.13069285 - samples/sec: 152.47 - lr: 0.100000\n",
      "2022-01-13 09:58:02,948 epoch 12 - iter 14/22 - loss 0.13070030 - samples/sec: 101.64 - lr: 0.100000\n",
      "2022-01-13 09:58:03,385 epoch 12 - iter 16/22 - loss 0.12888867 - samples/sec: 146.40 - lr: 0.100000\n",
      "2022-01-13 09:58:03,925 epoch 12 - iter 18/22 - loss 0.12871310 - samples/sec: 118.41 - lr: 0.100000\n",
      "2022-01-13 09:58:04,480 epoch 12 - iter 20/22 - loss 0.12571417 - samples/sec: 115.84 - lr: 0.100000\n",
      "2022-01-13 09:58:04,929 epoch 12 - iter 22/22 - loss 0.12626487 - samples/sec: 147.63 - lr: 0.100000\n",
      "2022-01-13 09:58:04,929 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-13 09:58:04,929 EPOCH 12 done: loss 0.1263 - lr 0.1000000\n",
      "2022-01-13 09:58:06,693 DEV : loss 0.07184390723705292 - f1-score (micro avg)  0.6667\n",
      "2022-01-13 09:58:06,721 BAD EPOCHS (no improvement): 0\n",
      "2022-01-13 09:58:06,721 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-13 09:58:07,169 epoch 13 - iter 2/22 - loss 0.14846811 - samples/sec: 146.60 - lr: 0.100000\n",
      "2022-01-13 09:58:07,573 epoch 13 - iter 4/22 - loss 0.12008616 - samples/sec: 158.47 - lr: 0.100000\n",
      "2022-01-13 09:58:08,147 epoch 13 - iter 6/22 - loss 0.12514961 - samples/sec: 111.55 - lr: 0.100000\n",
      "2022-01-13 09:58:08,661 epoch 13 - iter 8/22 - loss 0.12645423 - samples/sec: 124.55 - lr: 0.100000\n",
      "2022-01-13 09:58:09,076 epoch 13 - iter 10/22 - loss 0.12635742 - samples/sec: 154.24 - lr: 0.100000\n",
      "2022-01-13 09:58:09,476 epoch 13 - iter 12/22 - loss 0.13033027 - samples/sec: 159.78 - lr: 0.100000\n",
      "2022-01-13 09:58:09,990 epoch 13 - iter 14/22 - loss 0.12936007 - samples/sec: 124.56 - lr: 0.100000\n",
      "2022-01-13 09:58:10,477 epoch 13 - iter 16/22 - loss 0.12318144 - samples/sec: 131.37 - lr: 0.100000\n",
      "2022-01-13 09:58:10,932 epoch 13 - iter 18/22 - loss 0.12236629 - samples/sec: 140.70 - lr: 0.100000\n",
      "2022-01-13 09:58:11,375 epoch 13 - iter 20/22 - loss 0.12101524 - samples/sec: 149.91 - lr: 0.100000\n",
      "2022-01-13 09:58:11,903 epoch 13 - iter 22/22 - loss 0.12087226 - samples/sec: 121.14 - lr: 0.100000\n",
      "2022-01-13 09:58:11,903 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-13 09:58:11,903 EPOCH 13 done: loss 0.1209 - lr 0.1000000\n",
      "2022-01-13 09:58:13,451 DEV : loss 0.06353403627872467 - f1-score (micro avg)  0.6454\n",
      "2022-01-13 09:58:13,499 BAD EPOCHS (no improvement): 1\n",
      "2022-01-13 09:58:13,499 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-13 09:58:13,965 epoch 14 - iter 2/22 - loss 0.11880416 - samples/sec: 140.16 - lr: 0.100000\n",
      "2022-01-13 09:58:14,435 epoch 14 - iter 4/22 - loss 0.12423217 - samples/sec: 136.14 - lr: 0.100000\n",
      "2022-01-13 09:58:15,004 epoch 14 - iter 6/22 - loss 0.11412437 - samples/sec: 112.36 - lr: 0.100000\n",
      "2022-01-13 09:58:15,441 epoch 14 - iter 8/22 - loss 0.11232756 - samples/sec: 149.15 - lr: 0.100000\n",
      "2022-01-13 09:58:15,869 epoch 14 - iter 10/22 - loss 0.11289225 - samples/sec: 149.50 - lr: 0.100000\n",
      "2022-01-13 09:58:16,286 epoch 14 - iter 12/22 - loss 0.11152235 - samples/sec: 153.83 - lr: 0.100000\n",
      "2022-01-13 09:58:16,731 epoch 14 - iter 14/22 - loss 0.11123401 - samples/sec: 143.80 - lr: 0.100000\n",
      "2022-01-13 09:58:17,245 epoch 14 - iter 16/22 - loss 0.11314571 - samples/sec: 125.94 - lr: 0.100000\n",
      "2022-01-13 09:58:17,830 epoch 14 - iter 18/22 - loss 0.11307393 - samples/sec: 109.29 - lr: 0.100000\n",
      "2022-01-13 09:58:18,359 epoch 14 - iter 20/22 - loss 0.11419799 - samples/sec: 121.15 - lr: 0.100000\n",
      "2022-01-13 09:58:18,793 epoch 14 - iter 22/22 - loss 0.11634735 - samples/sec: 147.21 - lr: 0.100000\n",
      "2022-01-13 09:58:18,793 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-13 09:58:18,801 EPOCH 14 done: loss 0.1163 - lr 0.1000000\n",
      "2022-01-13 09:58:20,304 DEV : loss 0.059895146638154984 - f1-score (micro avg)  0.647\n",
      "2022-01-13 09:58:20,341 BAD EPOCHS (no improvement): 2\n",
      "2022-01-13 09:58:20,341 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-13 09:58:20,825 epoch 15 - iter 2/22 - loss 0.12697276 - samples/sec: 135.00 - lr: 0.100000\n",
      "2022-01-13 09:58:21,216 epoch 15 - iter 4/22 - loss 0.12573354 - samples/sec: 163.36 - lr: 0.100000\n",
      "2022-01-13 09:58:21,678 epoch 15 - iter 6/22 - loss 0.11992146 - samples/sec: 138.66 - lr: 0.100000\n",
      "2022-01-13 09:58:22,206 epoch 15 - iter 8/22 - loss 0.11582851 - samples/sec: 121.31 - lr: 0.100000\n",
      "2022-01-13 09:58:22,635 epoch 15 - iter 10/22 - loss 0.11635857 - samples/sec: 148.85 - lr: 0.100000\n",
      "2022-01-13 09:58:23,124 epoch 15 - iter 12/22 - loss 0.11572681 - samples/sec: 131.03 - lr: 0.100000\n",
      "2022-01-13 09:58:23,624 epoch 15 - iter 14/22 - loss 0.11373001 - samples/sec: 127.88 - lr: 0.100000\n",
      "2022-01-13 09:58:24,094 epoch 15 - iter 16/22 - loss 0.11571605 - samples/sec: 136.23 - lr: 0.100000\n",
      "2022-01-13 09:58:24,504 epoch 15 - iter 18/22 - loss 0.11514951 - samples/sec: 156.05 - lr: 0.100000\n",
      "2022-01-13 09:58:24,987 epoch 15 - iter 20/22 - loss 0.11648963 - samples/sec: 132.67 - lr: 0.100000\n",
      "2022-01-13 09:58:25,434 epoch 15 - iter 22/22 - loss 0.11562624 - samples/sec: 143.17 - lr: 0.100000\n",
      "2022-01-13 09:58:25,434 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-13 09:58:25,434 EPOCH 15 done: loss 0.1156 - lr 0.1000000\n",
      "2022-01-13 09:58:26,921 DEV : loss 0.06846851855516434 - f1-score (micro avg)  0.6754\n",
      "2022-01-13 09:58:26,950 BAD EPOCHS (no improvement): 0\n",
      "2022-01-13 09:58:26,950 saving best model\n",
      "2022-01-13 09:58:32,474 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-13 09:58:32,992 epoch 16 - iter 2/22 - loss 0.13363263 - samples/sec: 123.57 - lr: 0.100000\n",
      "2022-01-13 09:58:33,448 epoch 16 - iter 4/22 - loss 0.13558712 - samples/sec: 140.33 - lr: 0.100000\n",
      "2022-01-13 09:58:33,947 epoch 16 - iter 6/22 - loss 0.13198192 - samples/sec: 132.29 - lr: 0.100000\n",
      "2022-01-13 09:58:34,416 epoch 16 - iter 8/22 - loss 0.13077345 - samples/sec: 136.67 - lr: 0.100000\n",
      "2022-01-13 09:58:34,913 epoch 16 - iter 10/22 - loss 0.12507574 - samples/sec: 128.68 - lr: 0.100000\n",
      "2022-01-13 09:58:35,390 epoch 16 - iter 12/22 - loss 0.11868076 - samples/sec: 134.25 - lr: 0.100000\n",
      "2022-01-13 09:58:35,892 epoch 16 - iter 14/22 - loss 0.11623292 - samples/sec: 127.36 - lr: 0.100000\n",
      "2022-01-13 09:58:36,380 epoch 16 - iter 16/22 - loss 0.11585835 - samples/sec: 131.31 - lr: 0.100000\n",
      "2022-01-13 09:58:36,860 epoch 16 - iter 18/22 - loss 0.11580100 - samples/sec: 133.16 - lr: 0.100000\n",
      "2022-01-13 09:58:37,349 epoch 16 - iter 20/22 - loss 0.11382509 - samples/sec: 131.04 - lr: 0.100000\n",
      "2022-01-13 09:58:37,758 epoch 16 - iter 22/22 - loss 0.11135710 - samples/sec: 156.23 - lr: 0.100000\n",
      "2022-01-13 09:58:37,758 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-13 09:58:37,758 EPOCH 16 done: loss 0.1114 - lr 0.1000000\n",
      "2022-01-13 09:58:39,166 DEV : loss 0.08104006946086884 - f1-score (micro avg)  0.6717\n",
      "2022-01-13 09:58:39,196 BAD EPOCHS (no improvement): 1\n",
      "2022-01-13 09:58:39,196 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-13 09:58:39,562 epoch 17 - iter 2/22 - loss 0.10116284 - samples/sec: 174.51 - lr: 0.100000\n",
      "2022-01-13 09:58:40,071 epoch 17 - iter 4/22 - loss 0.10345789 - samples/sec: 129.29 - lr: 0.100000\n",
      "2022-01-13 09:58:40,568 epoch 17 - iter 6/22 - loss 0.10208697 - samples/sec: 128.79 - lr: 0.100000\n",
      "2022-01-13 09:58:41,177 epoch 17 - iter 8/22 - loss 0.10193675 - samples/sec: 106.63 - lr: 0.100000\n",
      "2022-01-13 09:58:41,678 epoch 17 - iter 10/22 - loss 0.10002688 - samples/sec: 129.29 - lr: 0.100000\n",
      "2022-01-13 09:58:42,263 epoch 17 - iter 12/22 - loss 0.10377962 - samples/sec: 109.46 - lr: 0.100000\n",
      "2022-01-13 09:58:42,704 epoch 17 - iter 14/22 - loss 0.10450250 - samples/sec: 145.01 - lr: 0.100000\n",
      "2022-01-13 09:58:43,245 epoch 17 - iter 16/22 - loss 0.10576630 - samples/sec: 118.31 - lr: 0.100000\n",
      "2022-01-13 09:58:43,851 epoch 17 - iter 18/22 - loss 0.10576835 - samples/sec: 105.57 - lr: 0.100000\n",
      "2022-01-13 09:58:44,321 epoch 17 - iter 20/22 - loss 0.10643060 - samples/sec: 136.22 - lr: 0.100000\n",
      "2022-01-13 09:58:44,811 epoch 17 - iter 22/22 - loss 0.10504831 - samples/sec: 130.53 - lr: 0.100000\n",
      "2022-01-13 09:58:44,811 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-13 09:58:44,811 EPOCH 17 done: loss 0.1050 - lr 0.1000000\n",
      "2022-01-13 09:58:46,303 DEV : loss 0.07452813535928726 - f1-score (micro avg)  0.6717\n",
      "2022-01-13 09:58:46,331 BAD EPOCHS (no improvement): 2\n",
      "2022-01-13 09:58:46,331 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-13 09:58:46,829 epoch 18 - iter 2/22 - loss 0.11035407 - samples/sec: 130.74 - lr: 0.100000\n",
      "2022-01-13 09:58:47,273 epoch 18 - iter 4/22 - loss 0.09585444 - samples/sec: 144.40 - lr: 0.100000\n",
      "2022-01-13 09:58:47,748 epoch 18 - iter 6/22 - loss 0.09164532 - samples/sec: 134.52 - lr: 0.100000\n",
      "2022-01-13 09:58:48,157 epoch 18 - iter 8/22 - loss 0.10065306 - samples/sec: 156.53 - lr: 0.100000\n",
      "2022-01-13 09:58:48,644 epoch 18 - iter 10/22 - loss 0.10083437 - samples/sec: 131.49 - lr: 0.100000\n",
      "2022-01-13 09:58:49,137 epoch 18 - iter 12/22 - loss 0.09790305 - samples/sec: 129.77 - lr: 0.100000\n",
      "2022-01-13 09:58:49,609 epoch 18 - iter 14/22 - loss 0.09991501 - samples/sec: 135.56 - lr: 0.100000\n",
      "2022-01-13 09:58:50,184 epoch 18 - iter 16/22 - loss 0.10224612 - samples/sec: 111.36 - lr: 0.100000\n",
      "2022-01-13 09:58:50,739 epoch 18 - iter 18/22 - loss 0.10207237 - samples/sec: 115.33 - lr: 0.100000\n",
      "2022-01-13 09:58:51,169 epoch 18 - iter 20/22 - loss 0.10000050 - samples/sec: 148.84 - lr: 0.100000\n",
      "2022-01-13 09:58:51,641 epoch 18 - iter 22/22 - loss 0.10063697 - samples/sec: 135.48 - lr: 0.100000\n",
      "2022-01-13 09:58:51,641 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-13 09:58:51,641 EPOCH 18 done: loss 0.1006 - lr 0.1000000\n",
      "2022-01-13 09:58:53,383 DEV : loss 0.06539101898670197 - f1-score (micro avg)  0.674\n",
      "2022-01-13 09:58:53,542 BAD EPOCHS (no improvement): 3\n",
      "2022-01-13 09:58:53,542 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-13 09:58:54,085 epoch 19 - iter 2/22 - loss 0.10183275 - samples/sec: 117.91 - lr: 0.100000\n",
      "2022-01-13 09:58:54,497 epoch 19 - iter 4/22 - loss 0.11415342 - samples/sec: 155.55 - lr: 0.100000\n",
      "2022-01-13 09:58:55,087 epoch 19 - iter 6/22 - loss 0.10275577 - samples/sec: 108.40 - lr: 0.100000\n",
      "2022-01-13 09:58:55,567 epoch 19 - iter 8/22 - loss 0.11020343 - samples/sec: 133.60 - lr: 0.100000\n",
      "2022-01-13 09:58:56,822 epoch 19 - iter 10/22 - loss 0.10672153 - samples/sec: 51.02 - lr: 0.100000\n",
      "2022-01-13 09:58:57,950 epoch 19 - iter 12/22 - loss 0.10644298 - samples/sec: 56.69 - lr: 0.100000\n",
      "2022-01-13 09:58:58,957 epoch 19 - iter 14/22 - loss 0.10461412 - samples/sec: 63.58 - lr: 0.100000\n",
      "2022-01-13 09:59:00,218 epoch 19 - iter 16/22 - loss 0.10292633 - samples/sec: 50.77 - lr: 0.100000\n",
      "2022-01-13 09:59:01,219 epoch 19 - iter 18/22 - loss 0.09925386 - samples/sec: 63.95 - lr: 0.100000\n",
      "2022-01-13 09:59:02,294 epoch 19 - iter 20/22 - loss 0.10040818 - samples/sec: 59.54 - lr: 0.100000\n",
      "2022-01-13 09:59:03,361 epoch 19 - iter 22/22 - loss 0.10002954 - samples/sec: 59.94 - lr: 0.100000\n",
      "2022-01-13 09:59:03,361 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-13 09:59:03,361 EPOCH 19 done: loss 0.1000 - lr 0.1000000\n",
      "2022-01-13 09:59:05,161 DEV : loss 0.06979351490736008 - f1-score (micro avg)  0.6736\n",
      "Epoch    19: reducing learning rate of group 0 to 5.0000e-02.\n",
      "2022-01-13 09:59:05,207 BAD EPOCHS (no improvement): 4\n",
      "2022-01-13 09:59:05,207 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-13 09:59:05,647 epoch 20 - iter 2/22 - loss 0.08060447 - samples/sec: 145.69 - lr: 0.050000\n",
      "2022-01-13 09:59:06,090 epoch 20 - iter 4/22 - loss 0.09109266 - samples/sec: 144.17 - lr: 0.050000\n",
      "2022-01-13 09:59:06,760 epoch 20 - iter 6/22 - loss 0.09156270 - samples/sec: 95.61 - lr: 0.050000\n",
      "2022-01-13 09:59:07,184 epoch 20 - iter 8/22 - loss 0.09606816 - samples/sec: 150.90 - lr: 0.050000\n",
      "2022-01-13 09:59:07,609 epoch 20 - iter 10/22 - loss 0.09286312 - samples/sec: 150.54 - lr: 0.050000\n",
      "2022-01-13 09:59:08,127 epoch 20 - iter 12/22 - loss 0.09241743 - samples/sec: 123.48 - lr: 0.050000\n",
      "2022-01-13 09:59:08,630 epoch 20 - iter 14/22 - loss 0.09387432 - samples/sec: 127.46 - lr: 0.050000\n",
      "2022-01-13 09:59:09,258 epoch 20 - iter 16/22 - loss 0.09330058 - samples/sec: 101.88 - lr: 0.050000\n",
      "2022-01-13 09:59:10,077 epoch 20 - iter 18/22 - loss 0.09199270 - samples/sec: 78.15 - lr: 0.050000\n",
      "2022-01-13 09:59:10,967 epoch 20 - iter 20/22 - loss 0.09082686 - samples/sec: 71.91 - lr: 0.050000\n",
      "2022-01-13 09:59:11,433 epoch 20 - iter 22/22 - loss 0.09002040 - samples/sec: 137.39 - lr: 0.050000\n",
      "2022-01-13 09:59:11,440 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-13 09:59:11,440 EPOCH 20 done: loss 0.0900 - lr 0.0500000\n",
      "2022-01-13 09:59:13,433 DEV : loss 0.05431269109249115 - f1-score (micro avg)  0.7145\n",
      "2022-01-13 09:59:13,489 BAD EPOCHS (no improvement): 0\n",
      "2022-01-13 09:59:13,489 saving best model\n",
      "2022-01-13 09:59:18,344 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-13 09:59:18,967 epoch 21 - iter 2/22 - loss 0.08964626 - samples/sec: 104.13 - lr: 0.050000\n",
      "2022-01-13 09:59:19,450 epoch 21 - iter 4/22 - loss 0.09432175 - samples/sec: 132.52 - lr: 0.050000\n",
      "2022-01-13 09:59:19,943 epoch 21 - iter 6/22 - loss 0.09483136 - samples/sec: 130.09 - lr: 0.050000\n",
      "2022-01-13 09:59:20,376 epoch 21 - iter 8/22 - loss 0.09736150 - samples/sec: 147.77 - lr: 0.050000\n",
      "2022-01-13 09:59:20,941 epoch 21 - iter 10/22 - loss 0.09428345 - samples/sec: 113.31 - lr: 0.050000\n",
      "2022-01-13 09:59:21,349 epoch 21 - iter 12/22 - loss 0.09353444 - samples/sec: 156.77 - lr: 0.050000\n",
      "2022-01-13 09:59:21,835 epoch 21 - iter 14/22 - loss 0.09492227 - samples/sec: 131.60 - lr: 0.050000\n",
      "2022-01-13 09:59:22,286 epoch 21 - iter 16/22 - loss 0.09298478 - samples/sec: 142.04 - lr: 0.050000\n",
      "2022-01-13 09:59:22,694 epoch 21 - iter 18/22 - loss 0.09310921 - samples/sec: 162.88 - lr: 0.050000\n",
      "2022-01-13 09:59:23,195 epoch 21 - iter 20/22 - loss 0.09180272 - samples/sec: 127.80 - lr: 0.050000\n",
      "2022-01-13 09:59:23,603 epoch 21 - iter 22/22 - loss 0.09187057 - samples/sec: 157.10 - lr: 0.050000\n",
      "2022-01-13 09:59:23,603 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-13 09:59:23,603 EPOCH 21 done: loss 0.0919 - lr 0.0500000\n",
      "2022-01-13 09:59:25,024 DEV : loss 0.05247922241687775 - f1-score (micro avg)  0.6935\n",
      "2022-01-13 09:59:25,071 BAD EPOCHS (no improvement): 1\n",
      "2022-01-13 09:59:25,071 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-13 09:59:25,552 epoch 22 - iter 2/22 - loss 0.09088801 - samples/sec: 133.00 - lr: 0.050000\n",
      "2022-01-13 09:59:25,953 epoch 22 - iter 4/22 - loss 0.08170553 - samples/sec: 159.61 - lr: 0.050000\n",
      "2022-01-13 09:59:26,376 epoch 22 - iter 6/22 - loss 0.08900599 - samples/sec: 151.36 - lr: 0.050000\n",
      "2022-01-13 09:59:26,903 epoch 22 - iter 8/22 - loss 0.08935361 - samples/sec: 121.67 - lr: 0.050000\n",
      "2022-01-13 09:59:27,312 epoch 22 - iter 10/22 - loss 0.08822597 - samples/sec: 156.74 - lr: 0.050000\n",
      "2022-01-13 09:59:27,735 epoch 22 - iter 12/22 - loss 0.08721793 - samples/sec: 151.09 - lr: 0.050000\n",
      "2022-01-13 09:59:28,259 epoch 22 - iter 14/22 - loss 0.08767588 - samples/sec: 122.28 - lr: 0.050000\n",
      "2022-01-13 09:59:28,677 epoch 22 - iter 16/22 - loss 0.08956677 - samples/sec: 152.99 - lr: 0.050000\n",
      "2022-01-13 09:59:29,210 epoch 22 - iter 18/22 - loss 0.08964269 - samples/sec: 120.00 - lr: 0.050000\n",
      "2022-01-13 09:59:29,603 epoch 22 - iter 20/22 - loss 0.08979270 - samples/sec: 162.95 - lr: 0.050000\n",
      "2022-01-13 09:59:30,062 epoch 22 - iter 22/22 - loss 0.08954523 - samples/sec: 139.43 - lr: 0.050000\n",
      "2022-01-13 09:59:30,062 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-13 09:59:30,062 EPOCH 22 done: loss 0.0895 - lr 0.0500000\n",
      "2022-01-13 09:59:31,436 DEV : loss 0.05536437779664993 - f1-score (micro avg)  0.6797\n",
      "2022-01-13 09:59:31,467 BAD EPOCHS (no improvement): 2\n",
      "2022-01-13 09:59:31,467 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-13 09:59:32,002 epoch 23 - iter 2/22 - loss 0.08311001 - samples/sec: 123.33 - lr: 0.050000\n",
      "2022-01-13 09:59:32,410 epoch 23 - iter 4/22 - loss 0.08928493 - samples/sec: 156.80 - lr: 0.050000\n",
      "2022-01-13 09:59:32,927 epoch 23 - iter 6/22 - loss 0.08957270 - samples/sec: 123.74 - lr: 0.050000\n",
      "2022-01-13 09:59:33,344 epoch 23 - iter 8/22 - loss 0.08680567 - samples/sec: 153.52 - lr: 0.050000\n",
      "2022-01-13 09:59:33,816 epoch 23 - iter 10/22 - loss 0.08891165 - samples/sec: 135.79 - lr: 0.050000\n",
      "2022-01-13 09:59:34,400 epoch 23 - iter 12/22 - loss 0.08810806 - samples/sec: 109.43 - lr: 0.050000\n",
      "2022-01-13 09:59:35,096 epoch 23 - iter 14/22 - loss 0.08981478 - samples/sec: 91.95 - lr: 0.050000\n",
      "2022-01-13 09:59:35,682 epoch 23 - iter 16/22 - loss 0.08916021 - samples/sec: 109.40 - lr: 0.050000\n",
      "2022-01-13 09:59:36,293 epoch 23 - iter 18/22 - loss 0.08925844 - samples/sec: 104.68 - lr: 0.050000\n",
      "2022-01-13 09:59:37,023 epoch 23 - iter 20/22 - loss 0.09018733 - samples/sec: 88.67 - lr: 0.050000\n",
      "2022-01-13 09:59:38,066 epoch 23 - iter 22/22 - loss 0.08739784 - samples/sec: 61.84 - lr: 0.050000\n",
      "2022-01-13 09:59:38,066 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-13 09:59:38,066 EPOCH 23 done: loss 0.0874 - lr 0.0500000\n",
      "2022-01-13 09:59:40,281 DEV : loss 0.05241862311959267 - f1-score (micro avg)  0.7148\n",
      "2022-01-13 09:59:40,316 BAD EPOCHS (no improvement): 0\n",
      "2022-01-13 09:59:40,317 saving best model\n",
      "2022-01-13 09:59:45,474 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-13 09:59:46,000 epoch 24 - iter 2/22 - loss 0.08834192 - samples/sec: 122.73 - lr: 0.050000\n",
      "2022-01-13 09:59:46,678 epoch 24 - iter 4/22 - loss 0.09336128 - samples/sec: 94.67 - lr: 0.050000\n",
      "2022-01-13 09:59:47,397 epoch 24 - iter 6/22 - loss 0.09546388 - samples/sec: 89.42 - lr: 0.050000\n",
      "2022-01-13 09:59:47,865 epoch 24 - iter 8/22 - loss 0.09291240 - samples/sec: 136.80 - lr: 0.050000\n",
      "2022-01-13 09:59:48,530 epoch 24 - iter 10/22 - loss 0.08609470 - samples/sec: 96.78 - lr: 0.050000\n",
      "2022-01-13 09:59:49,200 epoch 24 - iter 12/22 - loss 0.08692919 - samples/sec: 95.60 - lr: 0.050000\n",
      "2022-01-13 09:59:49,709 epoch 24 - iter 14/22 - loss 0.08504282 - samples/sec: 126.25 - lr: 0.050000\n",
      "2022-01-13 09:59:50,265 epoch 24 - iter 16/22 - loss 0.08541113 - samples/sec: 115.31 - lr: 0.050000\n",
      "2022-01-13 09:59:50,673 epoch 24 - iter 18/22 - loss 0.08646533 - samples/sec: 157.29 - lr: 0.050000\n",
      "2022-01-13 09:59:51,159 epoch 24 - iter 20/22 - loss 0.08644299 - samples/sec: 132.22 - lr: 0.050000\n",
      "2022-01-13 09:59:51,608 epoch 24 - iter 22/22 - loss 0.08799018 - samples/sec: 143.28 - lr: 0.050000\n",
      "2022-01-13 09:59:51,613 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-13 09:59:51,614 EPOCH 24 done: loss 0.0880 - lr 0.0500000\n",
      "2022-01-13 09:59:53,027 DEV : loss 0.05213426426053047 - f1-score (micro avg)  0.688\n",
      "2022-01-13 09:59:53,059 BAD EPOCHS (no improvement): 1\n",
      "2022-01-13 09:59:53,059 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-13 09:59:53,463 epoch 25 - iter 2/22 - loss 0.11726699 - samples/sec: 159.96 - lr: 0.050000\n",
      "2022-01-13 09:59:53,918 epoch 25 - iter 4/22 - loss 0.09491145 - samples/sec: 140.70 - lr: 0.050000\n",
      "2022-01-13 09:59:54,314 epoch 25 - iter 6/22 - loss 0.10710520 - samples/sec: 161.37 - lr: 0.050000\n",
      "2022-01-13 09:59:54,873 epoch 25 - iter 8/22 - loss 0.10239528 - samples/sec: 114.64 - lr: 0.050000\n",
      "2022-01-13 09:59:55,303 epoch 25 - iter 10/22 - loss 0.09958476 - samples/sec: 148.69 - lr: 0.050000\n",
      "2022-01-13 09:59:55,803 epoch 25 - iter 12/22 - loss 0.09710320 - samples/sec: 128.06 - lr: 0.050000\n",
      "2022-01-13 09:59:56,322 epoch 25 - iter 14/22 - loss 0.09857204 - samples/sec: 125.62 - lr: 0.050000\n",
      "2022-01-13 09:59:56,812 epoch 25 - iter 16/22 - loss 0.09967770 - samples/sec: 130.71 - lr: 0.050000\n",
      "2022-01-13 09:59:57,219 epoch 25 - iter 18/22 - loss 0.09888379 - samples/sec: 157.05 - lr: 0.050000\n",
      "2022-01-13 09:59:57,664 epoch 25 - iter 20/22 - loss 0.09794868 - samples/sec: 143.95 - lr: 0.050000\n",
      "2022-01-13 09:59:58,120 epoch 25 - iter 22/22 - loss 0.09551349 - samples/sec: 140.45 - lr: 0.050000\n",
      "2022-01-13 09:59:58,135 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-13 09:59:58,135 EPOCH 25 done: loss 0.0955 - lr 0.0500000\n",
      "2022-01-13 09:59:59,596 DEV : loss 0.05173986777663231 - f1-score (micro avg)  0.6731\n",
      "2022-01-13 09:59:59,628 BAD EPOCHS (no improvement): 2\n",
      "2022-01-13 09:59:59,631 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-13 10:00:00,121 epoch 26 - iter 2/22 - loss 0.07007993 - samples/sec: 130.97 - lr: 0.050000\n",
      "2022-01-13 10:00:00,645 epoch 26 - iter 4/22 - loss 0.07858070 - samples/sec: 122.45 - lr: 0.050000\n",
      "2022-01-13 10:00:01,086 epoch 26 - iter 6/22 - loss 0.07723140 - samples/sec: 145.92 - lr: 0.050000\n",
      "2022-01-13 10:00:01,540 epoch 26 - iter 8/22 - loss 0.08219962 - samples/sec: 141.39 - lr: 0.050000\n",
      "2022-01-13 10:00:02,045 epoch 26 - iter 10/22 - loss 0.08405783 - samples/sec: 127.10 - lr: 0.050000\n",
      "2022-01-13 10:00:02,552 epoch 26 - iter 12/22 - loss 0.08357298 - samples/sec: 126.48 - lr: 0.050000\n",
      "2022-01-13 10:00:03,133 epoch 26 - iter 14/22 - loss 0.08275571 - samples/sec: 110.40 - lr: 0.050000\n",
      "2022-01-13 10:00:03,504 epoch 26 - iter 16/22 - loss 0.08385449 - samples/sec: 173.20 - lr: 0.050000\n",
      "2022-01-13 10:00:03,976 epoch 26 - iter 18/22 - loss 0.08500184 - samples/sec: 136.10 - lr: 0.050000\n",
      "2022-01-13 10:00:04,389 epoch 26 - iter 20/22 - loss 0.08566746 - samples/sec: 155.91 - lr: 0.050000\n",
      "2022-01-13 10:00:04,847 epoch 26 - iter 22/22 - loss 0.08723999 - samples/sec: 140.00 - lr: 0.050000\n",
      "2022-01-13 10:00:04,848 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-13 10:00:04,849 EPOCH 26 done: loss 0.0872 - lr 0.0500000\n",
      "2022-01-13 10:00:06,556 DEV : loss 0.05222967267036438 - f1-score (micro avg)  0.7474\n",
      "2022-01-13 10:00:06,594 BAD EPOCHS (no improvement): 0\n",
      "2022-01-13 10:00:06,596 saving best model\n",
      "2022-01-13 10:00:11,063 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-13 10:00:11,542 epoch 27 - iter 2/22 - loss 0.08191173 - samples/sec: 136.24 - lr: 0.050000\n",
      "2022-01-13 10:00:12,022 epoch 27 - iter 4/22 - loss 0.09077069 - samples/sec: 133.31 - lr: 0.050000\n",
      "2022-01-13 10:00:12,467 epoch 27 - iter 6/22 - loss 0.08752265 - samples/sec: 143.70 - lr: 0.050000\n",
      "2022-01-13 10:00:13,027 epoch 27 - iter 8/22 - loss 0.08702004 - samples/sec: 115.32 - lr: 0.050000\n",
      "2022-01-13 10:00:13,467 epoch 27 - iter 10/22 - loss 0.08615127 - samples/sec: 145.29 - lr: 0.050000\n",
      "2022-01-13 10:00:13,947 epoch 27 - iter 12/22 - loss 0.08680082 - samples/sec: 133.33 - lr: 0.050000\n",
      "2022-01-13 10:00:14,441 epoch 27 - iter 14/22 - loss 0.08603040 - samples/sec: 129.91 - lr: 0.050000\n",
      "2022-01-13 10:00:14,992 epoch 27 - iter 16/22 - loss 0.08712510 - samples/sec: 117.17 - lr: 0.050000\n",
      "2022-01-13 10:00:15,474 epoch 27 - iter 18/22 - loss 0.08504538 - samples/sec: 132.68 - lr: 0.050000\n",
      "2022-01-13 10:00:15,985 epoch 27 - iter 20/22 - loss 0.08553809 - samples/sec: 126.19 - lr: 0.050000\n",
      "2022-01-13 10:00:16,411 epoch 27 - iter 22/22 - loss 0.08493870 - samples/sec: 149.96 - lr: 0.050000\n",
      "2022-01-13 10:00:16,411 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-13 10:00:16,411 EPOCH 27 done: loss 0.0849 - lr 0.0500000\n",
      "2022-01-13 10:00:17,939 DEV : loss 0.050695858895778656 - f1-score (micro avg)  0.7481\n",
      "2022-01-13 10:00:17,978 BAD EPOCHS (no improvement): 0\n",
      "2022-01-13 10:00:17,978 saving best model\n",
      "2022-01-13 10:00:23,221 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-13 10:00:23,799 epoch 28 - iter 2/22 - loss 0.08455693 - samples/sec: 110.67 - lr: 0.050000\n",
      "2022-01-13 10:00:24,245 epoch 28 - iter 4/22 - loss 0.08301359 - samples/sec: 143.70 - lr: 0.050000\n",
      "2022-01-13 10:00:24,789 epoch 28 - iter 6/22 - loss 0.08145305 - samples/sec: 117.49 - lr: 0.050000\n",
      "2022-01-13 10:00:25,394 epoch 28 - iter 8/22 - loss 0.08779383 - samples/sec: 106.71 - lr: 0.050000\n",
      "2022-01-13 10:00:25,869 epoch 28 - iter 10/22 - loss 0.08984217 - samples/sec: 136.04 - lr: 0.050000\n",
      "2022-01-13 10:00:26,532 epoch 28 - iter 12/22 - loss 0.08800458 - samples/sec: 97.34 - lr: 0.050000\n",
      "2022-01-13 10:00:27,165 epoch 28 - iter 14/22 - loss 0.08827690 - samples/sec: 101.11 - lr: 0.050000\n",
      "2022-01-13 10:00:27,559 epoch 28 - iter 16/22 - loss 0.08807542 - samples/sec: 162.42 - lr: 0.050000\n",
      "2022-01-13 10:00:28,061 epoch 28 - iter 18/22 - loss 0.08618484 - samples/sec: 127.41 - lr: 0.050000\n",
      "2022-01-13 10:00:28,516 epoch 28 - iter 20/22 - loss 0.08537803 - samples/sec: 140.85 - lr: 0.050000\n",
      "2022-01-13 10:00:29,079 epoch 28 - iter 22/22 - loss 0.08479470 - samples/sec: 113.59 - lr: 0.050000\n",
      "2022-01-13 10:00:29,095 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-13 10:00:29,095 EPOCH 28 done: loss 0.0848 - lr 0.0500000\n",
      "2022-01-13 10:00:30,531 DEV : loss 0.048471029847860336 - f1-score (micro avg)  0.735\n",
      "2022-01-13 10:00:30,575 BAD EPOCHS (no improvement): 1\n",
      "2022-01-13 10:00:30,671 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-13 10:00:31,125 epoch 29 - iter 2/22 - loss 0.10057118 - samples/sec: 141.10 - lr: 0.050000\n",
      "2022-01-13 10:00:31,635 epoch 29 - iter 4/22 - loss 0.08564688 - samples/sec: 129.42 - lr: 0.050000\n",
      "2022-01-13 10:00:32,075 epoch 29 - iter 6/22 - loss 0.08149972 - samples/sec: 145.36 - lr: 0.050000\n",
      "2022-01-13 10:00:32,494 epoch 29 - iter 8/22 - loss 0.08194251 - samples/sec: 152.76 - lr: 0.050000\n",
      "2022-01-13 10:00:32,982 epoch 29 - iter 10/22 - loss 0.08202571 - samples/sec: 131.10 - lr: 0.050000\n",
      "2022-01-13 10:00:33,502 epoch 29 - iter 12/22 - loss 0.08436180 - samples/sec: 123.06 - lr: 0.050000\n",
      "2022-01-13 10:00:33,910 epoch 29 - iter 14/22 - loss 0.08381045 - samples/sec: 157.34 - lr: 0.050000\n",
      "2022-01-13 10:00:34,372 epoch 29 - iter 16/22 - loss 0.08292199 - samples/sec: 138.50 - lr: 0.050000\n",
      "2022-01-13 10:00:34,888 epoch 29 - iter 18/22 - loss 0.08306016 - samples/sec: 125.45 - lr: 0.050000\n",
      "2022-01-13 10:00:35,339 epoch 29 - iter 20/22 - loss 0.08330034 - samples/sec: 142.44 - lr: 0.050000\n",
      "2022-01-13 10:00:35,811 epoch 29 - iter 22/22 - loss 0.08338262 - samples/sec: 135.65 - lr: 0.050000\n",
      "2022-01-13 10:00:35,811 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-13 10:00:35,811 EPOCH 29 done: loss 0.0834 - lr 0.0500000\n",
      "2022-01-13 10:00:37,598 DEV : loss 0.04703386873006821 - f1-score (micro avg)  0.7538\n",
      "2022-01-13 10:00:37,637 BAD EPOCHS (no improvement): 0\n",
      "2022-01-13 10:00:37,639 saving best model\n",
      "2022-01-13 10:00:42,918 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-13 10:00:43,433 epoch 30 - iter 2/22 - loss 0.09677531 - samples/sec: 124.33 - lr: 0.050000\n",
      "2022-01-13 10:00:43,982 epoch 30 - iter 4/22 - loss 0.07916826 - samples/sec: 116.56 - lr: 0.050000\n",
      "2022-01-13 10:00:44,420 epoch 30 - iter 6/22 - loss 0.07780364 - samples/sec: 146.09 - lr: 0.050000\n",
      "2022-01-13 10:00:44,909 epoch 30 - iter 8/22 - loss 0.07876713 - samples/sec: 130.91 - lr: 0.050000\n",
      "2022-01-13 10:00:45,443 epoch 30 - iter 10/22 - loss 0.07618776 - samples/sec: 119.85 - lr: 0.050000\n",
      "2022-01-13 10:00:45,978 epoch 30 - iter 12/22 - loss 0.07759093 - samples/sec: 123.20 - lr: 0.050000\n",
      "2022-01-13 10:00:46,440 epoch 30 - iter 14/22 - loss 0.08002243 - samples/sec: 138.44 - lr: 0.050000\n",
      "2022-01-13 10:00:46,947 epoch 30 - iter 16/22 - loss 0.07928599 - samples/sec: 126.49 - lr: 0.050000\n",
      "2022-01-13 10:00:47,381 epoch 30 - iter 18/22 - loss 0.07778814 - samples/sec: 148.40 - lr: 0.050000\n",
      "2022-01-13 10:00:47,858 epoch 30 - iter 20/22 - loss 0.07975041 - samples/sec: 138.52 - lr: 0.050000\n",
      "2022-01-13 10:00:48,264 epoch 30 - iter 22/22 - loss 0.07985806 - samples/sec: 157.77 - lr: 0.050000\n",
      "2022-01-13 10:00:48,264 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-13 10:00:48,264 EPOCH 30 done: loss 0.0799 - lr 0.0500000\n",
      "2022-01-13 10:00:49,740 DEV : loss 0.04588600993156433 - f1-score (micro avg)  0.768\n",
      "2022-01-13 10:00:49,785 BAD EPOCHS (no improvement): 0\n",
      "2022-01-13 10:00:49,785 saving best model\n",
      "2022-01-13 10:00:54,965 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-13 10:00:55,370 epoch 31 - iter 2/22 - loss 0.08091478 - samples/sec: 158.10 - lr: 0.050000\n",
      "2022-01-13 10:00:55,894 epoch 31 - iter 4/22 - loss 0.08233403 - samples/sec: 122.10 - lr: 0.050000\n",
      "2022-01-13 10:00:56,306 epoch 31 - iter 6/22 - loss 0.08043601 - samples/sec: 155.44 - lr: 0.050000\n",
      "2022-01-13 10:00:56,754 epoch 31 - iter 8/22 - loss 0.08051791 - samples/sec: 142.60 - lr: 0.050000\n",
      "2022-01-13 10:00:57,244 epoch 31 - iter 10/22 - loss 0.07994591 - samples/sec: 130.76 - lr: 0.050000\n",
      "2022-01-13 10:00:57,708 epoch 31 - iter 12/22 - loss 0.07980046 - samples/sec: 138.01 - lr: 0.050000\n",
      "2022-01-13 10:00:58,202 epoch 31 - iter 14/22 - loss 0.08371412 - samples/sec: 133.76 - lr: 0.050000\n",
      "2022-01-13 10:00:58,593 epoch 31 - iter 16/22 - loss 0.08025455 - samples/sec: 163.40 - lr: 0.050000\n",
      "2022-01-13 10:00:59,137 epoch 31 - iter 18/22 - loss 0.07893220 - samples/sec: 117.70 - lr: 0.050000\n",
      "2022-01-13 10:00:59,590 epoch 31 - iter 20/22 - loss 0.08228976 - samples/sec: 141.49 - lr: 0.050000\n",
      "2022-01-13 10:01:00,109 epoch 31 - iter 22/22 - loss 0.08176621 - samples/sec: 129.55 - lr: 0.050000\n",
      "2022-01-13 10:01:00,109 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-13 10:01:00,109 EPOCH 31 done: loss 0.0818 - lr 0.0500000\n",
      "2022-01-13 10:01:01,848 DEV : loss 0.04740128293633461 - f1-score (micro avg)  0.7547\n",
      "2022-01-13 10:01:01,885 BAD EPOCHS (no improvement): 1\n",
      "2022-01-13 10:01:01,896 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-13 10:01:02,338 epoch 32 - iter 2/22 - loss 0.07397286 - samples/sec: 145.28 - lr: 0.050000\n",
      "2022-01-13 10:01:02,840 epoch 32 - iter 4/22 - loss 0.08474711 - samples/sec: 127.53 - lr: 0.050000\n",
      "2022-01-13 10:01:03,290 epoch 32 - iter 6/22 - loss 0.08724198 - samples/sec: 142.30 - lr: 0.050000\n",
      "2022-01-13 10:01:03,799 epoch 32 - iter 8/22 - loss 0.09089503 - samples/sec: 125.85 - lr: 0.050000\n",
      "2022-01-13 10:01:04,282 epoch 32 - iter 10/22 - loss 0.09137550 - samples/sec: 132.52 - lr: 0.050000\n",
      "2022-01-13 10:01:04,676 epoch 32 - iter 12/22 - loss 0.08771540 - samples/sec: 162.46 - lr: 0.050000\n",
      "2022-01-13 10:01:05,192 epoch 32 - iter 14/22 - loss 0.08697640 - samples/sec: 127.75 - lr: 0.050000\n",
      "2022-01-13 10:01:05,641 epoch 32 - iter 16/22 - loss 0.08674947 - samples/sec: 143.10 - lr: 0.050000\n",
      "2022-01-13 10:01:06,152 epoch 32 - iter 18/22 - loss 0.08679465 - samples/sec: 129.02 - lr: 0.050000\n",
      "2022-01-13 10:01:06,607 epoch 32 - iter 20/22 - loss 0.08574958 - samples/sec: 140.60 - lr: 0.050000\n",
      "2022-01-13 10:01:07,117 epoch 32 - iter 22/22 - loss 0.08498013 - samples/sec: 125.41 - lr: 0.050000\n",
      "2022-01-13 10:01:07,117 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-13 10:01:07,133 EPOCH 32 done: loss 0.0850 - lr 0.0500000\n",
      "2022-01-13 10:01:08,485 DEV : loss 0.04653237387537956 - f1-score (micro avg)  0.7165\n",
      "2022-01-13 10:01:08,533 BAD EPOCHS (no improvement): 2\n",
      "2022-01-13 10:01:08,533 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-13 10:01:08,921 epoch 33 - iter 2/22 - loss 0.08008198 - samples/sec: 171.22 - lr: 0.050000\n",
      "2022-01-13 10:01:09,390 epoch 33 - iter 4/22 - loss 0.08029362 - samples/sec: 136.39 - lr: 0.050000\n",
      "2022-01-13 10:01:09,870 epoch 33 - iter 6/22 - loss 0.08461674 - samples/sec: 137.28 - lr: 0.050000\n",
      "2022-01-13 10:01:10,345 epoch 33 - iter 8/22 - loss 0.08087096 - samples/sec: 134.76 - lr: 0.050000\n",
      "2022-01-13 10:01:10,761 epoch 33 - iter 10/22 - loss 0.07631695 - samples/sec: 153.88 - lr: 0.050000\n",
      "2022-01-13 10:01:11,348 epoch 33 - iter 12/22 - loss 0.07809126 - samples/sec: 108.87 - lr: 0.050000\n",
      "2022-01-13 10:01:11,828 epoch 33 - iter 14/22 - loss 0.07870537 - samples/sec: 133.50 - lr: 0.050000\n",
      "2022-01-13 10:01:12,452 epoch 33 - iter 16/22 - loss 0.07960379 - samples/sec: 102.59 - lr: 0.050000\n",
      "2022-01-13 10:01:13,108 epoch 33 - iter 18/22 - loss 0.07917312 - samples/sec: 97.56 - lr: 0.050000\n",
      "2022-01-13 10:01:13,512 epoch 33 - iter 20/22 - loss 0.08230211 - samples/sec: 158.31 - lr: 0.050000\n",
      "2022-01-13 10:01:14,014 epoch 33 - iter 22/22 - loss 0.08134972 - samples/sec: 127.58 - lr: 0.050000\n",
      "2022-01-13 10:01:14,014 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-13 10:01:14,014 EPOCH 33 done: loss 0.0813 - lr 0.0500000\n",
      "2022-01-13 10:01:15,983 DEV : loss 0.04585075005888939 - f1-score (micro avg)  0.7385\n",
      "2022-01-13 10:01:16,030 BAD EPOCHS (no improvement): 3\n",
      "2022-01-13 10:01:16,030 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-13 10:01:16,810 epoch 34 - iter 2/22 - loss 0.09174558 - samples/sec: 82.97 - lr: 0.050000\n",
      "2022-01-13 10:01:17,263 epoch 34 - iter 4/22 - loss 0.08354282 - samples/sec: 141.24 - lr: 0.050000\n",
      "2022-01-13 10:01:17,759 epoch 34 - iter 6/22 - loss 0.07372543 - samples/sec: 129.38 - lr: 0.050000\n",
      "2022-01-13 10:01:18,333 epoch 34 - iter 8/22 - loss 0.07689645 - samples/sec: 111.53 - lr: 0.050000\n",
      "2022-01-13 10:01:18,850 epoch 34 - iter 10/22 - loss 0.07595897 - samples/sec: 123.69 - lr: 0.050000\n",
      "2022-01-13 10:01:19,399 epoch 34 - iter 12/22 - loss 0.07816731 - samples/sec: 117.20 - lr: 0.050000\n",
      "2022-01-13 10:01:19,878 epoch 34 - iter 14/22 - loss 0.07795422 - samples/sec: 133.62 - lr: 0.050000\n",
      "2022-01-13 10:01:20,342 epoch 34 - iter 16/22 - loss 0.07892819 - samples/sec: 138.16 - lr: 0.050000\n",
      "2022-01-13 10:01:20,864 epoch 34 - iter 18/22 - loss 0.07863683 - samples/sec: 122.60 - lr: 0.050000\n",
      "2022-01-13 10:01:21,332 epoch 34 - iter 20/22 - loss 0.08034334 - samples/sec: 136.92 - lr: 0.050000\n",
      "2022-01-13 10:01:21,822 epoch 34 - iter 22/22 - loss 0.08021214 - samples/sec: 130.47 - lr: 0.050000\n",
      "2022-01-13 10:01:21,822 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-13 10:01:21,822 EPOCH 34 done: loss 0.0802 - lr 0.0500000\n",
      "2022-01-13 10:01:23,593 DEV : loss 0.04486033320426941 - f1-score (micro avg)  0.7824\n",
      "2022-01-13 10:01:23,630 BAD EPOCHS (no improvement): 0\n",
      "2022-01-13 10:01:23,630 saving best model\n",
      "2022-01-13 10:01:28,362 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-13 10:01:29,309 epoch 35 - iter 2/22 - loss 0.07146220 - samples/sec: 68.76 - lr: 0.050000\n",
      "2022-01-13 10:01:30,068 epoch 35 - iter 4/22 - loss 0.07315209 - samples/sec: 84.27 - lr: 0.050000\n",
      "2022-01-13 10:01:30,521 epoch 35 - iter 6/22 - loss 0.07361630 - samples/sec: 141.18 - lr: 0.050000\n",
      "2022-01-13 10:01:31,030 epoch 35 - iter 8/22 - loss 0.07128038 - samples/sec: 125.86 - lr: 0.050000\n",
      "2022-01-13 10:01:31,751 epoch 35 - iter 10/22 - loss 0.07208194 - samples/sec: 88.75 - lr: 0.050000\n",
      "2022-01-13 10:01:32,249 epoch 35 - iter 12/22 - loss 0.07302430 - samples/sec: 128.46 - lr: 0.050000\n",
      "2022-01-13 10:01:32,742 epoch 35 - iter 14/22 - loss 0.07294718 - samples/sec: 129.84 - lr: 0.050000\n",
      "2022-01-13 10:01:33,341 epoch 35 - iter 16/22 - loss 0.07555125 - samples/sec: 107.05 - lr: 0.050000\n",
      "2022-01-13 10:01:34,110 epoch 35 - iter 18/22 - loss 0.07698740 - samples/sec: 83.42 - lr: 0.050000\n",
      "2022-01-13 10:01:34,527 epoch 35 - iter 20/22 - loss 0.07811478 - samples/sec: 153.56 - lr: 0.050000\n",
      "2022-01-13 10:01:34,997 epoch 35 - iter 22/22 - loss 0.07881498 - samples/sec: 136.18 - lr: 0.050000\n",
      "2022-01-13 10:01:34,997 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-13 10:01:34,997 EPOCH 35 done: loss 0.0788 - lr 0.0500000\n",
      "2022-01-13 10:01:36,793 DEV : loss 0.04303113743662834 - f1-score (micro avg)  0.7994\n",
      "2022-01-13 10:01:36,865 BAD EPOCHS (no improvement): 0\n",
      "2022-01-13 10:01:36,865 saving best model\n",
      "2022-01-13 10:01:43,383 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-13 10:01:44,027 epoch 36 - iter 2/22 - loss 0.08249319 - samples/sec: 100.79 - lr: 0.050000\n",
      "2022-01-13 10:01:44,539 epoch 36 - iter 4/22 - loss 0.06921552 - samples/sec: 125.00 - lr: 0.050000\n",
      "2022-01-13 10:01:45,053 epoch 36 - iter 6/22 - loss 0.06493884 - samples/sec: 124.59 - lr: 0.050000\n",
      "2022-01-13 10:01:45,570 epoch 36 - iter 8/22 - loss 0.06666202 - samples/sec: 123.74 - lr: 0.050000\n",
      "2022-01-13 10:01:46,096 epoch 36 - iter 10/22 - loss 0.07296256 - samples/sec: 121.64 - lr: 0.050000\n",
      "2022-01-13 10:01:46,576 epoch 36 - iter 12/22 - loss 0.07756460 - samples/sec: 133.34 - lr: 0.050000\n",
      "2022-01-13 10:01:47,113 epoch 36 - iter 14/22 - loss 0.07841105 - samples/sec: 119.16 - lr: 0.050000\n",
      "2022-01-13 10:01:47,629 epoch 36 - iter 16/22 - loss 0.07850337 - samples/sec: 124.42 - lr: 0.050000\n",
      "2022-01-13 10:01:48,109 epoch 36 - iter 18/22 - loss 0.08018672 - samples/sec: 133.09 - lr: 0.050000\n",
      "2022-01-13 10:01:48,509 epoch 36 - iter 20/22 - loss 0.08032005 - samples/sec: 160.22 - lr: 0.050000\n",
      "2022-01-13 10:01:48,957 epoch 36 - iter 22/22 - loss 0.07971263 - samples/sec: 142.91 - lr: 0.050000\n",
      "2022-01-13 10:01:48,957 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-13 10:01:48,957 EPOCH 36 done: loss 0.0797 - lr 0.0500000\n",
      "2022-01-13 10:01:50,503 DEV : loss 0.042255766689777374 - f1-score (micro avg)  0.8006\n",
      "2022-01-13 10:01:50,542 BAD EPOCHS (no improvement): 0\n",
      "2022-01-13 10:01:50,542 saving best model\n",
      "2022-01-13 10:01:55,381 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-13 10:01:55,839 epoch 37 - iter 2/22 - loss 0.09182394 - samples/sec: 139.70 - lr: 0.050000\n",
      "2022-01-13 10:01:56,293 epoch 37 - iter 4/22 - loss 0.09476281 - samples/sec: 145.97 - lr: 0.050000\n",
      "2022-01-13 10:01:56,765 epoch 37 - iter 6/22 - loss 0.08316363 - samples/sec: 135.61 - lr: 0.050000\n",
      "2022-01-13 10:01:57,289 epoch 37 - iter 8/22 - loss 0.07739422 - samples/sec: 122.11 - lr: 0.050000\n",
      "2022-01-13 10:01:57,781 epoch 37 - iter 10/22 - loss 0.07876915 - samples/sec: 130.27 - lr: 0.050000\n",
      "2022-01-13 10:01:58,282 epoch 37 - iter 12/22 - loss 0.07757055 - samples/sec: 127.74 - lr: 0.050000\n",
      "2022-01-13 10:01:58,784 epoch 37 - iter 14/22 - loss 0.07633521 - samples/sec: 127.52 - lr: 0.050000\n",
      "2022-01-13 10:01:59,309 epoch 37 - iter 16/22 - loss 0.07844410 - samples/sec: 123.63 - lr: 0.050000\n",
      "2022-01-13 10:01:59,816 epoch 37 - iter 18/22 - loss 0.07835364 - samples/sec: 126.23 - lr: 0.050000\n",
      "2022-01-13 10:02:00,471 epoch 37 - iter 20/22 - loss 0.07886642 - samples/sec: 97.77 - lr: 0.050000\n",
      "2022-01-13 10:02:00,945 epoch 37 - iter 22/22 - loss 0.08035723 - samples/sec: 134.90 - lr: 0.050000\n",
      "2022-01-13 10:02:00,945 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-13 10:02:00,945 EPOCH 37 done: loss 0.0804 - lr 0.0500000\n",
      "2022-01-13 10:02:02,642 DEV : loss 0.044005103409290314 - f1-score (micro avg)  0.7707\n",
      "2022-01-13 10:02:02,682 BAD EPOCHS (no improvement): 1\n",
      "2022-01-13 10:02:02,682 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-13 10:02:03,206 epoch 38 - iter 2/22 - loss 0.05071843 - samples/sec: 123.90 - lr: 0.050000\n",
      "2022-01-13 10:02:03,768 epoch 38 - iter 4/22 - loss 0.07340042 - samples/sec: 115.54 - lr: 0.050000\n",
      "2022-01-13 10:02:04,218 epoch 38 - iter 6/22 - loss 0.06948311 - samples/sec: 142.37 - lr: 0.050000\n",
      "2022-01-13 10:02:04,703 epoch 38 - iter 8/22 - loss 0.07019181 - samples/sec: 131.93 - lr: 0.050000\n",
      "2022-01-13 10:02:05,262 epoch 38 - iter 10/22 - loss 0.07128900 - samples/sec: 114.52 - lr: 0.050000\n",
      "2022-01-13 10:02:05,923 epoch 38 - iter 12/22 - loss 0.07565337 - samples/sec: 97.20 - lr: 0.050000\n",
      "2022-01-13 10:02:06,474 epoch 38 - iter 14/22 - loss 0.07770644 - samples/sec: 116.16 - lr: 0.050000\n",
      "2022-01-13 10:02:07,105 epoch 38 - iter 16/22 - loss 0.07687282 - samples/sec: 101.42 - lr: 0.050000\n",
      "2022-01-13 10:02:07,706 epoch 38 - iter 18/22 - loss 0.07635735 - samples/sec: 106.48 - lr: 0.050000\n",
      "2022-01-13 10:02:08,184 epoch 38 - iter 20/22 - loss 0.07489664 - samples/sec: 133.85 - lr: 0.050000\n",
      "2022-01-13 10:02:08,687 epoch 38 - iter 22/22 - loss 0.07606283 - samples/sec: 127.33 - lr: 0.050000\n",
      "2022-01-13 10:02:08,687 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-13 10:02:08,687 EPOCH 38 done: loss 0.0761 - lr 0.0500000\n",
      "2022-01-13 10:02:10,225 DEV : loss 0.0402568019926548 - f1-score (micro avg)  0.7688\n",
      "2022-01-13 10:02:10,272 BAD EPOCHS (no improvement): 2\n",
      "2022-01-13 10:02:10,272 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-13 10:02:10,775 epoch 39 - iter 2/22 - loss 0.06407402 - samples/sec: 127.33 - lr: 0.050000\n",
      "2022-01-13 10:02:11,292 epoch 39 - iter 4/22 - loss 0.07772410 - samples/sec: 123.69 - lr: 0.050000\n",
      "2022-01-13 10:02:11,850 epoch 39 - iter 6/22 - loss 0.07371223 - samples/sec: 114.77 - lr: 0.050000\n",
      "2022-01-13 10:02:12,383 epoch 39 - iter 8/22 - loss 0.07416063 - samples/sec: 120.10 - lr: 0.050000\n",
      "2022-01-13 10:02:12,900 epoch 39 - iter 10/22 - loss 0.07226164 - samples/sec: 123.84 - lr: 0.050000\n",
      "2022-01-13 10:02:13,387 epoch 39 - iter 12/22 - loss 0.07059407 - samples/sec: 131.41 - lr: 0.050000\n",
      "2022-01-13 10:02:13,922 epoch 39 - iter 14/22 - loss 0.07180740 - samples/sec: 119.54 - lr: 0.050000\n",
      "2022-01-13 10:02:14,331 epoch 39 - iter 16/22 - loss 0.07224349 - samples/sec: 156.64 - lr: 0.050000\n",
      "2022-01-13 10:02:14,809 epoch 39 - iter 18/22 - loss 0.07357023 - samples/sec: 133.95 - lr: 0.050000\n",
      "2022-01-13 10:02:15,342 epoch 39 - iter 20/22 - loss 0.07525580 - samples/sec: 120.00 - lr: 0.050000\n",
      "2022-01-13 10:02:15,775 epoch 39 - iter 22/22 - loss 0.07561788 - samples/sec: 147.84 - lr: 0.050000\n",
      "2022-01-13 10:02:15,775 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-13 10:02:15,775 EPOCH 39 done: loss 0.0756 - lr 0.0500000\n",
      "2022-01-13 10:02:17,441 DEV : loss 0.04309961199760437 - f1-score (micro avg)  0.7944\n",
      "2022-01-13 10:02:17,473 BAD EPOCHS (no improvement): 3\n",
      "2022-01-13 10:02:17,473 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-13 10:02:17,966 epoch 40 - iter 2/22 - loss 0.08429061 - samples/sec: 129.80 - lr: 0.050000\n",
      "2022-01-13 10:02:18,523 epoch 40 - iter 4/22 - loss 0.08125008 - samples/sec: 114.93 - lr: 0.050000\n",
      "2022-01-13 10:02:18,979 epoch 40 - iter 6/22 - loss 0.08624592 - samples/sec: 140.49 - lr: 0.050000\n",
      "2022-01-13 10:02:19,560 epoch 40 - iter 8/22 - loss 0.08902870 - samples/sec: 110.00 - lr: 0.050000\n",
      "2022-01-13 10:02:20,107 epoch 40 - iter 10/22 - loss 0.08764225 - samples/sec: 117.11 - lr: 0.050000\n",
      "2022-01-13 10:02:20,620 epoch 40 - iter 12/22 - loss 0.08287065 - samples/sec: 124.85 - lr: 0.050000\n",
      "2022-01-13 10:02:21,115 epoch 40 - iter 14/22 - loss 0.08056810 - samples/sec: 129.34 - lr: 0.050000\n",
      "2022-01-13 10:02:21,702 epoch 40 - iter 16/22 - loss 0.07804976 - samples/sec: 110.70 - lr: 0.050000\n",
      "2022-01-13 10:02:22,255 epoch 40 - iter 18/22 - loss 0.07874759 - samples/sec: 115.62 - lr: 0.050000\n",
      "2022-01-13 10:02:22,826 epoch 40 - iter 20/22 - loss 0.07797256 - samples/sec: 112.06 - lr: 0.050000\n",
      "2022-01-13 10:02:23,359 epoch 40 - iter 22/22 - loss 0.07914024 - samples/sec: 120.11 - lr: 0.050000\n",
      "2022-01-13 10:02:23,361 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-13 10:02:23,361 EPOCH 40 done: loss 0.0791 - lr 0.0500000\n",
      "2022-01-13 10:02:25,178 DEV : loss 0.03968120366334915 - f1-score (micro avg)  0.8066\n",
      "2022-01-13 10:02:25,226 BAD EPOCHS (no improvement): 0\n",
      "2022-01-13 10:02:25,226 saving best model\n",
      "2022-01-13 10:02:30,983 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-13 10:02:31,818 epoch 41 - iter 2/22 - loss 0.09025877 - samples/sec: 76.82 - lr: 0.050000\n",
      "2022-01-13 10:02:32,575 epoch 41 - iter 4/22 - loss 0.08715149 - samples/sec: 85.43 - lr: 0.050000\n",
      "2022-01-13 10:02:33,236 epoch 41 - iter 6/22 - loss 0.08067540 - samples/sec: 96.95 - lr: 0.050000\n",
      "2022-01-13 10:02:33,811 epoch 41 - iter 8/22 - loss 0.07832069 - samples/sec: 111.15 - lr: 0.050000\n",
      "2022-01-13 10:02:34,362 epoch 41 - iter 10/22 - loss 0.08082084 - samples/sec: 116.26 - lr: 0.050000\n",
      "2022-01-13 10:02:34,818 epoch 41 - iter 12/22 - loss 0.08358497 - samples/sec: 140.28 - lr: 0.050000\n",
      "2022-01-13 10:02:35,307 epoch 41 - iter 14/22 - loss 0.08289618 - samples/sec: 131.02 - lr: 0.050000\n",
      "2022-01-13 10:02:35,819 epoch 41 - iter 16/22 - loss 0.08242321 - samples/sec: 124.95 - lr: 0.050000\n",
      "2022-01-13 10:02:36,347 epoch 41 - iter 18/22 - loss 0.08145290 - samples/sec: 121.18 - lr: 0.050000\n",
      "2022-01-13 10:02:37,014 epoch 41 - iter 20/22 - loss 0.08096534 - samples/sec: 95.97 - lr: 0.050000\n",
      "2022-01-13 10:02:37,446 epoch 41 - iter 22/22 - loss 0.08069028 - samples/sec: 148.09 - lr: 0.050000\n",
      "2022-01-13 10:02:37,446 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-13 10:02:37,446 EPOCH 41 done: loss 0.0807 - lr 0.0500000\n",
      "2022-01-13 10:02:39,032 DEV : loss 0.04317927360534668 - f1-score (micro avg)  0.7495\n",
      "2022-01-13 10:02:39,080 BAD EPOCHS (no improvement): 1\n",
      "2022-01-13 10:02:39,080 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-13 10:02:39,647 epoch 42 - iter 2/22 - loss 0.10801843 - samples/sec: 112.85 - lr: 0.050000\n",
      "2022-01-13 10:02:40,318 epoch 42 - iter 4/22 - loss 0.09690512 - samples/sec: 95.36 - lr: 0.050000\n",
      "2022-01-13 10:02:40,801 epoch 42 - iter 6/22 - loss 0.08610458 - samples/sec: 132.58 - lr: 0.050000\n",
      "2022-01-13 10:02:41,273 epoch 42 - iter 8/22 - loss 0.07974656 - samples/sec: 137.92 - lr: 0.050000\n",
      "2022-01-13 10:02:41,769 epoch 42 - iter 10/22 - loss 0.07855308 - samples/sec: 131.10 - lr: 0.050000\n",
      "2022-01-13 10:02:42,356 epoch 42 - iter 12/22 - loss 0.07544194 - samples/sec: 108.98 - lr: 0.050000\n",
      "2022-01-13 10:02:42,804 epoch 42 - iter 14/22 - loss 0.07498038 - samples/sec: 142.91 - lr: 0.050000\n",
      "2022-01-13 10:02:43,347 epoch 42 - iter 16/22 - loss 0.07555167 - samples/sec: 119.62 - lr: 0.050000\n",
      "2022-01-13 10:02:43,778 epoch 42 - iter 18/22 - loss 0.07465327 - samples/sec: 148.51 - lr: 0.050000\n",
      "2022-01-13 10:02:44,331 epoch 42 - iter 20/22 - loss 0.07498113 - samples/sec: 115.84 - lr: 0.050000\n",
      "2022-01-13 10:02:44,739 epoch 42 - iter 22/22 - loss 0.07572932 - samples/sec: 156.86 - lr: 0.050000\n",
      "2022-01-13 10:02:44,739 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-13 10:02:44,747 EPOCH 42 done: loss 0.0757 - lr 0.0500000\n",
      "2022-01-13 10:02:46,218 DEV : loss 0.038353968411684036 - f1-score (micro avg)  0.8288\n",
      "2022-01-13 10:02:46,250 BAD EPOCHS (no improvement): 0\n",
      "2022-01-13 10:02:46,250 saving best model\n",
      "2022-01-13 10:02:51,228 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-13 10:02:51,771 epoch 43 - iter 2/22 - loss 0.08094729 - samples/sec: 119.65 - lr: 0.050000\n",
      "2022-01-13 10:02:52,372 epoch 43 - iter 4/22 - loss 0.06819283 - samples/sec: 107.81 - lr: 0.050000\n",
      "2022-01-13 10:02:52,851 epoch 43 - iter 6/22 - loss 0.06879217 - samples/sec: 134.02 - lr: 0.050000\n",
      "2022-01-13 10:02:53,410 epoch 43 - iter 8/22 - loss 0.06891567 - samples/sec: 114.38 - lr: 0.050000\n",
      "2022-01-13 10:02:54,077 epoch 43 - iter 10/22 - loss 0.07045375 - samples/sec: 96.05 - lr: 0.050000\n",
      "2022-01-13 10:02:54,469 epoch 43 - iter 12/22 - loss 0.07295097 - samples/sec: 163.05 - lr: 0.050000\n",
      "2022-01-13 10:02:54,987 epoch 43 - iter 14/22 - loss 0.07518950 - samples/sec: 123.49 - lr: 0.050000\n",
      "2022-01-13 10:02:55,610 epoch 43 - iter 16/22 - loss 0.07611962 - samples/sec: 102.81 - lr: 0.050000\n",
      "2022-01-13 10:02:56,273 epoch 43 - iter 18/22 - loss 0.07500595 - samples/sec: 96.53 - lr: 0.050000\n",
      "2022-01-13 10:02:56,791 epoch 43 - iter 20/22 - loss 0.07539934 - samples/sec: 123.42 - lr: 0.050000\n",
      "2022-01-13 10:02:57,238 epoch 43 - iter 22/22 - loss 0.07539324 - samples/sec: 148.46 - lr: 0.050000\n",
      "2022-01-13 10:02:57,238 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-13 10:02:57,238 EPOCH 43 done: loss 0.0754 - lr 0.0500000\n",
      "2022-01-13 10:02:59,123 DEV : loss 0.040304530411958694 - f1-score (micro avg)  0.8059\n",
      "2022-01-13 10:02:59,179 BAD EPOCHS (no improvement): 1\n",
      "2022-01-13 10:02:59,179 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-13 10:02:59,623 epoch 44 - iter 2/22 - loss 0.07882438 - samples/sec: 146.95 - lr: 0.050000\n",
      "2022-01-13 10:03:00,410 epoch 44 - iter 4/22 - loss 0.07890400 - samples/sec: 81.27 - lr: 0.050000\n",
      "2022-01-13 10:03:01,053 epoch 44 - iter 6/22 - loss 0.08129185 - samples/sec: 99.64 - lr: 0.050000\n",
      "2022-01-13 10:03:01,805 epoch 44 - iter 8/22 - loss 0.08223128 - samples/sec: 85.03 - lr: 0.050000\n",
      "2022-01-13 10:03:02,301 epoch 44 - iter 10/22 - loss 0.07766795 - samples/sec: 129.24 - lr: 0.050000\n",
      "2022-01-13 10:03:02,737 epoch 44 - iter 12/22 - loss 0.07736778 - samples/sec: 146.68 - lr: 0.050000\n",
      "2022-01-13 10:03:03,310 epoch 44 - iter 14/22 - loss 0.07653948 - samples/sec: 111.70 - lr: 0.050000\n",
      "2022-01-13 10:03:03,830 epoch 44 - iter 16/22 - loss 0.07919058 - samples/sec: 123.00 - lr: 0.050000\n",
      "2022-01-13 10:03:04,386 epoch 44 - iter 18/22 - loss 0.07746606 - samples/sec: 115.22 - lr: 0.050000\n",
      "2022-01-13 10:03:04,842 epoch 44 - iter 20/22 - loss 0.07719186 - samples/sec: 140.36 - lr: 0.050000\n",
      "2022-01-13 10:03:05,254 epoch 44 - iter 22/22 - loss 0.07758628 - samples/sec: 155.16 - lr: 0.050000\n",
      "2022-01-13 10:03:05,254 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-13 10:03:05,254 EPOCH 44 done: loss 0.0776 - lr 0.0500000\n",
      "2022-01-13 10:03:06,683 DEV : loss 0.039248015731573105 - f1-score (micro avg)  0.8111\n",
      "2022-01-13 10:03:06,714 BAD EPOCHS (no improvement): 2\n",
      "2022-01-13 10:03:06,714 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-13 10:03:07,170 epoch 45 - iter 2/22 - loss 0.07628675 - samples/sec: 141.47 - lr: 0.050000\n",
      "2022-01-13 10:03:07,640 epoch 45 - iter 4/22 - loss 0.07167225 - samples/sec: 136.33 - lr: 0.050000\n",
      "2022-01-13 10:03:08,186 epoch 45 - iter 6/22 - loss 0.06845803 - samples/sec: 117.29 - lr: 0.050000\n",
      "2022-01-13 10:03:08,634 epoch 45 - iter 8/22 - loss 0.07237922 - samples/sec: 142.70 - lr: 0.050000\n",
      "2022-01-13 10:03:09,133 epoch 45 - iter 10/22 - loss 0.06979819 - samples/sec: 128.14 - lr: 0.050000\n",
      "2022-01-13 10:03:09,685 epoch 45 - iter 12/22 - loss 0.06999478 - samples/sec: 115.96 - lr: 0.050000\n",
      "2022-01-13 10:03:10,201 epoch 45 - iter 14/22 - loss 0.07096526 - samples/sec: 124.30 - lr: 0.050000\n",
      "2022-01-13 10:03:10,662 epoch 45 - iter 16/22 - loss 0.07162462 - samples/sec: 143.77 - lr: 0.050000\n",
      "2022-01-13 10:03:11,178 epoch 45 - iter 18/22 - loss 0.07259017 - samples/sec: 124.00 - lr: 0.050000\n",
      "2022-01-13 10:03:11,624 epoch 45 - iter 20/22 - loss 0.07140848 - samples/sec: 146.20 - lr: 0.050000\n",
      "2022-01-13 10:03:12,184 epoch 45 - iter 22/22 - loss 0.07129262 - samples/sec: 114.32 - lr: 0.050000\n",
      "2022-01-13 10:03:12,184 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-13 10:03:12,184 EPOCH 45 done: loss 0.0713 - lr 0.0500000\n",
      "2022-01-13 10:03:13,848 DEV : loss 0.04050532355904579 - f1-score (micro avg)  0.7753\n",
      "2022-01-13 10:03:13,890 BAD EPOCHS (no improvement): 3\n",
      "2022-01-13 10:03:13,890 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-13 10:03:14,387 epoch 46 - iter 2/22 - loss 0.08088974 - samples/sec: 130.85 - lr: 0.050000\n",
      "2022-01-13 10:03:14,889 epoch 46 - iter 4/22 - loss 0.07347578 - samples/sec: 127.48 - lr: 0.050000\n",
      "2022-01-13 10:03:15,332 epoch 46 - iter 6/22 - loss 0.07587429 - samples/sec: 144.44 - lr: 0.050000\n",
      "2022-01-13 10:03:15,815 epoch 46 - iter 8/22 - loss 0.07818672 - samples/sec: 136.98 - lr: 0.050000\n",
      "2022-01-13 10:03:16,350 epoch 46 - iter 10/22 - loss 0.07343368 - samples/sec: 119.62 - lr: 0.050000\n",
      "2022-01-13 10:03:16,761 epoch 46 - iter 12/22 - loss 0.07452732 - samples/sec: 155.65 - lr: 0.050000\n",
      "2022-01-13 10:03:17,238 epoch 46 - iter 14/22 - loss 0.07390945 - samples/sec: 133.99 - lr: 0.050000\n",
      "2022-01-13 10:03:17,734 epoch 46 - iter 16/22 - loss 0.07649699 - samples/sec: 129.04 - lr: 0.050000\n",
      "2022-01-13 10:03:18,156 epoch 46 - iter 18/22 - loss 0.07749971 - samples/sec: 151.96 - lr: 0.050000\n",
      "2022-01-13 10:03:18,738 epoch 46 - iter 20/22 - loss 0.07837036 - samples/sec: 109.93 - lr: 0.050000\n",
      "2022-01-13 10:03:19,236 epoch 46 - iter 22/22 - loss 0.07808052 - samples/sec: 128.37 - lr: 0.050000\n",
      "2022-01-13 10:03:19,238 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-13 10:03:19,240 EPOCH 46 done: loss 0.0781 - lr 0.0500000\n",
      "2022-01-13 10:03:20,788 DEV : loss 0.03639667108654976 - f1-score (micro avg)  0.8448\n",
      "2022-01-13 10:03:20,830 BAD EPOCHS (no improvement): 0\n",
      "2022-01-13 10:03:20,830 saving best model\n",
      "2022-01-13 10:03:25,864 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-13 10:03:26,373 epoch 47 - iter 2/22 - loss 0.06198700 - samples/sec: 126.21 - lr: 0.050000\n",
      "2022-01-13 10:03:26,876 epoch 47 - iter 4/22 - loss 0.06626318 - samples/sec: 131.09 - lr: 0.050000\n",
      "2022-01-13 10:03:27,420 epoch 47 - iter 6/22 - loss 0.07211839 - samples/sec: 117.72 - lr: 0.050000\n",
      "2022-01-13 10:03:27,933 epoch 47 - iter 8/22 - loss 0.07660408 - samples/sec: 124.94 - lr: 0.050000\n",
      "2022-01-13 10:03:28,399 epoch 47 - iter 10/22 - loss 0.07559025 - samples/sec: 142.19 - lr: 0.050000\n",
      "2022-01-13 10:03:28,936 epoch 47 - iter 12/22 - loss 0.07556832 - samples/sec: 119.32 - lr: 0.050000\n",
      "2022-01-13 10:03:29,488 epoch 47 - iter 14/22 - loss 0.07598512 - samples/sec: 115.82 - lr: 0.050000\n",
      "2022-01-13 10:03:30,056 epoch 47 - iter 16/22 - loss 0.07360508 - samples/sec: 114.27 - lr: 0.050000\n",
      "2022-01-13 10:03:30,552 epoch 47 - iter 18/22 - loss 0.07428311 - samples/sec: 129.03 - lr: 0.050000\n",
      "2022-01-13 10:03:31,072 epoch 47 - iter 20/22 - loss 0.07393814 - samples/sec: 123.10 - lr: 0.050000\n",
      "2022-01-13 10:03:31,540 epoch 47 - iter 22/22 - loss 0.07332435 - samples/sec: 136.80 - lr: 0.050000\n",
      "2022-01-13 10:03:31,540 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-13 10:03:31,540 EPOCH 47 done: loss 0.0733 - lr 0.0500000\n",
      "2022-01-13 10:03:33,157 DEV : loss 0.03754464536905289 - f1-score (micro avg)  0.8029\n",
      "2022-01-13 10:03:33,197 BAD EPOCHS (no improvement): 1\n",
      "2022-01-13 10:03:33,197 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-13 10:03:33,693 epoch 48 - iter 2/22 - loss 0.06105033 - samples/sec: 128.92 - lr: 0.050000\n",
      "2022-01-13 10:03:34,281 epoch 48 - iter 4/22 - loss 0.06587923 - samples/sec: 110.43 - lr: 0.050000\n",
      "2022-01-13 10:03:34,826 epoch 48 - iter 6/22 - loss 0.06356570 - samples/sec: 117.44 - lr: 0.050000\n",
      "2022-01-13 10:03:35,479 epoch 48 - iter 8/22 - loss 0.06673356 - samples/sec: 100.33 - lr: 0.050000\n",
      "2022-01-13 10:03:36,059 epoch 48 - iter 10/22 - loss 0.06692718 - samples/sec: 110.26 - lr: 0.050000\n",
      "2022-01-13 10:03:36,625 epoch 48 - iter 12/22 - loss 0.06698228 - samples/sec: 113.07 - lr: 0.050000\n",
      "2022-01-13 10:03:37,307 epoch 48 - iter 14/22 - loss 0.06669973 - samples/sec: 95.97 - lr: 0.050000\n",
      "2022-01-13 10:03:37,700 epoch 48 - iter 16/22 - loss 0.06931121 - samples/sec: 163.01 - lr: 0.050000\n",
      "2022-01-13 10:03:38,332 epoch 48 - iter 18/22 - loss 0.07038928 - samples/sec: 103.83 - lr: 0.050000\n",
      "2022-01-13 10:03:38,861 epoch 48 - iter 20/22 - loss 0.07118247 - samples/sec: 120.81 - lr: 0.050000\n",
      "2022-01-13 10:03:39,444 epoch 48 - iter 22/22 - loss 0.07106100 - samples/sec: 109.94 - lr: 0.050000\n",
      "2022-01-13 10:03:39,459 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-13 10:03:39,459 EPOCH 48 done: loss 0.0711 - lr 0.0500000\n",
      "2022-01-13 10:03:41,116 DEV : loss 0.03426356613636017 - f1-score (micro avg)  0.8617\n",
      "2022-01-13 10:03:41,149 BAD EPOCHS (no improvement): 0\n",
      "2022-01-13 10:03:41,149 saving best model\n",
      "2022-01-13 10:03:45,461 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-13 10:03:45,978 epoch 49 - iter 2/22 - loss 0.05945110 - samples/sec: 127.09 - lr: 0.050000\n",
      "2022-01-13 10:03:46,559 epoch 49 - iter 4/22 - loss 0.07786721 - samples/sec: 110.09 - lr: 0.050000\n",
      "2022-01-13 10:03:47,543 epoch 49 - iter 6/22 - loss 0.07544377 - samples/sec: 65.03 - lr: 0.050000\n",
      "2022-01-13 10:03:48,073 epoch 49 - iter 8/22 - loss 0.07389952 - samples/sec: 122.58 - lr: 0.050000\n",
      "2022-01-13 10:03:48,519 epoch 49 - iter 10/22 - loss 0.07308344 - samples/sec: 148.95 - lr: 0.050000\n",
      "2022-01-13 10:03:49,027 epoch 49 - iter 12/22 - loss 0.07499227 - samples/sec: 126.49 - lr: 0.050000\n",
      "2022-01-13 10:03:49,461 epoch 49 - iter 14/22 - loss 0.07432321 - samples/sec: 147.41 - lr: 0.050000\n",
      "2022-01-13 10:03:50,237 epoch 49 - iter 16/22 - loss 0.07701925 - samples/sec: 82.48 - lr: 0.050000\n",
      "2022-01-13 10:03:50,855 epoch 49 - iter 18/22 - loss 0.07627714 - samples/sec: 106.24 - lr: 0.050000\n",
      "2022-01-13 10:03:51,515 epoch 49 - iter 20/22 - loss 0.07241061 - samples/sec: 96.96 - lr: 0.050000\n",
      "2022-01-13 10:03:52,041 epoch 49 - iter 22/22 - loss 0.07190764 - samples/sec: 121.66 - lr: 0.050000\n",
      "2022-01-13 10:03:52,041 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-13 10:03:52,041 EPOCH 49 done: loss 0.0719 - lr 0.0500000\n",
      "2022-01-13 10:03:53,601 DEV : loss 0.03392414003610611 - f1-score (micro avg)  0.843\n",
      "2022-01-13 10:03:53,647 BAD EPOCHS (no improvement): 1\n",
      "2022-01-13 10:03:53,647 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-13 10:03:54,191 epoch 50 - iter 2/22 - loss 0.06062667 - samples/sec: 118.96 - lr: 0.050000\n",
      "2022-01-13 10:03:54,628 epoch 50 - iter 4/22 - loss 0.06542544 - samples/sec: 146.37 - lr: 0.050000\n",
      "2022-01-13 10:03:55,163 epoch 50 - iter 6/22 - loss 0.06687390 - samples/sec: 123.30 - lr: 0.050000\n",
      "2022-01-13 10:03:55,619 epoch 50 - iter 8/22 - loss 0.07090142 - samples/sec: 140.19 - lr: 0.050000\n",
      "2022-01-13 10:03:56,313 epoch 50 - iter 10/22 - loss 0.07016111 - samples/sec: 93.32 - lr: 0.050000\n",
      "2022-01-13 10:03:56,833 epoch 50 - iter 12/22 - loss 0.07278357 - samples/sec: 122.99 - lr: 0.050000\n",
      "2022-01-13 10:03:57,321 epoch 50 - iter 14/22 - loss 0.07232766 - samples/sec: 131.12 - lr: 0.050000\n",
      "2022-01-13 10:03:57,825 epoch 50 - iter 16/22 - loss 0.07074211 - samples/sec: 127.13 - lr: 0.050000\n",
      "2022-01-13 10:03:58,395 epoch 50 - iter 18/22 - loss 0.07022993 - samples/sec: 112.26 - lr: 0.050000\n",
      "2022-01-13 10:03:58,876 epoch 50 - iter 20/22 - loss 0.07057252 - samples/sec: 133.10 - lr: 0.050000\n",
      "2022-01-13 10:03:59,449 epoch 50 - iter 22/22 - loss 0.07041224 - samples/sec: 111.66 - lr: 0.050000\n",
      "2022-01-13 10:03:59,449 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-13 10:03:59,449 EPOCH 50 done: loss 0.0704 - lr 0.0500000\n",
      "2022-01-13 10:04:01,017 DEV : loss 0.0333244614303112 - f1-score (micro avg)  0.8404\n",
      "2022-01-13 10:04:01,049 BAD EPOCHS (no improvement): 2\n",
      "2022-01-13 10:04:01,049 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-13 10:04:01,466 epoch 51 - iter 2/22 - loss 0.07452873 - samples/sec: 153.62 - lr: 0.050000\n",
      "2022-01-13 10:04:01,978 epoch 51 - iter 4/22 - loss 0.07232516 - samples/sec: 127.16 - lr: 0.050000\n",
      "2022-01-13 10:04:02,476 epoch 51 - iter 6/22 - loss 0.07365257 - samples/sec: 128.59 - lr: 0.050000\n",
      "2022-01-13 10:04:03,029 epoch 51 - iter 8/22 - loss 0.06809144 - samples/sec: 117.50 - lr: 0.050000\n",
      "2022-01-13 10:04:03,581 epoch 51 - iter 10/22 - loss 0.06490775 - samples/sec: 115.85 - lr: 0.050000\n",
      "2022-01-13 10:04:04,335 epoch 51 - iter 12/22 - loss 0.06642249 - samples/sec: 84.85 - lr: 0.050000\n",
      "2022-01-13 10:04:05,035 epoch 51 - iter 14/22 - loss 0.06599763 - samples/sec: 91.67 - lr: 0.050000\n",
      "2022-01-13 10:04:05,615 epoch 51 - iter 16/22 - loss 0.06608761 - samples/sec: 110.75 - lr: 0.050000\n",
      "2022-01-13 10:04:06,215 epoch 51 - iter 18/22 - loss 0.06675669 - samples/sec: 106.54 - lr: 0.050000\n",
      "2022-01-13 10:04:06,743 epoch 51 - iter 20/22 - loss 0.06675830 - samples/sec: 123.03 - lr: 0.050000\n",
      "2022-01-13 10:04:07,225 epoch 51 - iter 22/22 - loss 0.06770593 - samples/sec: 135.24 - lr: 0.050000\n",
      "2022-01-13 10:04:07,233 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-13 10:04:07,233 EPOCH 51 done: loss 0.0677 - lr 0.0500000\n",
      "2022-01-13 10:04:09,226 DEV : loss 0.03239589184522629 - f1-score (micro avg)  0.859\n",
      "2022-01-13 10:04:09,265 BAD EPOCHS (no improvement): 3\n",
      "2022-01-13 10:04:09,265 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-13 10:04:09,745 epoch 52 - iter 2/22 - loss 0.07949338 - samples/sec: 133.38 - lr: 0.050000\n",
      "2022-01-13 10:04:10,321 epoch 52 - iter 4/22 - loss 0.08315461 - samples/sec: 111.00 - lr: 0.050000\n",
      "2022-01-13 10:04:10,954 epoch 52 - iter 6/22 - loss 0.07218438 - samples/sec: 101.23 - lr: 0.050000\n",
      "2022-01-13 10:04:11,666 epoch 52 - iter 8/22 - loss 0.06878957 - samples/sec: 89.89 - lr: 0.050000\n",
      "2022-01-13 10:04:12,250 epoch 52 - iter 10/22 - loss 0.06944657 - samples/sec: 109.55 - lr: 0.050000\n",
      "2022-01-13 10:04:12,722 epoch 52 - iter 12/22 - loss 0.07006658 - samples/sec: 135.51 - lr: 0.050000\n",
      "2022-01-13 10:04:13,298 epoch 52 - iter 14/22 - loss 0.07102922 - samples/sec: 111.11 - lr: 0.050000\n",
      "2022-01-13 10:04:13,740 epoch 52 - iter 16/22 - loss 0.07190918 - samples/sec: 144.82 - lr: 0.050000\n",
      "2022-01-13 10:04:14,277 epoch 52 - iter 18/22 - loss 0.07062240 - samples/sec: 119.24 - lr: 0.050000\n",
      "2022-01-13 10:04:14,820 epoch 52 - iter 20/22 - loss 0.07038044 - samples/sec: 117.71 - lr: 0.050000\n",
      "2022-01-13 10:04:15,389 epoch 52 - iter 22/22 - loss 0.07253621 - samples/sec: 112.59 - lr: 0.050000\n",
      "2022-01-13 10:04:15,389 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-13 10:04:15,397 EPOCH 52 done: loss 0.0725 - lr 0.0500000\n",
      "2022-01-13 10:04:16,918 DEV : loss 0.03221532702445984 - f1-score (micro avg)  0.8457\n",
      "Epoch    52: reducing learning rate of group 0 to 2.5000e-02.\n",
      "2022-01-13 10:04:16,950 BAD EPOCHS (no improvement): 4\n",
      "2022-01-13 10:04:16,950 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-13 10:04:17,359 epoch 53 - iter 2/22 - loss 0.06401606 - samples/sec: 156.68 - lr: 0.025000\n",
      "2022-01-13 10:04:17,807 epoch 53 - iter 4/22 - loss 0.06278948 - samples/sec: 142.73 - lr: 0.025000\n",
      "2022-01-13 10:04:18,328 epoch 53 - iter 6/22 - loss 0.06219892 - samples/sec: 122.89 - lr: 0.025000\n",
      "2022-01-13 10:04:18,792 epoch 53 - iter 8/22 - loss 0.06686507 - samples/sec: 137.98 - lr: 0.025000\n",
      "2022-01-13 10:04:19,344 epoch 53 - iter 10/22 - loss 0.06277977 - samples/sec: 115.83 - lr: 0.025000\n",
      "2022-01-13 10:04:19,841 epoch 53 - iter 12/22 - loss 0.06074282 - samples/sec: 128.76 - lr: 0.025000\n",
      "2022-01-13 10:04:20,361 epoch 53 - iter 14/22 - loss 0.06227759 - samples/sec: 123.04 - lr: 0.025000\n",
      "2022-01-13 10:04:20,857 epoch 53 - iter 16/22 - loss 0.06394415 - samples/sec: 129.03 - lr: 0.025000\n",
      "2022-01-13 10:04:21,330 epoch 53 - iter 18/22 - loss 0.06683355 - samples/sec: 135.34 - lr: 0.025000\n",
      "2022-01-13 10:04:21,787 epoch 53 - iter 20/22 - loss 0.06620339 - samples/sec: 142.68 - lr: 0.025000\n",
      "2022-01-13 10:04:22,252 epoch 53 - iter 22/22 - loss 0.06618834 - samples/sec: 140.14 - lr: 0.025000\n",
      "2022-01-13 10:04:22,252 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-13 10:04:22,252 EPOCH 53 done: loss 0.0662 - lr 0.0250000\n",
      "2022-01-13 10:04:23,891 DEV : loss 0.03060731664299965 - f1-score (micro avg)  0.8763\n",
      "2022-01-13 10:04:23,923 BAD EPOCHS (no improvement): 0\n",
      "2022-01-13 10:04:23,923 saving best model\n",
      "2022-01-13 10:04:29,528 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-13 10:04:30,032 epoch 54 - iter 2/22 - loss 0.04349615 - samples/sec: 126.89 - lr: 0.025000\n",
      "2022-01-13 10:04:30,399 epoch 54 - iter 4/22 - loss 0.06129197 - samples/sec: 174.22 - lr: 0.025000\n",
      "2022-01-13 10:04:30,933 epoch 54 - iter 6/22 - loss 0.06368842 - samples/sec: 119.89 - lr: 0.025000\n",
      "2022-01-13 10:04:31,446 epoch 54 - iter 8/22 - loss 0.06168371 - samples/sec: 124.83 - lr: 0.025000\n",
      "2022-01-13 10:04:32,011 epoch 54 - iter 10/22 - loss 0.06675617 - samples/sec: 113.31 - lr: 0.025000\n",
      "2022-01-13 10:04:32,639 epoch 54 - iter 12/22 - loss 0.06761696 - samples/sec: 101.86 - lr: 0.025000\n",
      "2022-01-13 10:04:33,300 epoch 54 - iter 14/22 - loss 0.06727562 - samples/sec: 96.74 - lr: 0.025000\n",
      "2022-01-13 10:04:33,852 epoch 54 - iter 16/22 - loss 0.06513587 - samples/sec: 116.12 - lr: 0.025000\n",
      "2022-01-13 10:04:34,406 epoch 54 - iter 18/22 - loss 0.06553034 - samples/sec: 115.34 - lr: 0.025000\n",
      "2022-01-13 10:04:34,928 epoch 54 - iter 20/22 - loss 0.06606458 - samples/sec: 122.78 - lr: 0.025000\n",
      "2022-01-13 10:04:35,330 epoch 54 - iter 22/22 - loss 0.06723047 - samples/sec: 159.21 - lr: 0.025000\n",
      "2022-01-13 10:04:35,330 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-13 10:04:35,330 EPOCH 54 done: loss 0.0672 - lr 0.0250000\n",
      "2022-01-13 10:04:36,983 DEV : loss 0.030681254342198372 - f1-score (micro avg)  0.8817\n",
      "2022-01-13 10:04:37,015 BAD EPOCHS (no improvement): 0\n",
      "2022-01-13 10:04:37,015 saving best model\n",
      "2022-01-13 10:04:41,694 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-13 10:04:42,205 epoch 55 - iter 2/22 - loss 0.04814819 - samples/sec: 125.14 - lr: 0.025000\n",
      "2022-01-13 10:04:42,710 epoch 55 - iter 4/22 - loss 0.04847034 - samples/sec: 126.81 - lr: 0.025000\n",
      "2022-01-13 10:04:43,190 epoch 55 - iter 6/22 - loss 0.05508680 - samples/sec: 133.28 - lr: 0.025000\n",
      "2022-01-13 10:04:43,614 epoch 55 - iter 8/22 - loss 0.06194849 - samples/sec: 150.87 - lr: 0.025000\n",
      "2022-01-13 10:04:44,144 epoch 55 - iter 10/22 - loss 0.06283735 - samples/sec: 120.83 - lr: 0.025000\n",
      "2022-01-13 10:04:44,724 epoch 55 - iter 12/22 - loss 0.06445644 - samples/sec: 110.37 - lr: 0.025000\n",
      "2022-01-13 10:04:45,277 epoch 55 - iter 14/22 - loss 0.06496106 - samples/sec: 115.62 - lr: 0.025000\n",
      "2022-01-13 10:04:45,916 epoch 55 - iter 16/22 - loss 0.06387315 - samples/sec: 100.16 - lr: 0.025000\n",
      "2022-01-13 10:04:46,521 epoch 55 - iter 18/22 - loss 0.06445799 - samples/sec: 105.88 - lr: 0.025000\n",
      "2022-01-13 10:04:47,098 epoch 55 - iter 20/22 - loss 0.06620958 - samples/sec: 110.85 - lr: 0.025000\n",
      "2022-01-13 10:04:47,508 epoch 55 - iter 22/22 - loss 0.06546695 - samples/sec: 156.17 - lr: 0.025000\n",
      "2022-01-13 10:04:47,516 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-13 10:04:47,516 EPOCH 55 done: loss 0.0655 - lr 0.0250000\n",
      "2022-01-13 10:04:49,237 DEV : loss 0.030649971216917038 - f1-score (micro avg)  0.8535\n",
      "2022-01-13 10:04:49,293 BAD EPOCHS (no improvement): 1\n",
      "2022-01-13 10:04:49,293 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-13 10:04:49,954 epoch 56 - iter 2/22 - loss 0.06386146 - samples/sec: 96.80 - lr: 0.025000\n",
      "2022-01-13 10:04:50,507 epoch 56 - iter 4/22 - loss 0.06659531 - samples/sec: 116.11 - lr: 0.025000\n",
      "2022-01-13 10:04:51,043 epoch 56 - iter 6/22 - loss 0.07168227 - samples/sec: 119.42 - lr: 0.025000\n",
      "2022-01-13 10:04:51,451 epoch 56 - iter 8/22 - loss 0.07154707 - samples/sec: 156.73 - lr: 0.025000\n",
      "2022-01-13 10:04:51,899 epoch 56 - iter 10/22 - loss 0.07430524 - samples/sec: 145.29 - lr: 0.025000\n",
      "2022-01-13 10:04:52,350 epoch 56 - iter 12/22 - loss 0.07190200 - samples/sec: 142.09 - lr: 0.025000\n",
      "2022-01-13 10:04:52,856 epoch 56 - iter 14/22 - loss 0.07254636 - samples/sec: 128.42 - lr: 0.025000\n",
      "2022-01-13 10:04:53,448 epoch 56 - iter 16/22 - loss 0.07023890 - samples/sec: 109.72 - lr: 0.025000\n",
      "2022-01-13 10:04:53,920 epoch 56 - iter 18/22 - loss 0.06973233 - samples/sec: 135.49 - lr: 0.025000\n",
      "2022-01-13 10:04:54,409 epoch 56 - iter 20/22 - loss 0.06727221 - samples/sec: 133.18 - lr: 0.025000\n",
      "2022-01-13 10:04:54,840 epoch 56 - iter 22/22 - loss 0.06854344 - samples/sec: 148.21 - lr: 0.025000\n",
      "2022-01-13 10:04:54,848 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-13 10:04:54,848 EPOCH 56 done: loss 0.0685 - lr 0.0250000\n",
      "2022-01-13 10:04:56,427 DEV : loss 0.029989778995513916 - f1-score (micro avg)  0.8727\n",
      "2022-01-13 10:04:56,467 BAD EPOCHS (no improvement): 2\n",
      "2022-01-13 10:04:56,467 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-13 10:04:56,932 epoch 57 - iter 2/22 - loss 0.04625506 - samples/sec: 137.69 - lr: 0.025000\n",
      "2022-01-13 10:04:57,411 epoch 57 - iter 4/22 - loss 0.05650014 - samples/sec: 133.38 - lr: 0.025000\n",
      "2022-01-13 10:04:57,940 epoch 57 - iter 6/22 - loss 0.06044519 - samples/sec: 121.19 - lr: 0.025000\n",
      "2022-01-13 10:04:58,405 epoch 57 - iter 8/22 - loss 0.06073007 - samples/sec: 137.46 - lr: 0.025000\n",
      "2022-01-13 10:04:58,829 epoch 57 - iter 10/22 - loss 0.06382282 - samples/sec: 151.03 - lr: 0.025000\n",
      "2022-01-13 10:04:59,366 epoch 57 - iter 12/22 - loss 0.06322750 - samples/sec: 119.15 - lr: 0.025000\n",
      "2022-01-13 10:04:59,815 epoch 57 - iter 14/22 - loss 0.06487137 - samples/sec: 142.64 - lr: 0.025000\n",
      "2022-01-13 10:05:00,248 epoch 57 - iter 16/22 - loss 0.06654773 - samples/sec: 150.63 - lr: 0.025000\n",
      "2022-01-13 10:05:00,672 epoch 57 - iter 18/22 - loss 0.06615771 - samples/sec: 153.64 - lr: 0.025000\n",
      "2022-01-13 10:05:01,160 epoch 57 - iter 20/22 - loss 0.06595836 - samples/sec: 131.11 - lr: 0.025000\n",
      "2022-01-13 10:05:01,592 epoch 57 - iter 22/22 - loss 0.06351603 - samples/sec: 148.19 - lr: 0.025000\n",
      "2022-01-13 10:05:01,592 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-13 10:05:01,592 EPOCH 57 done: loss 0.0635 - lr 0.0250000\n",
      "2022-01-13 10:05:03,296 DEV : loss 0.029133876785635948 - f1-score (micro avg)  0.8892\n",
      "2022-01-13 10:05:03,328 BAD EPOCHS (no improvement): 0\n",
      "2022-01-13 10:05:03,336 saving best model\n",
      "2022-01-13 10:05:08,870 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-13 10:05:09,374 epoch 58 - iter 2/22 - loss 0.07114591 - samples/sec: 129.03 - lr: 0.025000\n",
      "2022-01-13 10:05:09,951 epoch 58 - iter 4/22 - loss 0.06402094 - samples/sec: 112.51 - lr: 0.025000\n",
      "2022-01-13 10:05:10,367 epoch 58 - iter 6/22 - loss 0.06565704 - samples/sec: 153.85 - lr: 0.025000\n",
      "2022-01-13 10:05:10,818 epoch 58 - iter 8/22 - loss 0.06247206 - samples/sec: 141.93 - lr: 0.025000\n",
      "2022-01-13 10:05:11,320 epoch 58 - iter 10/22 - loss 0.06144592 - samples/sec: 128.83 - lr: 0.025000\n",
      "2022-01-13 10:05:11,746 epoch 58 - iter 12/22 - loss 0.06319472 - samples/sec: 150.33 - lr: 0.025000\n",
      "2022-01-13 10:05:12,337 epoch 58 - iter 14/22 - loss 0.06479204 - samples/sec: 108.20 - lr: 0.025000\n",
      "2022-01-13 10:05:12,826 epoch 58 - iter 16/22 - loss 0.06454369 - samples/sec: 130.94 - lr: 0.025000\n",
      "2022-01-13 10:05:13,364 epoch 58 - iter 18/22 - loss 0.06445346 - samples/sec: 119.00 - lr: 0.025000\n",
      "2022-01-13 10:05:13,869 epoch 58 - iter 20/22 - loss 0.06588638 - samples/sec: 128.79 - lr: 0.025000\n",
      "2022-01-13 10:05:14,341 epoch 58 - iter 22/22 - loss 0.06448927 - samples/sec: 135.44 - lr: 0.025000\n",
      "2022-01-13 10:05:14,341 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-13 10:05:14,341 EPOCH 58 done: loss 0.0645 - lr 0.0250000\n",
      "2022-01-13 10:05:15,928 DEV : loss 0.029090849682688713 - f1-score (micro avg)  0.8792\n",
      "2022-01-13 10:05:15,968 BAD EPOCHS (no improvement): 1\n",
      "2022-01-13 10:05:15,968 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-13 10:05:16,432 epoch 59 - iter 2/22 - loss 0.04883206 - samples/sec: 140.53 - lr: 0.025000\n",
      "2022-01-13 10:05:17,123 epoch 59 - iter 4/22 - loss 0.05835319 - samples/sec: 94.72 - lr: 0.025000\n",
      "2022-01-13 10:05:17,610 epoch 59 - iter 6/22 - loss 0.06164743 - samples/sec: 131.36 - lr: 0.025000\n",
      "2022-01-13 10:05:18,067 epoch 59 - iter 8/22 - loss 0.06298433 - samples/sec: 140.12 - lr: 0.025000\n",
      "2022-01-13 10:05:18,537 epoch 59 - iter 10/22 - loss 0.06371434 - samples/sec: 136.19 - lr: 0.025000\n",
      "2022-01-13 10:05:19,116 epoch 59 - iter 12/22 - loss 0.06451923 - samples/sec: 113.52 - lr: 0.025000\n",
      "2022-01-13 10:05:19,572 epoch 59 - iter 14/22 - loss 0.06406868 - samples/sec: 143.34 - lr: 0.025000\n",
      "2022-01-13 10:05:20,020 epoch 59 - iter 16/22 - loss 0.06303669 - samples/sec: 142.87 - lr: 0.025000\n",
      "2022-01-13 10:05:20,444 epoch 59 - iter 18/22 - loss 0.06252190 - samples/sec: 150.78 - lr: 0.025000\n",
      "2022-01-13 10:05:20,932 epoch 59 - iter 20/22 - loss 0.06322785 - samples/sec: 131.21 - lr: 0.025000\n",
      "2022-01-13 10:05:21,346 epoch 59 - iter 22/22 - loss 0.06341302 - samples/sec: 154.64 - lr: 0.025000\n",
      "2022-01-13 10:05:21,346 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-13 10:05:21,346 EPOCH 59 done: loss 0.0634 - lr 0.0250000\n",
      "2022-01-13 10:05:23,174 DEV : loss 0.027643989771604538 - f1-score (micro avg)  0.8778\n",
      "2022-01-13 10:05:23,231 BAD EPOCHS (no improvement): 2\n",
      "2022-01-13 10:05:23,231 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-13 10:05:23,872 epoch 60 - iter 2/22 - loss 0.04900533 - samples/sec: 101.00 - lr: 0.025000\n",
      "2022-01-13 10:05:24,295 epoch 60 - iter 4/22 - loss 0.05540626 - samples/sec: 151.38 - lr: 0.025000\n",
      "2022-01-13 10:05:24,739 epoch 60 - iter 6/22 - loss 0.05499235 - samples/sec: 144.32 - lr: 0.025000\n",
      "2022-01-13 10:05:25,334 epoch 60 - iter 8/22 - loss 0.06019873 - samples/sec: 107.57 - lr: 0.025000\n",
      "2022-01-13 10:05:25,803 epoch 60 - iter 10/22 - loss 0.06075589 - samples/sec: 136.49 - lr: 0.025000\n",
      "2022-01-13 10:05:26,270 epoch 60 - iter 12/22 - loss 0.06240832 - samples/sec: 136.92 - lr: 0.025000\n",
      "2022-01-13 10:05:26,887 epoch 60 - iter 14/22 - loss 0.06059007 - samples/sec: 103.74 - lr: 0.025000\n",
      "2022-01-13 10:05:27,371 epoch 60 - iter 16/22 - loss 0.06096876 - samples/sec: 132.15 - lr: 0.025000\n",
      "2022-01-13 10:05:27,922 epoch 60 - iter 18/22 - loss 0.06111968 - samples/sec: 119.65 - lr: 0.025000\n",
      "2022-01-13 10:05:28,381 epoch 60 - iter 20/22 - loss 0.06132683 - samples/sec: 139.28 - lr: 0.025000\n",
      "2022-01-13 10:05:28,771 epoch 60 - iter 22/22 - loss 0.06124864 - samples/sec: 164.31 - lr: 0.025000\n",
      "2022-01-13 10:05:28,871 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-13 10:05:28,871 EPOCH 60 done: loss 0.0612 - lr 0.0250000\n",
      "2022-01-13 10:05:30,367 DEV : loss 0.027548540383577347 - f1-score (micro avg)  0.886\n",
      "2022-01-13 10:05:30,415 BAD EPOCHS (no improvement): 3\n",
      "2022-01-13 10:05:30,415 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-13 10:05:31,178 epoch 61 - iter 2/22 - loss 0.06503352 - samples/sec: 84.75 - lr: 0.025000\n",
      "2022-01-13 10:05:31,911 epoch 61 - iter 4/22 - loss 0.05975959 - samples/sec: 87.40 - lr: 0.025000\n",
      "2022-01-13 10:05:32,648 epoch 61 - iter 6/22 - loss 0.06202041 - samples/sec: 87.81 - lr: 0.025000\n",
      "2022-01-13 10:05:33,481 epoch 61 - iter 8/22 - loss 0.06632430 - samples/sec: 76.75 - lr: 0.025000\n",
      "2022-01-13 10:05:34,294 epoch 61 - iter 10/22 - loss 0.06797336 - samples/sec: 78.80 - lr: 0.025000\n",
      "2022-01-13 10:05:35,195 epoch 61 - iter 12/22 - loss 0.06453774 - samples/sec: 71.03 - lr: 0.025000\n",
      "2022-01-13 10:05:36,345 epoch 61 - iter 14/22 - loss 0.06493220 - samples/sec: 55.64 - lr: 0.025000\n",
      "2022-01-13 10:05:37,281 epoch 61 - iter 16/22 - loss 0.06492495 - samples/sec: 68.89 - lr: 0.025000\n",
      "2022-01-13 10:05:37,771 epoch 61 - iter 18/22 - loss 0.06734863 - samples/sec: 130.56 - lr: 0.025000\n",
      "2022-01-13 10:05:38,310 epoch 61 - iter 20/22 - loss 0.06815192 - samples/sec: 118.84 - lr: 0.025000\n",
      "2022-01-13 10:05:38,829 epoch 61 - iter 22/22 - loss 0.06805895 - samples/sec: 123.21 - lr: 0.025000\n",
      "2022-01-13 10:05:38,829 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-13 10:05:38,829 EPOCH 61 done: loss 0.0681 - lr 0.0250000\n",
      "2022-01-13 10:05:40,231 DEV : loss 0.027406424283981323 - f1-score (micro avg)  0.8933\n",
      "2022-01-13 10:05:40,262 BAD EPOCHS (no improvement): 0\n",
      "2022-01-13 10:05:40,262 saving best model\n",
      "2022-01-13 10:05:45,659 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-13 10:05:46,130 epoch 62 - iter 2/22 - loss 0.07388245 - samples/sec: 135.73 - lr: 0.025000\n",
      "2022-01-13 10:05:46,708 epoch 62 - iter 4/22 - loss 0.06636417 - samples/sec: 110.77 - lr: 0.025000\n",
      "2022-01-13 10:05:47,424 epoch 62 - iter 6/22 - loss 0.06487919 - samples/sec: 89.40 - lr: 0.025000\n",
      "2022-01-13 10:05:48,087 epoch 62 - iter 8/22 - loss 0.06241587 - samples/sec: 96.43 - lr: 0.025000\n",
      "2022-01-13 10:05:48,693 epoch 62 - iter 10/22 - loss 0.05899620 - samples/sec: 105.68 - lr: 0.025000\n",
      "2022-01-13 10:05:49,357 epoch 62 - iter 12/22 - loss 0.06098450 - samples/sec: 96.32 - lr: 0.025000\n",
      "2022-01-13 10:05:49,880 epoch 62 - iter 14/22 - loss 0.06129246 - samples/sec: 122.43 - lr: 0.025000\n",
      "2022-01-13 10:05:50,378 epoch 62 - iter 16/22 - loss 0.06188341 - samples/sec: 128.54 - lr: 0.025000\n",
      "2022-01-13 10:05:50,772 epoch 62 - iter 18/22 - loss 0.06287137 - samples/sec: 162.47 - lr: 0.025000\n",
      "2022-01-13 10:05:51,272 epoch 62 - iter 20/22 - loss 0.06211350 - samples/sec: 128.09 - lr: 0.025000\n",
      "2022-01-13 10:05:51,772 epoch 62 - iter 22/22 - loss 0.06233199 - samples/sec: 128.02 - lr: 0.025000\n",
      "2022-01-13 10:05:51,772 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-13 10:05:51,772 EPOCH 62 done: loss 0.0623 - lr 0.0250000\n",
      "2022-01-13 10:05:53,744 DEV : loss 0.027612578123807907 - f1-score (micro avg)  0.8798\n",
      "2022-01-13 10:05:53,777 BAD EPOCHS (no improvement): 1\n",
      "2022-01-13 10:05:53,786 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-13 10:05:54,252 epoch 63 - iter 2/22 - loss 0.06642340 - samples/sec: 137.23 - lr: 0.025000\n",
      "2022-01-13 10:05:54,915 epoch 63 - iter 4/22 - loss 0.06328558 - samples/sec: 96.74 - lr: 0.025000\n",
      "2022-01-13 10:05:55,459 epoch 63 - iter 6/22 - loss 0.06373708 - samples/sec: 117.68 - lr: 0.025000\n",
      "2022-01-13 10:05:56,150 epoch 63 - iter 8/22 - loss 0.06379592 - samples/sec: 92.66 - lr: 0.025000\n",
      "2022-01-13 10:05:56,568 epoch 63 - iter 10/22 - loss 0.06623160 - samples/sec: 153.20 - lr: 0.025000\n",
      "2022-01-13 10:05:57,212 epoch 63 - iter 12/22 - loss 0.06699116 - samples/sec: 101.84 - lr: 0.025000\n",
      "2022-01-13 10:05:57,846 epoch 63 - iter 14/22 - loss 0.06681521 - samples/sec: 100.94 - lr: 0.025000\n",
      "2022-01-13 10:05:58,394 epoch 63 - iter 16/22 - loss 0.06887686 - samples/sec: 116.74 - lr: 0.025000\n",
      "2022-01-13 10:05:58,929 epoch 63 - iter 18/22 - loss 0.06743370 - samples/sec: 119.73 - lr: 0.025000\n",
      "2022-01-13 10:05:59,491 epoch 63 - iter 20/22 - loss 0.06705794 - samples/sec: 115.56 - lr: 0.025000\n",
      "2022-01-13 10:06:00,074 epoch 63 - iter 22/22 - loss 0.06742257 - samples/sec: 109.72 - lr: 0.025000\n",
      "2022-01-13 10:06:00,074 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-13 10:06:00,074 EPOCH 63 done: loss 0.0674 - lr 0.0250000\n",
      "2022-01-13 10:06:01,935 DEV : loss 0.0263905618339777 - f1-score (micro avg)  0.9034\n",
      "2022-01-13 10:06:01,983 BAD EPOCHS (no improvement): 0\n",
      "2022-01-13 10:06:01,983 saving best model\n",
      "2022-01-13 10:06:07,894 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-13 10:06:08,602 epoch 64 - iter 2/22 - loss 0.06739448 - samples/sec: 90.37 - lr: 0.025000\n",
      "2022-01-13 10:06:09,155 epoch 64 - iter 4/22 - loss 0.06178711 - samples/sec: 115.82 - lr: 0.025000\n",
      "2022-01-13 10:06:09,575 epoch 64 - iter 6/22 - loss 0.06603428 - samples/sec: 152.40 - lr: 0.025000\n",
      "2022-01-13 10:06:10,036 epoch 64 - iter 8/22 - loss 0.06324657 - samples/sec: 138.98 - lr: 0.025000\n",
      "2022-01-13 10:06:10,522 epoch 64 - iter 10/22 - loss 0.06229402 - samples/sec: 133.18 - lr: 0.025000\n",
      "2022-01-13 10:06:10,931 epoch 64 - iter 12/22 - loss 0.06414246 - samples/sec: 156.41 - lr: 0.025000\n",
      "2022-01-13 10:06:11,347 epoch 64 - iter 14/22 - loss 0.06466018 - samples/sec: 153.69 - lr: 0.025000\n",
      "2022-01-13 10:06:11,862 epoch 64 - iter 16/22 - loss 0.06248394 - samples/sec: 124.36 - lr: 0.025000\n",
      "2022-01-13 10:06:12,382 epoch 64 - iter 18/22 - loss 0.06316839 - samples/sec: 122.94 - lr: 0.025000\n",
      "2022-01-13 10:06:12,926 epoch 64 - iter 20/22 - loss 0.06318130 - samples/sec: 117.81 - lr: 0.025000\n",
      "2022-01-13 10:06:13,414 epoch 64 - iter 22/22 - loss 0.06296808 - samples/sec: 131.02 - lr: 0.025000\n",
      "2022-01-13 10:06:13,414 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-13 10:06:13,414 EPOCH 64 done: loss 0.0630 - lr 0.0250000\n",
      "2022-01-13 10:06:15,423 DEV : loss 0.02676415629684925 - f1-score (micro avg)  0.8862\n",
      "2022-01-13 10:06:15,470 BAD EPOCHS (no improvement): 1\n",
      "2022-01-13 10:06:15,472 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-13 10:06:16,093 epoch 65 - iter 2/22 - loss 0.03365244 - samples/sec: 103.08 - lr: 0.025000\n",
      "2022-01-13 10:06:16,647 epoch 65 - iter 4/22 - loss 0.05213166 - samples/sec: 115.58 - lr: 0.025000\n",
      "2022-01-13 10:06:17,343 epoch 65 - iter 6/22 - loss 0.05745822 - samples/sec: 94.03 - lr: 0.025000\n",
      "2022-01-13 10:06:17,931 epoch 65 - iter 8/22 - loss 0.05274602 - samples/sec: 108.91 - lr: 0.025000\n",
      "2022-01-13 10:06:18,744 epoch 65 - iter 10/22 - loss 0.05134304 - samples/sec: 78.73 - lr: 0.025000\n",
      "2022-01-13 10:06:19,445 epoch 65 - iter 12/22 - loss 0.05315975 - samples/sec: 91.60 - lr: 0.025000\n",
      "2022-01-13 10:06:20,037 epoch 65 - iter 14/22 - loss 0.05437817 - samples/sec: 107.94 - lr: 0.025000\n",
      "2022-01-13 10:06:20,529 epoch 65 - iter 16/22 - loss 0.05609162 - samples/sec: 132.70 - lr: 0.025000\n",
      "2022-01-13 10:06:21,230 epoch 65 - iter 18/22 - loss 0.05576796 - samples/sec: 91.55 - lr: 0.025000\n",
      "2022-01-13 10:06:21,777 epoch 65 - iter 20/22 - loss 0.05624268 - samples/sec: 117.13 - lr: 0.025000\n",
      "2022-01-13 10:06:22,223 epoch 65 - iter 22/22 - loss 0.05628567 - samples/sec: 143.28 - lr: 0.025000\n",
      "2022-01-13 10:06:22,231 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-13 10:06:22,231 EPOCH 65 done: loss 0.0563 - lr 0.0250000\n",
      "2022-01-13 10:06:23,961 DEV : loss 0.026330603286623955 - f1-score (micro avg)  0.8749\n",
      "2022-01-13 10:06:24,028 BAD EPOCHS (no improvement): 2\n",
      "2022-01-13 10:06:24,028 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-13 10:06:24,567 epoch 66 - iter 2/22 - loss 0.06172114 - samples/sec: 120.04 - lr: 0.025000\n",
      "2022-01-13 10:06:25,062 epoch 66 - iter 4/22 - loss 0.05611086 - samples/sec: 129.29 - lr: 0.025000\n",
      "2022-01-13 10:06:25,508 epoch 66 - iter 6/22 - loss 0.05499102 - samples/sec: 143.55 - lr: 0.025000\n",
      "2022-01-13 10:06:26,158 epoch 66 - iter 8/22 - loss 0.05228234 - samples/sec: 98.44 - lr: 0.025000\n",
      "2022-01-13 10:06:26,628 epoch 66 - iter 10/22 - loss 0.05224038 - samples/sec: 136.19 - lr: 0.025000\n",
      "2022-01-13 10:06:27,158 epoch 66 - iter 12/22 - loss 0.05379535 - samples/sec: 120.64 - lr: 0.025000\n",
      "2022-01-13 10:06:27,672 epoch 66 - iter 14/22 - loss 0.05632780 - samples/sec: 124.53 - lr: 0.025000\n",
      "2022-01-13 10:06:28,155 epoch 66 - iter 16/22 - loss 0.05731837 - samples/sec: 132.45 - lr: 0.025000\n",
      "2022-01-13 10:06:28,583 epoch 66 - iter 18/22 - loss 0.05997257 - samples/sec: 149.74 - lr: 0.025000\n",
      "2022-01-13 10:06:29,065 epoch 66 - iter 20/22 - loss 0.05962366 - samples/sec: 137.18 - lr: 0.025000\n",
      "2022-01-13 10:06:29,459 epoch 66 - iter 22/22 - loss 0.06132984 - samples/sec: 162.27 - lr: 0.025000\n",
      "2022-01-13 10:06:29,459 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-13 10:06:29,461 EPOCH 66 done: loss 0.0613 - lr 0.0250000\n",
      "2022-01-13 10:06:30,838 DEV : loss 0.02578234300017357 - f1-score (micro avg)  0.8841\n",
      "2022-01-13 10:06:30,877 BAD EPOCHS (no improvement): 3\n",
      "2022-01-13 10:06:30,877 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-13 10:06:31,322 epoch 67 - iter 2/22 - loss 0.07084734 - samples/sec: 145.76 - lr: 0.025000\n",
      "2022-01-13 10:06:31,794 epoch 67 - iter 4/22 - loss 0.05975318 - samples/sec: 135.52 - lr: 0.025000\n",
      "2022-01-13 10:06:32,301 epoch 67 - iter 6/22 - loss 0.05512829 - samples/sec: 126.26 - lr: 0.025000\n",
      "2022-01-13 10:06:32,746 epoch 67 - iter 8/22 - loss 0.05988143 - samples/sec: 144.01 - lr: 0.025000\n",
      "2022-01-13 10:06:33,317 epoch 67 - iter 10/22 - loss 0.06166989 - samples/sec: 112.11 - lr: 0.025000\n",
      "2022-01-13 10:06:33,720 epoch 67 - iter 12/22 - loss 0.06135082 - samples/sec: 165.27 - lr: 0.025000\n",
      "2022-01-13 10:06:34,254 epoch 67 - iter 14/22 - loss 0.06086428 - samples/sec: 119.66 - lr: 0.025000\n",
      "2022-01-13 10:06:34,712 epoch 67 - iter 16/22 - loss 0.06160882 - samples/sec: 139.96 - lr: 0.025000\n",
      "2022-01-13 10:06:35,263 epoch 67 - iter 18/22 - loss 0.06077931 - samples/sec: 116.06 - lr: 0.025000\n",
      "2022-01-13 10:06:35,666 epoch 67 - iter 20/22 - loss 0.05989031 - samples/sec: 159.04 - lr: 0.025000\n",
      "2022-01-13 10:06:36,149 epoch 67 - iter 22/22 - loss 0.06082642 - samples/sec: 132.30 - lr: 0.025000\n",
      "2022-01-13 10:06:36,149 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-13 10:06:36,149 EPOCH 67 done: loss 0.0608 - lr 0.0250000\n",
      "2022-01-13 10:06:37,542 DEV : loss 0.026107516139745712 - f1-score (micro avg)  0.8868\n",
      "Epoch    67: reducing learning rate of group 0 to 1.2500e-02.\n",
      "2022-01-13 10:06:37,578 BAD EPOCHS (no improvement): 4\n",
      "2022-01-13 10:06:37,578 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-13 10:06:38,166 epoch 68 - iter 2/22 - loss 0.05113707 - samples/sec: 110.77 - lr: 0.012500\n",
      "2022-01-13 10:06:38,597 epoch 68 - iter 4/22 - loss 0.05985168 - samples/sec: 149.31 - lr: 0.012500\n",
      "2022-01-13 10:06:39,003 epoch 68 - iter 6/22 - loss 0.06066049 - samples/sec: 157.84 - lr: 0.012500\n",
      "2022-01-13 10:06:39,477 epoch 68 - iter 8/22 - loss 0.06145725 - samples/sec: 134.79 - lr: 0.012500\n",
      "2022-01-13 10:06:39,949 epoch 68 - iter 10/22 - loss 0.06149279 - samples/sec: 135.91 - lr: 0.012500\n",
      "2022-01-13 10:06:40,588 epoch 68 - iter 12/22 - loss 0.06271014 - samples/sec: 100.21 - lr: 0.012500\n",
      "2022-01-13 10:06:41,129 epoch 68 - iter 14/22 - loss 0.06320223 - samples/sec: 118.24 - lr: 0.012500\n",
      "2022-01-13 10:06:41,772 epoch 68 - iter 16/22 - loss 0.06358922 - samples/sec: 100.90 - lr: 0.012500\n",
      "2022-01-13 10:06:42,239 epoch 68 - iter 18/22 - loss 0.06377309 - samples/sec: 137.04 - lr: 0.012500\n",
      "2022-01-13 10:06:42,698 epoch 68 - iter 20/22 - loss 0.06460570 - samples/sec: 139.31 - lr: 0.012500\n",
      "2022-01-13 10:06:43,120 epoch 68 - iter 22/22 - loss 0.06528404 - samples/sec: 151.60 - lr: 0.012500\n",
      "2022-01-13 10:06:43,120 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-13 10:06:43,120 EPOCH 68 done: loss 0.0653 - lr 0.0125000\n",
      "2022-01-13 10:06:44,901 DEV : loss 0.02425065077841282 - f1-score (micro avg)  0.9049\n",
      "2022-01-13 10:06:44,950 BAD EPOCHS (no improvement): 0\n",
      "2022-01-13 10:06:44,950 saving best model\n",
      "2022-01-13 10:06:49,427 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-13 10:06:49,991 epoch 69 - iter 2/22 - loss 0.05173139 - samples/sec: 113.36 - lr: 0.012500\n",
      "2022-01-13 10:06:50,477 epoch 69 - iter 4/22 - loss 0.05581330 - samples/sec: 131.78 - lr: 0.012500\n",
      "2022-01-13 10:06:50,929 epoch 69 - iter 6/22 - loss 0.05507690 - samples/sec: 144.43 - lr: 0.012500\n",
      "2022-01-13 10:06:51,359 epoch 69 - iter 8/22 - loss 0.06085113 - samples/sec: 148.86 - lr: 0.012500\n",
      "2022-01-13 10:06:51,889 epoch 69 - iter 10/22 - loss 0.05965202 - samples/sec: 120.85 - lr: 0.012500\n",
      "2022-01-13 10:06:52,439 epoch 69 - iter 12/22 - loss 0.06288109 - samples/sec: 116.73 - lr: 0.012500\n",
      "2022-01-13 10:06:52,856 epoch 69 - iter 14/22 - loss 0.06216247 - samples/sec: 153.68 - lr: 0.012500\n",
      "2022-01-13 10:06:53,339 epoch 69 - iter 16/22 - loss 0.06264875 - samples/sec: 132.52 - lr: 0.012500\n",
      "2022-01-13 10:06:53,860 epoch 69 - iter 18/22 - loss 0.06160046 - samples/sec: 122.70 - lr: 0.012500\n",
      "2022-01-13 10:06:54,412 epoch 69 - iter 20/22 - loss 0.06035267 - samples/sec: 115.93 - lr: 0.012500\n",
      "2022-01-13 10:06:54,878 epoch 69 - iter 22/22 - loss 0.06105376 - samples/sec: 137.55 - lr: 0.012500\n",
      "2022-01-13 10:06:54,878 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-13 10:06:54,878 EPOCH 69 done: loss 0.0611 - lr 0.0125000\n",
      "2022-01-13 10:06:56,553 DEV : loss 0.02474798448383808 - f1-score (micro avg)  0.8956\n",
      "2022-01-13 10:06:56,584 BAD EPOCHS (no improvement): 1\n",
      "2022-01-13 10:06:56,600 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-13 10:06:57,041 epoch 70 - iter 2/22 - loss 0.07877149 - samples/sec: 145.16 - lr: 0.012500\n",
      "2022-01-13 10:06:57,464 epoch 70 - iter 4/22 - loss 0.06220710 - samples/sec: 151.37 - lr: 0.012500\n",
      "2022-01-13 10:06:57,854 epoch 70 - iter 6/22 - loss 0.06808774 - samples/sec: 164.21 - lr: 0.012500\n",
      "2022-01-13 10:06:58,298 epoch 70 - iter 8/22 - loss 0.06503873 - samples/sec: 144.21 - lr: 0.012500\n",
      "2022-01-13 10:06:58,803 epoch 70 - iter 10/22 - loss 0.06566646 - samples/sec: 126.57 - lr: 0.012500\n",
      "2022-01-13 10:06:59,331 epoch 70 - iter 12/22 - loss 0.06358385 - samples/sec: 121.36 - lr: 0.012500\n",
      "2022-01-13 10:06:59,798 epoch 70 - iter 14/22 - loss 0.06492994 - samples/sec: 141.54 - lr: 0.012500\n",
      "2022-01-13 10:07:00,332 epoch 70 - iter 16/22 - loss 0.06399205 - samples/sec: 123.47 - lr: 0.012500\n",
      "2022-01-13 10:07:00,808 epoch 70 - iter 18/22 - loss 0.06394144 - samples/sec: 134.49 - lr: 0.012500\n",
      "2022-01-13 10:07:01,370 epoch 70 - iter 20/22 - loss 0.06239824 - samples/sec: 114.17 - lr: 0.012500\n",
      "2022-01-13 10:07:01,925 epoch 70 - iter 22/22 - loss 0.06219754 - samples/sec: 115.20 - lr: 0.012500\n",
      "2022-01-13 10:07:01,925 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-13 10:07:01,925 EPOCH 70 done: loss 0.0622 - lr 0.0125000\n",
      "2022-01-13 10:07:03,649 DEV : loss 0.024226780980825424 - f1-score (micro avg)  0.9006\n",
      "2022-01-13 10:07:03,708 BAD EPOCHS (no improvement): 2\n",
      "2022-01-13 10:07:03,710 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-13 10:07:04,382 epoch 71 - iter 2/22 - loss 0.05437869 - samples/sec: 95.51 - lr: 0.012500\n",
      "2022-01-13 10:07:04,818 epoch 71 - iter 4/22 - loss 0.06547092 - samples/sec: 151.11 - lr: 0.012500\n",
      "2022-01-13 10:07:05,427 epoch 71 - iter 6/22 - loss 0.06761188 - samples/sec: 106.58 - lr: 0.012500\n",
      "2022-01-13 10:07:06,021 epoch 71 - iter 8/22 - loss 0.07103804 - samples/sec: 107.69 - lr: 0.012500\n",
      "2022-01-13 10:07:06,510 epoch 71 - iter 10/22 - loss 0.06748921 - samples/sec: 130.81 - lr: 0.012500\n",
      "2022-01-13 10:07:07,047 epoch 71 - iter 12/22 - loss 0.06929392 - samples/sec: 122.85 - lr: 0.012500\n",
      "2022-01-13 10:07:07,591 epoch 71 - iter 14/22 - loss 0.06572657 - samples/sec: 117.55 - lr: 0.012500\n",
      "2022-01-13 10:07:08,141 epoch 71 - iter 16/22 - loss 0.06482458 - samples/sec: 116.38 - lr: 0.012500\n",
      "2022-01-13 10:07:08,565 epoch 71 - iter 18/22 - loss 0.06378741 - samples/sec: 151.11 - lr: 0.012500\n",
      "2022-01-13 10:07:09,271 epoch 71 - iter 20/22 - loss 0.06241904 - samples/sec: 90.60 - lr: 0.012500\n",
      "2022-01-13 10:07:09,794 epoch 71 - iter 22/22 - loss 0.06255226 - samples/sec: 122.34 - lr: 0.012500\n",
      "2022-01-13 10:07:09,794 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-13 10:07:09,794 EPOCH 71 done: loss 0.0626 - lr 0.0125000\n",
      "2022-01-13 10:07:11,734 DEV : loss 0.02376643195748329 - f1-score (micro avg)  0.9142\n",
      "2022-01-13 10:07:11,767 BAD EPOCHS (no improvement): 0\n",
      "2022-01-13 10:07:11,775 saving best model\n",
      "2022-01-13 10:07:17,532 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-13 10:07:18,412 epoch 72 - iter 2/22 - loss 0.04135260 - samples/sec: 73.37 - lr: 0.012500\n",
      "2022-01-13 10:07:19,081 epoch 72 - iter 4/22 - loss 0.05181948 - samples/sec: 95.71 - lr: 0.012500\n",
      "2022-01-13 10:07:19,566 epoch 72 - iter 6/22 - loss 0.05376980 - samples/sec: 132.39 - lr: 0.012500\n",
      "2022-01-13 10:07:20,073 epoch 72 - iter 8/22 - loss 0.05796554 - samples/sec: 126.13 - lr: 0.012500\n",
      "2022-01-13 10:07:20,607 epoch 72 - iter 10/22 - loss 0.05751564 - samples/sec: 119.99 - lr: 0.012500\n",
      "2022-01-13 10:07:21,172 epoch 72 - iter 12/22 - loss 0.05648770 - samples/sec: 113.09 - lr: 0.012500\n",
      "2022-01-13 10:07:21,660 epoch 72 - iter 14/22 - loss 0.05711199 - samples/sec: 131.36 - lr: 0.012500\n",
      "2022-01-13 10:07:22,231 epoch 72 - iter 16/22 - loss 0.05650697 - samples/sec: 118.52 - lr: 0.012500\n",
      "2022-01-13 10:07:22,663 epoch 72 - iter 18/22 - loss 0.05678328 - samples/sec: 148.07 - lr: 0.012500\n",
      "2022-01-13 10:07:23,246 epoch 72 - iter 20/22 - loss 0.05662356 - samples/sec: 109.76 - lr: 0.012500\n",
      "2022-01-13 10:07:23,724 epoch 72 - iter 22/22 - loss 0.05796879 - samples/sec: 134.54 - lr: 0.012500\n",
      "2022-01-13 10:07:23,724 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-13 10:07:23,724 EPOCH 72 done: loss 0.0580 - lr 0.0125000\n",
      "2022-01-13 10:07:25,236 DEV : loss 0.024037819355726242 - f1-score (micro avg)  0.917\n",
      "2022-01-13 10:07:25,268 BAD EPOCHS (no improvement): 0\n",
      "2022-01-13 10:07:25,268 saving best model\n",
      "2022-01-13 10:07:30,219 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-13 10:07:30,728 epoch 73 - iter 2/22 - loss 0.05424844 - samples/sec: 125.59 - lr: 0.012500\n",
      "2022-01-13 10:07:31,230 epoch 73 - iter 4/22 - loss 0.06187999 - samples/sec: 127.58 - lr: 0.012500\n",
      "2022-01-13 10:07:31,716 epoch 73 - iter 6/22 - loss 0.06165626 - samples/sec: 131.74 - lr: 0.012500\n",
      "2022-01-13 10:07:32,350 epoch 73 - iter 8/22 - loss 0.06174941 - samples/sec: 100.96 - lr: 0.012500\n",
      "2022-01-13 10:07:32,867 epoch 73 - iter 10/22 - loss 0.06233322 - samples/sec: 127.78 - lr: 0.012500\n",
      "2022-01-13 10:07:33,446 epoch 73 - iter 12/22 - loss 0.06124331 - samples/sec: 110.59 - lr: 0.012500\n",
      "2022-01-13 10:07:33,991 epoch 73 - iter 14/22 - loss 0.06001356 - samples/sec: 117.32 - lr: 0.012500\n",
      "2022-01-13 10:07:34,501 epoch 73 - iter 16/22 - loss 0.06066503 - samples/sec: 125.62 - lr: 0.012500\n",
      "2022-01-13 10:07:35,117 epoch 73 - iter 18/22 - loss 0.06163491 - samples/sec: 103.77 - lr: 0.012500\n",
      "2022-01-13 10:07:35,600 epoch 73 - iter 20/22 - loss 0.06160805 - samples/sec: 134.95 - lr: 0.012500\n",
      "2022-01-13 10:07:36,183 epoch 73 - iter 22/22 - loss 0.06148056 - samples/sec: 109.75 - lr: 0.012500\n",
      "2022-01-13 10:07:36,183 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-13 10:07:36,183 EPOCH 73 done: loss 0.0615 - lr 0.0125000\n",
      "2022-01-13 10:07:38,035 DEV : loss 0.0238706786185503 - f1-score (micro avg)  0.8992\n",
      "2022-01-13 10:07:38,091 BAD EPOCHS (no improvement): 1\n",
      "2022-01-13 10:07:38,091 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-13 10:07:38,795 epoch 74 - iter 2/22 - loss 0.07273978 - samples/sec: 91.97 - lr: 0.012500\n",
      "2022-01-13 10:07:39,674 epoch 74 - iter 4/22 - loss 0.07311268 - samples/sec: 72.84 - lr: 0.012500\n",
      "2022-01-13 10:07:40,354 epoch 74 - iter 6/22 - loss 0.06360889 - samples/sec: 94.06 - lr: 0.012500\n",
      "2022-01-13 10:07:40,776 epoch 74 - iter 8/22 - loss 0.06640821 - samples/sec: 154.56 - lr: 0.012500\n",
      "2022-01-13 10:07:41,354 epoch 74 - iter 10/22 - loss 0.06441785 - samples/sec: 110.79 - lr: 0.012500\n",
      "2022-01-13 10:07:41,840 epoch 74 - iter 12/22 - loss 0.06146984 - samples/sec: 136.01 - lr: 0.012500\n",
      "2022-01-13 10:07:42,333 epoch 74 - iter 14/22 - loss 0.06145412 - samples/sec: 129.95 - lr: 0.012500\n",
      "2022-01-13 10:07:42,765 epoch 74 - iter 16/22 - loss 0.06180349 - samples/sec: 147.87 - lr: 0.012500\n",
      "2022-01-13 10:07:43,324 epoch 74 - iter 18/22 - loss 0.06129096 - samples/sec: 114.53 - lr: 0.012500\n",
      "2022-01-13 10:07:43,816 epoch 74 - iter 20/22 - loss 0.06201749 - samples/sec: 130.06 - lr: 0.012500\n",
      "2022-01-13 10:07:44,262 epoch 74 - iter 22/22 - loss 0.05974189 - samples/sec: 143.55 - lr: 0.012500\n",
      "2022-01-13 10:07:44,262 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-13 10:07:44,262 EPOCH 74 done: loss 0.0597 - lr 0.0125000\n",
      "2022-01-13 10:07:45,730 DEV : loss 0.023369548842310905 - f1-score (micro avg)  0.9056\n",
      "2022-01-13 10:07:45,769 BAD EPOCHS (no improvement): 2\n",
      "2022-01-13 10:07:45,769 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-13 10:07:46,204 epoch 75 - iter 2/22 - loss 0.06163752 - samples/sec: 147.97 - lr: 0.012500\n",
      "2022-01-13 10:07:46,672 epoch 75 - iter 4/22 - loss 0.05800007 - samples/sec: 136.91 - lr: 0.012500\n",
      "2022-01-13 10:07:47,257 epoch 75 - iter 6/22 - loss 0.05714220 - samples/sec: 109.32 - lr: 0.012500\n",
      "2022-01-13 10:07:47,759 epoch 75 - iter 8/22 - loss 0.05826411 - samples/sec: 127.44 - lr: 0.012500\n",
      "2022-01-13 10:07:48,183 epoch 75 - iter 10/22 - loss 0.05461337 - samples/sec: 151.23 - lr: 0.012500\n",
      "2022-01-13 10:07:48,671 epoch 75 - iter 12/22 - loss 0.05724824 - samples/sec: 131.17 - lr: 0.012500\n",
      "2022-01-13 10:07:49,142 epoch 75 - iter 14/22 - loss 0.05868129 - samples/sec: 140.42 - lr: 0.012500\n",
      "2022-01-13 10:07:49,651 epoch 75 - iter 16/22 - loss 0.05940711 - samples/sec: 126.00 - lr: 0.012500\n",
      "2022-01-13 10:07:50,170 epoch 75 - iter 18/22 - loss 0.06003969 - samples/sec: 123.41 - lr: 0.012500\n",
      "2022-01-13 10:07:50,684 epoch 75 - iter 20/22 - loss 0.05999684 - samples/sec: 124.34 - lr: 0.012500\n",
      "2022-01-13 10:07:51,151 epoch 75 - iter 22/22 - loss 0.05892928 - samples/sec: 137.07 - lr: 0.012500\n",
      "2022-01-13 10:07:51,151 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-13 10:07:51,151 EPOCH 75 done: loss 0.0589 - lr 0.0125000\n",
      "2022-01-13 10:07:52,787 DEV : loss 0.022829897701740265 - f1-score (micro avg)  0.9078\n",
      "2022-01-13 10:07:52,816 BAD EPOCHS (no improvement): 3\n",
      "2022-01-13 10:07:52,816 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-13 10:07:53,313 epoch 76 - iter 2/22 - loss 0.07684804 - samples/sec: 130.98 - lr: 0.012500\n",
      "2022-01-13 10:07:53,787 epoch 76 - iter 4/22 - loss 0.07298230 - samples/sec: 135.02 - lr: 0.012500\n",
      "2022-01-13 10:07:54,306 epoch 76 - iter 6/22 - loss 0.06739744 - samples/sec: 123.14 - lr: 0.012500\n",
      "2022-01-13 10:07:54,703 epoch 76 - iter 8/22 - loss 0.06606501 - samples/sec: 161.45 - lr: 0.012500\n",
      "2022-01-13 10:07:55,348 epoch 76 - iter 10/22 - loss 0.06374121 - samples/sec: 99.20 - lr: 0.012500\n",
      "2022-01-13 10:07:55,885 epoch 76 - iter 12/22 - loss 0.06515294 - samples/sec: 119.13 - lr: 0.012500\n",
      "2022-01-13 10:07:56,452 epoch 76 - iter 14/22 - loss 0.06562145 - samples/sec: 112.97 - lr: 0.012500\n",
      "2022-01-13 10:07:56,914 epoch 76 - iter 16/22 - loss 0.06704171 - samples/sec: 138.50 - lr: 0.012500\n",
      "2022-01-13 10:07:57,546 epoch 76 - iter 18/22 - loss 0.06688055 - samples/sec: 101.17 - lr: 0.012500\n",
      "2022-01-13 10:07:58,019 epoch 76 - iter 20/22 - loss 0.06674496 - samples/sec: 135.56 - lr: 0.012500\n",
      "2022-01-13 10:07:58,459 epoch 76 - iter 22/22 - loss 0.06574580 - samples/sec: 148.11 - lr: 0.012500\n",
      "2022-01-13 10:07:58,467 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-13 10:07:58,467 EPOCH 76 done: loss 0.0657 - lr 0.0125000\n",
      "2022-01-13 10:07:59,954 DEV : loss 0.02313356287777424 - f1-score (micro avg)  0.9113\n",
      "Epoch    76: reducing learning rate of group 0 to 6.2500e-03.\n",
      "2022-01-13 10:07:59,986 BAD EPOCHS (no improvement): 4\n",
      "2022-01-13 10:07:59,994 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-13 10:08:00,569 epoch 77 - iter 2/22 - loss 0.04952952 - samples/sec: 111.12 - lr: 0.006250\n",
      "2022-01-13 10:08:01,153 epoch 77 - iter 4/22 - loss 0.05372782 - samples/sec: 109.81 - lr: 0.006250\n",
      "2022-01-13 10:08:01,621 epoch 77 - iter 6/22 - loss 0.05438230 - samples/sec: 136.97 - lr: 0.006250\n",
      "2022-01-13 10:08:02,149 epoch 77 - iter 8/22 - loss 0.05305066 - samples/sec: 121.21 - lr: 0.006250\n",
      "2022-01-13 10:08:02,636 epoch 77 - iter 10/22 - loss 0.05530713 - samples/sec: 131.21 - lr: 0.006250\n",
      "2022-01-13 10:08:03,264 epoch 77 - iter 12/22 - loss 0.05379743 - samples/sec: 101.96 - lr: 0.006250\n",
      "2022-01-13 10:08:03,749 epoch 77 - iter 14/22 - loss 0.05524655 - samples/sec: 131.90 - lr: 0.006250\n",
      "2022-01-13 10:08:04,365 epoch 77 - iter 16/22 - loss 0.05627453 - samples/sec: 104.00 - lr: 0.006250\n",
      "2022-01-13 10:08:04,848 epoch 77 - iter 18/22 - loss 0.05664505 - samples/sec: 132.44 - lr: 0.006250\n",
      "2022-01-13 10:08:05,294 epoch 77 - iter 20/22 - loss 0.05673822 - samples/sec: 143.50 - lr: 0.006250\n",
      "2022-01-13 10:08:05,726 epoch 77 - iter 22/22 - loss 0.05911499 - samples/sec: 148.18 - lr: 0.006250\n",
      "2022-01-13 10:08:05,726 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-13 10:08:05,742 EPOCH 77 done: loss 0.0591 - lr 0.0062500\n",
      "2022-01-13 10:08:07,419 DEV : loss 0.023085201159119606 - f1-score (micro avg)  0.9142\n",
      "2022-01-13 10:08:07,451 BAD EPOCHS (no improvement): 1\n",
      "2022-01-13 10:08:07,451 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-13 10:08:07,948 epoch 78 - iter 2/22 - loss 0.05389985 - samples/sec: 128.83 - lr: 0.006250\n",
      "2022-01-13 10:08:08,597 epoch 78 - iter 4/22 - loss 0.05171423 - samples/sec: 98.55 - lr: 0.006250\n",
      "2022-01-13 10:08:09,171 epoch 78 - iter 6/22 - loss 0.05234264 - samples/sec: 111.57 - lr: 0.006250\n",
      "2022-01-13 10:08:09,785 epoch 78 - iter 8/22 - loss 0.05744044 - samples/sec: 105.60 - lr: 0.006250\n",
      "2022-01-13 10:08:10,348 epoch 78 - iter 10/22 - loss 0.05595676 - samples/sec: 113.62 - lr: 0.006250\n",
      "2022-01-13 10:08:10,895 epoch 78 - iter 12/22 - loss 0.05314657 - samples/sec: 118.92 - lr: 0.006250\n",
      "2022-01-13 10:08:11,324 epoch 78 - iter 14/22 - loss 0.05344369 - samples/sec: 149.10 - lr: 0.006250\n",
      "2022-01-13 10:08:11,903 epoch 78 - iter 16/22 - loss 0.05359668 - samples/sec: 110.56 - lr: 0.006250\n",
      "2022-01-13 10:08:12,434 epoch 78 - iter 18/22 - loss 0.05529243 - samples/sec: 124.03 - lr: 0.006250\n",
      "2022-01-13 10:08:13,020 epoch 78 - iter 20/22 - loss 0.05610828 - samples/sec: 109.34 - lr: 0.006250\n",
      "2022-01-13 10:08:13,445 epoch 78 - iter 22/22 - loss 0.05789327 - samples/sec: 150.48 - lr: 0.006250\n",
      "2022-01-13 10:08:13,445 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-13 10:08:13,445 EPOCH 78 done: loss 0.0579 - lr 0.0062500\n",
      "2022-01-13 10:08:14,947 DEV : loss 0.022268099710345268 - f1-score (micro avg)  0.9157\n",
      "2022-01-13 10:08:14,993 BAD EPOCHS (no improvement): 2\n",
      "2022-01-13 10:08:14,993 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-13 10:08:15,549 epoch 79 - iter 2/22 - loss 0.04214161 - samples/sec: 115.18 - lr: 0.006250\n",
      "2022-01-13 10:08:16,259 epoch 79 - iter 4/22 - loss 0.05050245 - samples/sec: 91.17 - lr: 0.006250\n",
      "2022-01-13 10:08:16,877 epoch 79 - iter 6/22 - loss 0.05647514 - samples/sec: 103.49 - lr: 0.006250\n",
      "2022-01-13 10:08:17,374 epoch 79 - iter 8/22 - loss 0.05566401 - samples/sec: 128.89 - lr: 0.006250\n",
      "2022-01-13 10:08:17,976 epoch 79 - iter 10/22 - loss 0.05276925 - samples/sec: 106.39 - lr: 0.006250\n",
      "2022-01-13 10:08:18,528 epoch 79 - iter 12/22 - loss 0.05148826 - samples/sec: 115.81 - lr: 0.006250\n",
      "2022-01-13 10:08:19,052 epoch 79 - iter 14/22 - loss 0.05285415 - samples/sec: 122.15 - lr: 0.006250\n",
      "2022-01-13 10:08:19,610 epoch 79 - iter 16/22 - loss 0.05097429 - samples/sec: 114.64 - lr: 0.006250\n",
      "2022-01-13 10:08:20,104 epoch 79 - iter 18/22 - loss 0.05170437 - samples/sec: 133.84 - lr: 0.006250\n",
      "2022-01-13 10:08:20,601 epoch 79 - iter 20/22 - loss 0.05350407 - samples/sec: 128.81 - lr: 0.006250\n",
      "2022-01-13 10:08:21,040 epoch 79 - iter 22/22 - loss 0.05351358 - samples/sec: 145.98 - lr: 0.006250\n",
      "2022-01-13 10:08:21,040 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-13 10:08:21,040 EPOCH 79 done: loss 0.0535 - lr 0.0062500\n",
      "2022-01-13 10:08:22,821 DEV : loss 0.022714093327522278 - f1-score (micro avg)  0.9107\n",
      "2022-01-13 10:08:22,848 BAD EPOCHS (no improvement): 3\n",
      "2022-01-13 10:08:22,848 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-13 10:08:23,256 epoch 80 - iter 2/22 - loss 0.06255087 - samples/sec: 156.90 - lr: 0.006250\n",
      "2022-01-13 10:08:23,728 epoch 80 - iter 4/22 - loss 0.05235130 - samples/sec: 135.82 - lr: 0.006250\n",
      "2022-01-13 10:08:24,241 epoch 80 - iter 6/22 - loss 0.05173013 - samples/sec: 124.66 - lr: 0.006250\n",
      "2022-01-13 10:08:24,654 epoch 80 - iter 8/22 - loss 0.05231824 - samples/sec: 154.77 - lr: 0.006250\n",
      "2022-01-13 10:08:25,388 epoch 80 - iter 10/22 - loss 0.05025744 - samples/sec: 97.88 - lr: 0.006250\n",
      "2022-01-13 10:08:25,925 epoch 80 - iter 12/22 - loss 0.05207311 - samples/sec: 119.28 - lr: 0.006250\n",
      "2022-01-13 10:08:26,541 epoch 80 - iter 14/22 - loss 0.05189982 - samples/sec: 103.86 - lr: 0.006250\n",
      "2022-01-13 10:08:27,028 epoch 80 - iter 16/22 - loss 0.05349545 - samples/sec: 131.58 - lr: 0.006250\n",
      "2022-01-13 10:08:27,500 epoch 80 - iter 18/22 - loss 0.05441335 - samples/sec: 135.72 - lr: 0.006250\n",
      "2022-01-13 10:08:27,946 epoch 80 - iter 20/22 - loss 0.05558746 - samples/sec: 143.36 - lr: 0.006250\n",
      "2022-01-13 10:08:28,338 epoch 80 - iter 22/22 - loss 0.05609076 - samples/sec: 163.54 - lr: 0.006250\n",
      "2022-01-13 10:08:28,338 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-13 10:08:28,338 EPOCH 80 done: loss 0.0561 - lr 0.0062500\n",
      "2022-01-13 10:08:29,811 DEV : loss 0.022423770278692245 - f1-score (micro avg)  0.9149\n",
      "Epoch    80: reducing learning rate of group 0 to 3.1250e-03.\n",
      "2022-01-13 10:08:29,858 BAD EPOCHS (no improvement): 4\n",
      "2022-01-13 10:08:29,858 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-13 10:08:30,266 epoch 81 - iter 2/22 - loss 0.06276170 - samples/sec: 162.97 - lr: 0.003125\n",
      "2022-01-13 10:08:30,694 epoch 81 - iter 4/22 - loss 0.05636754 - samples/sec: 149.50 - lr: 0.003125\n",
      "2022-01-13 10:08:31,200 epoch 81 - iter 6/22 - loss 0.05605798 - samples/sec: 126.75 - lr: 0.003125\n",
      "2022-01-13 10:08:31,624 epoch 81 - iter 8/22 - loss 0.05647506 - samples/sec: 150.87 - lr: 0.003125\n",
      "2022-01-13 10:08:32,116 epoch 81 - iter 10/22 - loss 0.05885479 - samples/sec: 130.08 - lr: 0.003125\n",
      "2022-01-13 10:08:32,503 epoch 81 - iter 12/22 - loss 0.05936176 - samples/sec: 165.38 - lr: 0.003125\n",
      "2022-01-13 10:08:32,975 epoch 81 - iter 14/22 - loss 0.05954052 - samples/sec: 135.73 - lr: 0.003125\n",
      "2022-01-13 10:08:33,477 epoch 81 - iter 16/22 - loss 0.05968297 - samples/sec: 127.38 - lr: 0.003125\n",
      "2022-01-13 10:08:33,949 epoch 81 - iter 18/22 - loss 0.06045795 - samples/sec: 135.71 - lr: 0.003125\n",
      "2022-01-13 10:08:34,447 epoch 81 - iter 20/22 - loss 0.05967085 - samples/sec: 128.58 - lr: 0.003125\n",
      "2022-01-13 10:08:34,860 epoch 81 - iter 22/22 - loss 0.05897171 - samples/sec: 155.28 - lr: 0.003125\n",
      "2022-01-13 10:08:34,860 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-13 10:08:34,860 EPOCH 81 done: loss 0.0590 - lr 0.0031250\n",
      "2022-01-13 10:08:36,297 DEV : loss 0.022223714739084244 - f1-score (micro avg)  0.9164\n",
      "2022-01-13 10:08:36,329 BAD EPOCHS (no improvement): 1\n",
      "2022-01-13 10:08:36,329 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-13 10:08:36,763 epoch 82 - iter 2/22 - loss 0.08236837 - samples/sec: 152.95 - lr: 0.003125\n",
      "2022-01-13 10:08:37,249 epoch 82 - iter 4/22 - loss 0.06619357 - samples/sec: 131.63 - lr: 0.003125\n",
      "2022-01-13 10:08:37,810 epoch 82 - iter 6/22 - loss 0.06625195 - samples/sec: 114.06 - lr: 0.003125\n",
      "2022-01-13 10:08:38,344 epoch 82 - iter 8/22 - loss 0.06577708 - samples/sec: 119.94 - lr: 0.003125\n",
      "2022-01-13 10:08:38,887 epoch 82 - iter 10/22 - loss 0.06598111 - samples/sec: 117.83 - lr: 0.003125\n",
      "2022-01-13 10:08:39,440 epoch 82 - iter 12/22 - loss 0.06373471 - samples/sec: 115.72 - lr: 0.003125\n",
      "2022-01-13 10:08:39,894 epoch 82 - iter 14/22 - loss 0.05934932 - samples/sec: 141.05 - lr: 0.003125\n",
      "2022-01-13 10:08:40,333 epoch 82 - iter 16/22 - loss 0.05909210 - samples/sec: 145.69 - lr: 0.003125\n",
      "2022-01-13 10:08:40,757 epoch 82 - iter 18/22 - loss 0.05856520 - samples/sec: 150.91 - lr: 0.003125\n",
      "2022-01-13 10:08:41,159 epoch 82 - iter 20/22 - loss 0.05941401 - samples/sec: 159.31 - lr: 0.003125\n",
      "2022-01-13 10:08:41,583 epoch 82 - iter 22/22 - loss 0.05944341 - samples/sec: 150.83 - lr: 0.003125\n",
      "2022-01-13 10:08:41,583 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-13 10:08:41,583 EPOCH 82 done: loss 0.0594 - lr 0.0031250\n",
      "2022-01-13 10:08:42,983 DEV : loss 0.022191714495420456 - f1-score (micro avg)  0.9149\n",
      "2022-01-13 10:08:43,022 BAD EPOCHS (no improvement): 2\n",
      "2022-01-13 10:08:43,022 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-13 10:08:43,549 epoch 83 - iter 2/22 - loss 0.06014538 - samples/sec: 123.16 - lr: 0.003125\n",
      "2022-01-13 10:08:44,033 epoch 83 - iter 4/22 - loss 0.05746174 - samples/sec: 134.52 - lr: 0.003125\n",
      "2022-01-13 10:08:44,482 epoch 83 - iter 6/22 - loss 0.05504690 - samples/sec: 142.55 - lr: 0.003125\n",
      "2022-01-13 10:08:45,148 epoch 83 - iter 8/22 - loss 0.05643883 - samples/sec: 96.14 - lr: 0.003125\n",
      "2022-01-13 10:08:45,802 epoch 83 - iter 10/22 - loss 0.05830273 - samples/sec: 97.85 - lr: 0.003125\n",
      "2022-01-13 10:08:46,405 epoch 83 - iter 12/22 - loss 0.05819737 - samples/sec: 106.11 - lr: 0.003125\n",
      "2022-01-13 10:08:46,932 epoch 83 - iter 14/22 - loss 0.05810671 - samples/sec: 121.70 - lr: 0.003125\n",
      "2022-01-13 10:08:47,486 epoch 83 - iter 16/22 - loss 0.05729209 - samples/sec: 117.29 - lr: 0.003125\n",
      "2022-01-13 10:08:48,083 epoch 83 - iter 18/22 - loss 0.05561221 - samples/sec: 107.11 - lr: 0.003125\n",
      "2022-01-13 10:08:48,556 epoch 83 - iter 20/22 - loss 0.05647163 - samples/sec: 135.46 - lr: 0.003125\n",
      "2022-01-13 10:08:49,024 epoch 83 - iter 22/22 - loss 0.05550782 - samples/sec: 139.02 - lr: 0.003125\n",
      "2022-01-13 10:08:49,024 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-13 10:08:49,032 EPOCH 83 done: loss 0.0555 - lr 0.0031250\n",
      "2022-01-13 10:08:50,455 DEV : loss 0.022140420973300934 - f1-score (micro avg)  0.9149\n",
      "2022-01-13 10:08:50,502 BAD EPOCHS (no improvement): 3\n",
      "2022-01-13 10:08:50,502 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-13 10:08:50,928 epoch 84 - iter 2/22 - loss 0.04738999 - samples/sec: 150.35 - lr: 0.003125\n",
      "2022-01-13 10:08:51,487 epoch 84 - iter 4/22 - loss 0.05177838 - samples/sec: 114.37 - lr: 0.003125\n",
      "2022-01-13 10:08:51,943 epoch 84 - iter 6/22 - loss 0.05877705 - samples/sec: 140.57 - lr: 0.003125\n",
      "2022-01-13 10:08:52,431 epoch 84 - iter 8/22 - loss 0.05819754 - samples/sec: 131.06 - lr: 0.003125\n",
      "2022-01-13 10:08:53,002 epoch 84 - iter 10/22 - loss 0.05967280 - samples/sec: 112.09 - lr: 0.003125\n",
      "2022-01-13 10:08:53,509 epoch 84 - iter 12/22 - loss 0.05986132 - samples/sec: 127.07 - lr: 0.003125\n",
      "2022-01-13 10:08:54,086 epoch 84 - iter 14/22 - loss 0.05994615 - samples/sec: 111.01 - lr: 0.003125\n",
      "2022-01-13 10:08:54,669 epoch 84 - iter 16/22 - loss 0.05860287 - samples/sec: 109.78 - lr: 0.003125\n",
      "2022-01-13 10:08:55,162 epoch 84 - iter 18/22 - loss 0.05885369 - samples/sec: 129.65 - lr: 0.003125\n",
      "2022-01-13 10:08:55,750 epoch 84 - iter 20/22 - loss 0.05765015 - samples/sec: 108.79 - lr: 0.003125\n",
      "2022-01-13 10:08:56,398 epoch 84 - iter 22/22 - loss 0.05657649 - samples/sec: 98.79 - lr: 0.003125\n",
      "2022-01-13 10:08:56,398 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-13 10:08:56,398 EPOCH 84 done: loss 0.0566 - lr 0.0031250\n",
      "2022-01-13 10:08:58,378 DEV : loss 0.022220799699425697 - f1-score (micro avg)  0.9199\n",
      "2022-01-13 10:08:58,410 BAD EPOCHS (no improvement): 0\n",
      "2022-01-13 10:08:58,410 saving best model\n",
      "2022-01-13 10:09:03,398 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-13 10:09:04,014 epoch 85 - iter 2/22 - loss 0.06952878 - samples/sec: 104.01 - lr: 0.003125\n",
      "2022-01-13 10:09:04,527 epoch 85 - iter 4/22 - loss 0.05720662 - samples/sec: 124.57 - lr: 0.003125\n",
      "2022-01-13 10:09:04,989 epoch 85 - iter 6/22 - loss 0.05277563 - samples/sec: 138.69 - lr: 0.003125\n",
      "2022-01-13 10:09:05,455 epoch 85 - iter 8/22 - loss 0.05430273 - samples/sec: 137.86 - lr: 0.003125\n",
      "2022-01-13 10:09:06,001 epoch 85 - iter 10/22 - loss 0.05534163 - samples/sec: 117.21 - lr: 0.003125\n",
      "2022-01-13 10:09:06,523 epoch 85 - iter 12/22 - loss 0.05742849 - samples/sec: 122.63 - lr: 0.003125\n",
      "2022-01-13 10:09:06,982 epoch 85 - iter 14/22 - loss 0.05751489 - samples/sec: 139.55 - lr: 0.003125\n",
      "2022-01-13 10:09:07,497 epoch 85 - iter 16/22 - loss 0.05793062 - samples/sec: 124.31 - lr: 0.003125\n",
      "2022-01-13 10:09:08,006 epoch 85 - iter 18/22 - loss 0.05883511 - samples/sec: 125.82 - lr: 0.003125\n",
      "2022-01-13 10:09:08,574 epoch 85 - iter 20/22 - loss 0.05881136 - samples/sec: 112.52 - lr: 0.003125\n",
      "2022-01-13 10:09:09,178 epoch 85 - iter 22/22 - loss 0.05970582 - samples/sec: 106.01 - lr: 0.003125\n",
      "2022-01-13 10:09:09,186 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-13 10:09:09,186 EPOCH 85 done: loss 0.0597 - lr 0.0031250\n",
      "2022-01-13 10:09:10,860 DEV : loss 0.022127896547317505 - f1-score (micro avg)  0.9185\n",
      "2022-01-13 10:09:10,884 BAD EPOCHS (no improvement): 1\n",
      "2022-01-13 10:09:10,892 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-13 10:09:11,392 epoch 86 - iter 2/22 - loss 0.05954751 - samples/sec: 127.84 - lr: 0.003125\n",
      "2022-01-13 10:09:11,800 epoch 86 - iter 4/22 - loss 0.05795304 - samples/sec: 160.10 - lr: 0.003125\n",
      "2022-01-13 10:09:12,512 epoch 86 - iter 6/22 - loss 0.05488470 - samples/sec: 89.92 - lr: 0.003125\n",
      "2022-01-13 10:09:12,988 epoch 86 - iter 8/22 - loss 0.05640294 - samples/sec: 134.37 - lr: 0.003125\n",
      "2022-01-13 10:09:13,540 epoch 86 - iter 10/22 - loss 0.05536743 - samples/sec: 115.91 - lr: 0.003125\n",
      "2022-01-13 10:09:14,153 epoch 86 - iter 12/22 - loss 0.05472884 - samples/sec: 104.48 - lr: 0.003125\n",
      "2022-01-13 10:09:14,642 epoch 86 - iter 14/22 - loss 0.05800718 - samples/sec: 130.92 - lr: 0.003125\n",
      "2022-01-13 10:09:15,185 epoch 86 - iter 16/22 - loss 0.05726941 - samples/sec: 117.68 - lr: 0.003125\n",
      "2022-01-13 10:09:15,638 epoch 86 - iter 18/22 - loss 0.05642929 - samples/sec: 143.88 - lr: 0.003125\n",
      "2022-01-13 10:09:16,129 epoch 86 - iter 20/22 - loss 0.05568845 - samples/sec: 130.45 - lr: 0.003125\n",
      "2022-01-13 10:09:16,599 epoch 86 - iter 22/22 - loss 0.05604475 - samples/sec: 136.01 - lr: 0.003125\n",
      "2022-01-13 10:09:16,599 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-13 10:09:16,599 EPOCH 86 done: loss 0.0560 - lr 0.0031250\n",
      "2022-01-13 10:09:18,255 DEV : loss 0.021870117634534836 - f1-score (micro avg)  0.9164\n",
      "2022-01-13 10:09:18,295 BAD EPOCHS (no improvement): 2\n",
      "2022-01-13 10:09:18,295 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-13 10:09:18,817 epoch 87 - iter 2/22 - loss 0.05908584 - samples/sec: 124.53 - lr: 0.003125\n",
      "2022-01-13 10:09:19,367 epoch 87 - iter 4/22 - loss 0.05342044 - samples/sec: 116.42 - lr: 0.003125\n",
      "2022-01-13 10:09:19,874 epoch 87 - iter 6/22 - loss 0.05282798 - samples/sec: 126.06 - lr: 0.003125\n",
      "2022-01-13 10:09:20,387 epoch 87 - iter 8/22 - loss 0.05075751 - samples/sec: 124.72 - lr: 0.003125\n",
      "2022-01-13 10:09:20,867 epoch 87 - iter 10/22 - loss 0.05342460 - samples/sec: 133.52 - lr: 0.003125\n",
      "2022-01-13 10:09:21,401 epoch 87 - iter 12/22 - loss 0.05382283 - samples/sec: 123.29 - lr: 0.003125\n",
      "2022-01-13 10:09:21,773 epoch 87 - iter 14/22 - loss 0.05583372 - samples/sec: 172.07 - lr: 0.003125\n",
      "2022-01-13 10:09:22,328 epoch 87 - iter 16/22 - loss 0.05703833 - samples/sec: 115.34 - lr: 0.003125\n",
      "2022-01-13 10:09:22,819 epoch 87 - iter 18/22 - loss 0.05694384 - samples/sec: 132.30 - lr: 0.003125\n",
      "2022-01-13 10:09:23,480 epoch 87 - iter 20/22 - loss 0.05812257 - samples/sec: 96.81 - lr: 0.003125\n",
      "2022-01-13 10:09:23,977 epoch 87 - iter 22/22 - loss 0.05913047 - samples/sec: 128.74 - lr: 0.003125\n",
      "2022-01-13 10:09:23,977 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-13 10:09:23,977 EPOCH 87 done: loss 0.0591 - lr 0.0031250\n",
      "2022-01-13 10:09:25,436 DEV : loss 0.02202322520315647 - f1-score (micro avg)  0.9178\n",
      "2022-01-13 10:09:25,479 BAD EPOCHS (no improvement): 3\n",
      "2022-01-13 10:09:25,483 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-13 10:09:25,880 epoch 88 - iter 2/22 - loss 0.04291764 - samples/sec: 161.44 - lr: 0.003125\n",
      "2022-01-13 10:09:26,341 epoch 88 - iter 4/22 - loss 0.05529897 - samples/sec: 138.61 - lr: 0.003125\n",
      "2022-01-13 10:09:26,782 epoch 88 - iter 6/22 - loss 0.05891113 - samples/sec: 145.34 - lr: 0.003125\n",
      "2022-01-13 10:09:27,413 epoch 88 - iter 8/22 - loss 0.05743782 - samples/sec: 103.89 - lr: 0.003125\n",
      "2022-01-13 10:09:27,867 epoch 88 - iter 10/22 - loss 0.05975232 - samples/sec: 140.94 - lr: 0.003125\n",
      "2022-01-13 10:09:28,397 epoch 88 - iter 12/22 - loss 0.06128900 - samples/sec: 120.83 - lr: 0.003125\n",
      "2022-01-13 10:09:28,822 epoch 88 - iter 14/22 - loss 0.06161997 - samples/sec: 150.72 - lr: 0.003125\n",
      "2022-01-13 10:09:29,377 epoch 88 - iter 16/22 - loss 0.05750247 - samples/sec: 115.33 - lr: 0.003125\n",
      "2022-01-13 10:09:29,841 epoch 88 - iter 18/22 - loss 0.05567513 - samples/sec: 137.84 - lr: 0.003125\n",
      "2022-01-13 10:09:30,378 epoch 88 - iter 20/22 - loss 0.05466099 - samples/sec: 119.07 - lr: 0.003125\n",
      "2022-01-13 10:09:30,834 epoch 88 - iter 22/22 - loss 0.05558979 - samples/sec: 140.39 - lr: 0.003125\n",
      "2022-01-13 10:09:30,834 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-13 10:09:30,834 EPOCH 88 done: loss 0.0556 - lr 0.0031250\n",
      "2022-01-13 10:09:32,257 DEV : loss 0.021923361346125603 - f1-score (micro avg)  0.9178\n",
      "Epoch    88: reducing learning rate of group 0 to 1.5625e-03.\n",
      "2022-01-13 10:09:32,304 BAD EPOCHS (no improvement): 4\n",
      "2022-01-13 10:09:32,304 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-13 10:09:32,784 epoch 89 - iter 2/22 - loss 0.05368593 - samples/sec: 133.07 - lr: 0.001563\n",
      "2022-01-13 10:09:33,313 epoch 89 - iter 4/22 - loss 0.06543763 - samples/sec: 121.00 - lr: 0.001563\n",
      "2022-01-13 10:09:33,905 epoch 89 - iter 6/22 - loss 0.06172781 - samples/sec: 108.11 - lr: 0.001563\n",
      "2022-01-13 10:09:34,446 epoch 89 - iter 8/22 - loss 0.05581645 - samples/sec: 118.43 - lr: 0.001563\n",
      "2022-01-13 10:09:34,863 epoch 89 - iter 10/22 - loss 0.05652426 - samples/sec: 153.49 - lr: 0.001563\n",
      "2022-01-13 10:09:35,450 epoch 89 - iter 12/22 - loss 0.05619222 - samples/sec: 111.98 - lr: 0.001563\n",
      "2022-01-13 10:09:36,064 epoch 89 - iter 14/22 - loss 0.05686921 - samples/sec: 105.07 - lr: 0.001563\n",
      "2022-01-13 10:09:36,607 epoch 89 - iter 16/22 - loss 0.05573027 - samples/sec: 121.31 - lr: 0.001563\n",
      "2022-01-13 10:09:37,118 epoch 89 - iter 18/22 - loss 0.05456074 - samples/sec: 125.19 - lr: 0.001563\n",
      "2022-01-13 10:09:37,574 epoch 89 - iter 20/22 - loss 0.05509877 - samples/sec: 140.42 - lr: 0.001563\n",
      "2022-01-13 10:09:38,006 epoch 89 - iter 22/22 - loss 0.05444428 - samples/sec: 148.22 - lr: 0.001563\n",
      "2022-01-13 10:09:38,007 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-13 10:09:38,008 EPOCH 89 done: loss 0.0544 - lr 0.0015625\n",
      "2022-01-13 10:09:39,477 DEV : loss 0.02187913842499256 - f1-score (micro avg)  0.9192\n",
      "2022-01-13 10:09:39,509 BAD EPOCHS (no improvement): 1\n",
      "2022-01-13 10:09:39,509 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-13 10:09:39,931 epoch 90 - iter 2/22 - loss 0.06536474 - samples/sec: 151.49 - lr: 0.001563\n",
      "2022-01-13 10:09:40,418 epoch 90 - iter 4/22 - loss 0.05374888 - samples/sec: 131.49 - lr: 0.001563\n",
      "2022-01-13 10:09:40,886 epoch 90 - iter 6/22 - loss 0.05603057 - samples/sec: 136.73 - lr: 0.001563\n",
      "2022-01-13 10:09:41,344 epoch 90 - iter 8/22 - loss 0.05793542 - samples/sec: 139.62 - lr: 0.001563\n",
      "2022-01-13 10:09:41,850 epoch 90 - iter 10/22 - loss 0.05557378 - samples/sec: 126.57 - lr: 0.001563\n",
      "2022-01-13 10:09:42,362 epoch 90 - iter 12/22 - loss 0.05672314 - samples/sec: 124.98 - lr: 0.001563\n",
      "2022-01-13 10:09:42,772 epoch 90 - iter 14/22 - loss 0.05678131 - samples/sec: 156.07 - lr: 0.001563\n",
      "2022-01-13 10:09:43,358 epoch 90 - iter 16/22 - loss 0.05375984 - samples/sec: 109.29 - lr: 0.001563\n",
      "2022-01-13 10:09:43,856 epoch 90 - iter 18/22 - loss 0.05474960 - samples/sec: 132.67 - lr: 0.001563\n",
      "2022-01-13 10:09:44,381 epoch 90 - iter 20/22 - loss 0.05518530 - samples/sec: 121.86 - lr: 0.001563\n",
      "2022-01-13 10:09:44,841 epoch 90 - iter 22/22 - loss 0.05489878 - samples/sec: 139.00 - lr: 0.001563\n",
      "2022-01-13 10:09:44,841 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-13 10:09:44,841 EPOCH 90 done: loss 0.0549 - lr 0.0015625\n",
      "2022-01-13 10:09:46,556 DEV : loss 0.021748419851064682 - f1-score (micro avg)  0.9178\n",
      "2022-01-13 10:09:46,587 BAD EPOCHS (no improvement): 2\n",
      "2022-01-13 10:09:46,603 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-13 10:09:47,067 epoch 91 - iter 2/22 - loss 0.04584940 - samples/sec: 138.13 - lr: 0.001563\n",
      "2022-01-13 10:09:47,511 epoch 91 - iter 4/22 - loss 0.05269082 - samples/sec: 144.19 - lr: 0.001563\n",
      "2022-01-13 10:09:47,928 epoch 91 - iter 6/22 - loss 0.05808807 - samples/sec: 153.52 - lr: 0.001563\n",
      "2022-01-13 10:09:48,466 epoch 91 - iter 8/22 - loss 0.05889159 - samples/sec: 118.99 - lr: 0.001563\n",
      "2022-01-13 10:09:48,966 epoch 91 - iter 10/22 - loss 0.05909535 - samples/sec: 127.91 - lr: 0.001563\n",
      "2022-01-13 10:09:49,447 epoch 91 - iter 12/22 - loss 0.05811148 - samples/sec: 133.24 - lr: 0.001563\n",
      "2022-01-13 10:09:49,877 epoch 91 - iter 14/22 - loss 0.05795502 - samples/sec: 148.68 - lr: 0.001563\n",
      "2022-01-13 10:09:50,335 epoch 91 - iter 16/22 - loss 0.05738679 - samples/sec: 139.67 - lr: 0.001563\n",
      "2022-01-13 10:09:50,807 epoch 91 - iter 18/22 - loss 0.05686259 - samples/sec: 135.68 - lr: 0.001563\n",
      "2022-01-13 10:09:51,229 epoch 91 - iter 20/22 - loss 0.05891926 - samples/sec: 151.49 - lr: 0.001563\n",
      "2022-01-13 10:09:51,678 epoch 91 - iter 22/22 - loss 0.05818821 - samples/sec: 142.65 - lr: 0.001563\n",
      "2022-01-13 10:09:51,678 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-13 10:09:51,678 EPOCH 91 done: loss 0.0582 - lr 0.0015625\n",
      "2022-01-13 10:09:53,229 DEV : loss 0.021784326061606407 - f1-score (micro avg)  0.9178\n",
      "2022-01-13 10:09:53,269 BAD EPOCHS (no improvement): 3\n",
      "2022-01-13 10:09:53,269 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-13 10:09:53,655 epoch 92 - iter 2/22 - loss 0.07896131 - samples/sec: 165.93 - lr: 0.001563\n",
      "2022-01-13 10:09:54,346 epoch 92 - iter 4/22 - loss 0.08731477 - samples/sec: 92.62 - lr: 0.001563\n",
      "2022-01-13 10:09:54,917 epoch 92 - iter 6/22 - loss 0.08226459 - samples/sec: 112.12 - lr: 0.001563\n",
      "2022-01-13 10:09:55,668 epoch 92 - iter 8/22 - loss 0.07475272 - samples/sec: 85.15 - lr: 0.001563\n",
      "2022-01-13 10:09:56,313 epoch 92 - iter 10/22 - loss 0.07065869 - samples/sec: 100.46 - lr: 0.001563\n",
      "2022-01-13 10:09:56,842 epoch 92 - iter 12/22 - loss 0.06714743 - samples/sec: 121.12 - lr: 0.001563\n",
      "2022-01-13 10:09:57,523 epoch 92 - iter 14/22 - loss 0.06660617 - samples/sec: 93.91 - lr: 0.001563\n",
      "2022-01-13 10:09:58,426 epoch 92 - iter 16/22 - loss 0.06350936 - samples/sec: 70.87 - lr: 0.001563\n",
      "2022-01-13 10:09:59,104 epoch 92 - iter 18/22 - loss 0.06249490 - samples/sec: 94.46 - lr: 0.001563\n",
      "2022-01-13 10:09:59,806 epoch 92 - iter 20/22 - loss 0.06119547 - samples/sec: 92.19 - lr: 0.001563\n",
      "2022-01-13 10:10:00,421 epoch 92 - iter 22/22 - loss 0.06127209 - samples/sec: 104.09 - lr: 0.001563\n",
      "2022-01-13 10:10:00,421 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-13 10:10:00,421 EPOCH 92 done: loss 0.0613 - lr 0.0015625\n",
      "2022-01-13 10:10:01,902 DEV : loss 0.021711384877562523 - f1-score (micro avg)  0.9207\n",
      "2022-01-13 10:10:01,928 BAD EPOCHS (no improvement): 0\n",
      "2022-01-13 10:10:01,944 saving best model\n",
      "2022-01-13 10:10:07,175 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-13 10:10:07,660 epoch 93 - iter 2/22 - loss 0.04619110 - samples/sec: 132.02 - lr: 0.001563\n",
      "2022-01-13 10:10:08,052 epoch 93 - iter 4/22 - loss 0.05068304 - samples/sec: 163.08 - lr: 0.001563\n",
      "2022-01-13 10:10:08,531 epoch 93 - iter 6/22 - loss 0.05348677 - samples/sec: 135.96 - lr: 0.001563\n",
      "2022-01-13 10:10:08,978 epoch 93 - iter 8/22 - loss 0.06069298 - samples/sec: 143.30 - lr: 0.001563\n",
      "2022-01-13 10:10:09,480 epoch 93 - iter 10/22 - loss 0.06040080 - samples/sec: 127.44 - lr: 0.001563\n",
      "2022-01-13 10:10:09,901 epoch 93 - iter 12/22 - loss 0.05842322 - samples/sec: 151.96 - lr: 0.001563\n",
      "2022-01-13 10:10:10,430 epoch 93 - iter 14/22 - loss 0.05843518 - samples/sec: 121.17 - lr: 0.001563\n",
      "2022-01-13 10:10:10,912 epoch 93 - iter 16/22 - loss 0.05843622 - samples/sec: 132.60 - lr: 0.001563\n",
      "2022-01-13 10:10:11,440 epoch 93 - iter 18/22 - loss 0.05720358 - samples/sec: 121.47 - lr: 0.001563\n",
      "2022-01-13 10:10:11,881 epoch 93 - iter 20/22 - loss 0.05734776 - samples/sec: 148.65 - lr: 0.001563\n",
      "2022-01-13 10:10:12,407 epoch 93 - iter 22/22 - loss 0.05734919 - samples/sec: 121.68 - lr: 0.001563\n",
      "2022-01-13 10:10:12,407 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-13 10:10:12,407 EPOCH 93 done: loss 0.0573 - lr 0.0015625\n",
      "2022-01-13 10:10:13,790 DEV : loss 0.02158859185874462 - f1-score (micro avg)  0.9207\n",
      "2022-01-13 10:10:13,823 BAD EPOCHS (no improvement): 0\n",
      "2022-01-13 10:10:13,823 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-13 10:10:14,249 epoch 94 - iter 2/22 - loss 0.05243791 - samples/sec: 150.26 - lr: 0.001563\n",
      "2022-01-13 10:10:14,655 epoch 94 - iter 4/22 - loss 0.05859239 - samples/sec: 157.58 - lr: 0.001563\n",
      "2022-01-13 10:10:15,168 epoch 94 - iter 6/22 - loss 0.05397546 - samples/sec: 124.64 - lr: 0.001563\n",
      "2022-01-13 10:10:15,639 epoch 94 - iter 8/22 - loss 0.05287203 - samples/sec: 136.06 - lr: 0.001563\n",
      "2022-01-13 10:10:16,080 epoch 94 - iter 10/22 - loss 0.05207553 - samples/sec: 145.08 - lr: 0.001563\n",
      "2022-01-13 10:10:16,557 epoch 94 - iter 12/22 - loss 0.05307841 - samples/sec: 134.58 - lr: 0.001563\n",
      "2022-01-13 10:10:16,993 epoch 94 - iter 14/22 - loss 0.05580311 - samples/sec: 146.65 - lr: 0.001563\n",
      "2022-01-13 10:10:17,500 epoch 94 - iter 16/22 - loss 0.05517501 - samples/sec: 126.21 - lr: 0.001563\n",
      "2022-01-13 10:10:17,953 epoch 94 - iter 18/22 - loss 0.05527348 - samples/sec: 141.36 - lr: 0.001563\n",
      "2022-01-13 10:10:18,431 epoch 94 - iter 20/22 - loss 0.05430030 - samples/sec: 133.92 - lr: 0.001563\n",
      "2022-01-13 10:10:18,896 epoch 94 - iter 22/22 - loss 0.05594980 - samples/sec: 137.52 - lr: 0.001563\n",
      "2022-01-13 10:10:18,896 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-13 10:10:18,896 EPOCH 94 done: loss 0.0559 - lr 0.0015625\n",
      "2022-01-13 10:10:20,310 DEV : loss 0.021575678139925003 - f1-score (micro avg)  0.9207\n",
      "2022-01-13 10:10:20,336 BAD EPOCHS (no improvement): 0\n",
      "2022-01-13 10:10:20,336 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-13 10:10:20,805 epoch 95 - iter 2/22 - loss 0.04625906 - samples/sec: 139.63 - lr: 0.001563\n",
      "2022-01-13 10:10:21,254 epoch 95 - iter 4/22 - loss 0.04955479 - samples/sec: 144.66 - lr: 0.001563\n",
      "2022-01-13 10:10:21,743 epoch 95 - iter 6/22 - loss 0.05700207 - samples/sec: 130.86 - lr: 0.001563\n",
      "2022-01-13 10:10:22,263 epoch 95 - iter 8/22 - loss 0.05883449 - samples/sec: 123.02 - lr: 0.001563\n",
      "2022-01-13 10:10:22,724 epoch 95 - iter 10/22 - loss 0.05761179 - samples/sec: 138.72 - lr: 0.001563\n",
      "2022-01-13 10:10:23,267 epoch 95 - iter 12/22 - loss 0.05784436 - samples/sec: 118.06 - lr: 0.001563\n",
      "2022-01-13 10:10:23,639 epoch 95 - iter 14/22 - loss 0.05833975 - samples/sec: 171.98 - lr: 0.001563\n",
      "2022-01-13 10:10:24,084 epoch 95 - iter 16/22 - loss 0.05784725 - samples/sec: 143.56 - lr: 0.001563\n",
      "2022-01-13 10:10:24,562 epoch 95 - iter 18/22 - loss 0.05772401 - samples/sec: 134.08 - lr: 0.001563\n",
      "2022-01-13 10:10:24,992 epoch 95 - iter 20/22 - loss 0.05863805 - samples/sec: 148.74 - lr: 0.001563\n",
      "2022-01-13 10:10:25,443 epoch 95 - iter 22/22 - loss 0.05804847 - samples/sec: 142.21 - lr: 0.001563\n",
      "2022-01-13 10:10:25,443 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-13 10:10:25,443 EPOCH 95 done: loss 0.0580 - lr 0.0015625\n",
      "2022-01-13 10:10:27,070 DEV : loss 0.02154368907213211 - f1-score (micro avg)  0.9221\n",
      "2022-01-13 10:10:27,119 BAD EPOCHS (no improvement): 0\n",
      "2022-01-13 10:10:27,119 saving best model\n",
      "2022-01-13 10:10:31,987 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-13 10:10:32,501 epoch 96 - iter 2/22 - loss 0.05092343 - samples/sec: 128.40 - lr: 0.001563\n",
      "2022-01-13 10:10:33,082 epoch 96 - iter 4/22 - loss 0.05480170 - samples/sec: 110.06 - lr: 0.001563\n",
      "2022-01-13 10:10:33,593 epoch 96 - iter 6/22 - loss 0.05431437 - samples/sec: 125.20 - lr: 0.001563\n",
      "2022-01-13 10:10:34,043 epoch 96 - iter 8/22 - loss 0.06038932 - samples/sec: 142.23 - lr: 0.001563\n",
      "2022-01-13 10:10:34,531 epoch 96 - iter 10/22 - loss 0.05987882 - samples/sec: 131.30 - lr: 0.001563\n",
      "2022-01-13 10:10:34,965 epoch 96 - iter 12/22 - loss 0.05933589 - samples/sec: 147.40 - lr: 0.001563\n",
      "2022-01-13 10:10:35,461 epoch 96 - iter 14/22 - loss 0.05844976 - samples/sec: 129.14 - lr: 0.001563\n",
      "2022-01-13 10:10:35,961 epoch 96 - iter 16/22 - loss 0.05790719 - samples/sec: 127.79 - lr: 0.001563\n",
      "2022-01-13 10:10:36,435 epoch 96 - iter 18/22 - loss 0.05742812 - samples/sec: 135.18 - lr: 0.001563\n",
      "2022-01-13 10:10:36,827 epoch 96 - iter 20/22 - loss 0.06036190 - samples/sec: 162.99 - lr: 0.001563\n",
      "2022-01-13 10:10:37,318 epoch 96 - iter 22/22 - loss 0.06094452 - samples/sec: 130.41 - lr: 0.001563\n",
      "2022-01-13 10:10:37,318 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-13 10:10:37,318 EPOCH 96 done: loss 0.0609 - lr 0.0015625\n",
      "2022-01-13 10:10:38,757 DEV : loss 0.021447090432047844 - f1-score (micro avg)  0.9207\n",
      "2022-01-13 10:10:38,790 BAD EPOCHS (no improvement): 1\n",
      "2022-01-13 10:10:38,790 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-13 10:10:39,244 epoch 97 - iter 2/22 - loss 0.05962136 - samples/sec: 143.89 - lr: 0.001563\n",
      "2022-01-13 10:10:39,701 epoch 97 - iter 4/22 - loss 0.05266428 - samples/sec: 140.01 - lr: 0.001563\n",
      "2022-01-13 10:10:40,199 epoch 97 - iter 6/22 - loss 0.05899708 - samples/sec: 132.53 - lr: 0.001563\n",
      "2022-01-13 10:10:40,686 epoch 97 - iter 8/22 - loss 0.05234866 - samples/sec: 131.45 - lr: 0.001563\n",
      "2022-01-13 10:10:41,084 epoch 97 - iter 10/22 - loss 0.05484754 - samples/sec: 160.87 - lr: 0.001563\n",
      "2022-01-13 10:10:41,607 epoch 97 - iter 12/22 - loss 0.05502568 - samples/sec: 122.30 - lr: 0.001563\n",
      "2022-01-13 10:10:42,029 epoch 97 - iter 14/22 - loss 0.05430270 - samples/sec: 156.53 - lr: 0.001563\n",
      "2022-01-13 10:10:42,551 epoch 97 - iter 16/22 - loss 0.05273413 - samples/sec: 122.58 - lr: 0.001563\n",
      "2022-01-13 10:10:43,020 epoch 97 - iter 18/22 - loss 0.05319444 - samples/sec: 136.39 - lr: 0.001563\n",
      "2022-01-13 10:10:43,557 epoch 97 - iter 20/22 - loss 0.05439073 - samples/sec: 119.23 - lr: 0.001563\n",
      "2022-01-13 10:10:44,022 epoch 97 - iter 22/22 - loss 0.05446155 - samples/sec: 137.48 - lr: 0.001563\n",
      "2022-01-13 10:10:44,022 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-13 10:10:44,022 EPOCH 97 done: loss 0.0545 - lr 0.0015625\n",
      "2022-01-13 10:10:45,571 DEV : loss 0.021452927961945534 - f1-score (micro avg)  0.9235\n",
      "2022-01-13 10:10:45,620 BAD EPOCHS (no improvement): 0\n",
      "2022-01-13 10:10:45,620 saving best model\n",
      "2022-01-13 10:10:51,090 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-13 10:10:51,585 epoch 98 - iter 2/22 - loss 0.04901909 - samples/sec: 129.32 - lr: 0.001563\n",
      "2022-01-13 10:10:52,000 epoch 98 - iter 4/22 - loss 0.05941665 - samples/sec: 154.39 - lr: 0.001563\n",
      "2022-01-13 10:10:52,506 epoch 98 - iter 6/22 - loss 0.05610835 - samples/sec: 126.52 - lr: 0.001563\n",
      "2022-01-13 10:10:52,990 epoch 98 - iter 8/22 - loss 0.05584171 - samples/sec: 132.14 - lr: 0.001563\n",
      "2022-01-13 10:10:53,501 epoch 98 - iter 10/22 - loss 0.05803715 - samples/sec: 125.15 - lr: 0.001563\n",
      "2022-01-13 10:10:53,951 epoch 98 - iter 12/22 - loss 0.05724107 - samples/sec: 142.31 - lr: 0.001563\n",
      "2022-01-13 10:10:54,457 epoch 98 - iter 14/22 - loss 0.05786101 - samples/sec: 126.46 - lr: 0.001563\n",
      "2022-01-13 10:10:54,902 epoch 98 - iter 16/22 - loss 0.05630710 - samples/sec: 149.02 - lr: 0.001563\n",
      "2022-01-13 10:10:55,446 epoch 98 - iter 18/22 - loss 0.05529607 - samples/sec: 117.77 - lr: 0.001563\n",
      "2022-01-13 10:10:55,813 epoch 98 - iter 20/22 - loss 0.05679537 - samples/sec: 174.29 - lr: 0.001563\n",
      "2022-01-13 10:10:56,328 epoch 98 - iter 22/22 - loss 0.05763528 - samples/sec: 125.61 - lr: 0.001563\n",
      "2022-01-13 10:10:56,328 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-13 10:10:56,328 EPOCH 98 done: loss 0.0576 - lr 0.0015625\n",
      "2022-01-13 10:10:57,735 DEV : loss 0.021675579249858856 - f1-score (micro avg)  0.9207\n",
      "2022-01-13 10:10:57,783 BAD EPOCHS (no improvement): 1\n",
      "2022-01-13 10:10:57,783 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-13 10:10:58,279 epoch 99 - iter 2/22 - loss 0.06381628 - samples/sec: 131.02 - lr: 0.001563\n",
      "2022-01-13 10:10:58,743 epoch 99 - iter 4/22 - loss 0.06472869 - samples/sec: 137.85 - lr: 0.001563\n",
      "2022-01-13 10:10:59,169 epoch 99 - iter 6/22 - loss 0.05814035 - samples/sec: 150.22 - lr: 0.001563\n",
      "2022-01-13 10:10:59,614 epoch 99 - iter 8/22 - loss 0.05706807 - samples/sec: 143.78 - lr: 0.001563\n",
      "2022-01-13 10:11:00,099 epoch 99 - iter 10/22 - loss 0.05743494 - samples/sec: 131.89 - lr: 0.001563\n",
      "2022-01-13 10:11:00,575 epoch 99 - iter 12/22 - loss 0.05833154 - samples/sec: 134.49 - lr: 0.001563\n",
      "2022-01-13 10:11:00,995 epoch 99 - iter 14/22 - loss 0.05893151 - samples/sec: 152.59 - lr: 0.001563\n",
      "2022-01-13 10:11:01,505 epoch 99 - iter 16/22 - loss 0.05878303 - samples/sec: 125.43 - lr: 0.001563\n",
      "2022-01-13 10:11:01,950 epoch 99 - iter 18/22 - loss 0.05833488 - samples/sec: 143.87 - lr: 0.001563\n",
      "2022-01-13 10:11:02,449 epoch 99 - iter 20/22 - loss 0.06019381 - samples/sec: 128.09 - lr: 0.001563\n",
      "2022-01-13 10:11:02,866 epoch 99 - iter 22/22 - loss 0.05824096 - samples/sec: 153.67 - lr: 0.001563\n",
      "2022-01-13 10:11:02,866 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-13 10:11:02,866 EPOCH 99 done: loss 0.0582 - lr 0.0015625\n",
      "2022-01-13 10:11:04,290 DEV : loss 0.021753210574388504 - f1-score (micro avg)  0.9192\n",
      "2022-01-13 10:11:04,324 BAD EPOCHS (no improvement): 2\n",
      "2022-01-13 10:11:04,324 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-13 10:11:04,824 epoch 100 - iter 2/22 - loss 0.05712365 - samples/sec: 132.33 - lr: 0.001563\n",
      "2022-01-13 10:11:05,345 epoch 100 - iter 4/22 - loss 0.05340041 - samples/sec: 122.78 - lr: 0.001563\n",
      "2022-01-13 10:11:05,771 epoch 100 - iter 6/22 - loss 0.05242294 - samples/sec: 150.08 - lr: 0.001563\n",
      "2022-01-13 10:11:06,295 epoch 100 - iter 8/22 - loss 0.05362789 - samples/sec: 122.22 - lr: 0.001563\n",
      "2022-01-13 10:11:06,694 epoch 100 - iter 10/22 - loss 0.05254041 - samples/sec: 160.19 - lr: 0.001563\n",
      "2022-01-13 10:11:07,091 epoch 100 - iter 12/22 - loss 0.05491091 - samples/sec: 161.38 - lr: 0.001563\n",
      "2022-01-13 10:11:07,621 epoch 100 - iter 14/22 - loss 0.05552272 - samples/sec: 121.81 - lr: 0.001563\n",
      "2022-01-13 10:11:08,070 epoch 100 - iter 16/22 - loss 0.05702435 - samples/sec: 142.50 - lr: 0.001563\n",
      "2022-01-13 10:11:08,492 epoch 100 - iter 18/22 - loss 0.05761152 - samples/sec: 151.60 - lr: 0.001563\n",
      "2022-01-13 10:11:08,902 epoch 100 - iter 20/22 - loss 0.05979637 - samples/sec: 156.22 - lr: 0.001563\n",
      "2022-01-13 10:11:09,359 epoch 100 - iter 22/22 - loss 0.05918894 - samples/sec: 140.11 - lr: 0.001563\n",
      "2022-01-13 10:11:09,359 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-13 10:11:09,359 EPOCH 100 done: loss 0.0592 - lr 0.0015625\n",
      "2022-01-13 10:11:10,790 DEV : loss 0.02166793681681156 - f1-score (micro avg)  0.9207\n",
      "2022-01-13 10:11:10,826 BAD EPOCHS (no improvement): 3\n",
      "2022-01-13 10:11:10,826 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-13 10:11:11,245 epoch 101 - iter 2/22 - loss 0.05910588 - samples/sec: 154.47 - lr: 0.001563\n",
      "2022-01-13 10:11:11,693 epoch 101 - iter 4/22 - loss 0.04758295 - samples/sec: 143.00 - lr: 0.001563\n",
      "2022-01-13 10:11:12,107 epoch 101 - iter 6/22 - loss 0.05669080 - samples/sec: 154.68 - lr: 0.001563\n",
      "2022-01-13 10:11:12,555 epoch 101 - iter 8/22 - loss 0.06057596 - samples/sec: 142.87 - lr: 0.001563\n",
      "2022-01-13 10:11:13,046 epoch 101 - iter 10/22 - loss 0.05887519 - samples/sec: 130.38 - lr: 0.001563\n",
      "2022-01-13 10:11:13,589 epoch 101 - iter 12/22 - loss 0.05815383 - samples/sec: 117.84 - lr: 0.001563\n",
      "2022-01-13 10:11:14,045 epoch 101 - iter 14/22 - loss 0.05569561 - samples/sec: 140.18 - lr: 0.001563\n",
      "2022-01-13 10:11:14,591 epoch 101 - iter 16/22 - loss 0.05504620 - samples/sec: 117.19 - lr: 0.001563\n",
      "2022-01-13 10:11:15,044 epoch 101 - iter 18/22 - loss 0.05640184 - samples/sec: 141.32 - lr: 0.001563\n",
      "2022-01-13 10:11:15,554 epoch 101 - iter 20/22 - loss 0.05596839 - samples/sec: 125.52 - lr: 0.001563\n",
      "2022-01-13 10:11:16,038 epoch 101 - iter 22/22 - loss 0.05533640 - samples/sec: 134.52 - lr: 0.001563\n",
      "2022-01-13 10:11:16,038 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-13 10:11:16,042 EPOCH 101 done: loss 0.0553 - lr 0.0015625\n",
      "2022-01-13 10:11:17,629 DEV : loss 0.0215266365557909 - f1-score (micro avg)  0.9207\n",
      "Epoch   101: reducing learning rate of group 0 to 7.8125e-04.\n",
      "2022-01-13 10:11:17,676 BAD EPOCHS (no improvement): 4\n",
      "2022-01-13 10:11:17,676 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-13 10:11:18,180 epoch 102 - iter 2/22 - loss 0.07185464 - samples/sec: 127.12 - lr: 0.000781\n",
      "2022-01-13 10:11:18,588 epoch 102 - iter 4/22 - loss 0.07043511 - samples/sec: 156.67 - lr: 0.000781\n",
      "2022-01-13 10:11:18,988 epoch 102 - iter 6/22 - loss 0.06744528 - samples/sec: 160.25 - lr: 0.000781\n",
      "2022-01-13 10:11:19,398 epoch 102 - iter 8/22 - loss 0.06714874 - samples/sec: 155.83 - lr: 0.000781\n",
      "2022-01-13 10:11:19,868 epoch 102 - iter 10/22 - loss 0.06127365 - samples/sec: 136.11 - lr: 0.000781\n",
      "2022-01-13 10:11:20,393 epoch 102 - iter 12/22 - loss 0.05787745 - samples/sec: 122.08 - lr: 0.000781\n",
      "2022-01-13 10:11:20,900 epoch 102 - iter 14/22 - loss 0.05486154 - samples/sec: 126.05 - lr: 0.000781\n",
      "2022-01-13 10:11:21,405 epoch 102 - iter 16/22 - loss 0.05523054 - samples/sec: 126.94 - lr: 0.000781\n",
      "2022-01-13 10:11:21,928 epoch 102 - iter 18/22 - loss 0.05589824 - samples/sec: 124.24 - lr: 0.000781\n",
      "2022-01-13 10:11:22,466 epoch 102 - iter 20/22 - loss 0.05712219 - samples/sec: 131.39 - lr: 0.000781\n",
      "2022-01-13 10:11:22,870 epoch 102 - iter 22/22 - loss 0.05790862 - samples/sec: 158.29 - lr: 0.000781\n",
      "2022-01-13 10:11:22,870 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-13 10:11:22,870 EPOCH 102 done: loss 0.0579 - lr 0.0007813\n",
      "2022-01-13 10:11:24,323 DEV : loss 0.021513089537620544 - f1-score (micro avg)  0.9207\n",
      "2022-01-13 10:11:24,371 BAD EPOCHS (no improvement): 1\n",
      "2022-01-13 10:11:24,371 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-13 10:11:24,770 epoch 103 - iter 2/22 - loss 0.06910334 - samples/sec: 166.85 - lr: 0.000781\n",
      "2022-01-13 10:11:25,236 epoch 103 - iter 4/22 - loss 0.06577448 - samples/sec: 137.29 - lr: 0.000781\n",
      "2022-01-13 10:11:25,664 epoch 103 - iter 6/22 - loss 0.06803871 - samples/sec: 155.24 - lr: 0.000781\n",
      "2022-01-13 10:11:26,086 epoch 103 - iter 8/22 - loss 0.06915042 - samples/sec: 151.84 - lr: 0.000781\n",
      "2022-01-13 10:11:26,603 epoch 103 - iter 10/22 - loss 0.06376449 - samples/sec: 123.64 - lr: 0.000781\n",
      "2022-01-13 10:11:27,018 epoch 103 - iter 12/22 - loss 0.06149934 - samples/sec: 154.27 - lr: 0.000781\n",
      "2022-01-13 10:11:27,508 epoch 103 - iter 14/22 - loss 0.06133854 - samples/sec: 130.66 - lr: 0.000781\n",
      "2022-01-13 10:11:28,074 epoch 103 - iter 16/22 - loss 0.05889953 - samples/sec: 113.00 - lr: 0.000781\n",
      "2022-01-13 10:11:28,557 epoch 103 - iter 18/22 - loss 0.05792625 - samples/sec: 132.49 - lr: 0.000781\n",
      "2022-01-13 10:11:28,943 epoch 103 - iter 20/22 - loss 0.05663326 - samples/sec: 166.17 - lr: 0.000781\n",
      "2022-01-13 10:11:29,468 epoch 103 - iter 22/22 - loss 0.05633647 - samples/sec: 121.77 - lr: 0.000781\n",
      "2022-01-13 10:11:29,468 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-13 10:11:29,468 EPOCH 103 done: loss 0.0563 - lr 0.0007813\n",
      "2022-01-13 10:11:30,820 DEV : loss 0.021478848531842232 - f1-score (micro avg)  0.9207\n",
      "2022-01-13 10:11:30,851 BAD EPOCHS (no improvement): 2\n",
      "2022-01-13 10:11:30,851 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-13 10:11:31,255 epoch 104 - iter 2/22 - loss 0.05829137 - samples/sec: 162.12 - lr: 0.000781\n",
      "2022-01-13 10:11:31,754 epoch 104 - iter 4/22 - loss 0.06163177 - samples/sec: 128.32 - lr: 0.000781\n",
      "2022-01-13 10:11:32,292 epoch 104 - iter 6/22 - loss 0.06028758 - samples/sec: 118.89 - lr: 0.000781\n",
      "2022-01-13 10:11:32,813 epoch 104 - iter 8/22 - loss 0.05796628 - samples/sec: 122.87 - lr: 0.000781\n",
      "2022-01-13 10:11:33,350 epoch 104 - iter 10/22 - loss 0.05834973 - samples/sec: 119.13 - lr: 0.000781\n",
      "2022-01-13 10:11:33,785 epoch 104 - iter 12/22 - loss 0.05822430 - samples/sec: 152.75 - lr: 0.000781\n",
      "2022-01-13 10:11:34,322 epoch 104 - iter 14/22 - loss 0.05782200 - samples/sec: 121.96 - lr: 0.000781\n",
      "2022-01-13 10:11:34,780 epoch 104 - iter 16/22 - loss 0.05744287 - samples/sec: 139.77 - lr: 0.000781\n",
      "2022-01-13 10:11:35,216 epoch 104 - iter 18/22 - loss 0.05897917 - samples/sec: 152.29 - lr: 0.000781\n",
      "2022-01-13 10:11:35,663 epoch 104 - iter 20/22 - loss 0.05770362 - samples/sec: 143.11 - lr: 0.000781\n",
      "2022-01-13 10:11:36,062 epoch 104 - iter 22/22 - loss 0.05830563 - samples/sec: 160.42 - lr: 0.000781\n",
      "2022-01-13 10:11:36,062 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-13 10:11:36,062 EPOCH 104 done: loss 0.0583 - lr 0.0007813\n",
      "2022-01-13 10:11:37,476 DEV : loss 0.021470481529831886 - f1-score (micro avg)  0.9207\n",
      "2022-01-13 10:11:37,512 BAD EPOCHS (no improvement): 3\n",
      "2022-01-13 10:11:37,512 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-13 10:11:37,959 epoch 105 - iter 2/22 - loss 0.04922065 - samples/sec: 145.41 - lr: 0.000781\n",
      "2022-01-13 10:11:38,414 epoch 105 - iter 4/22 - loss 0.05213306 - samples/sec: 140.63 - lr: 0.000781\n",
      "2022-01-13 10:11:38,829 epoch 105 - iter 6/22 - loss 0.05057172 - samples/sec: 154.31 - lr: 0.000781\n",
      "2022-01-13 10:11:39,355 epoch 105 - iter 8/22 - loss 0.05367084 - samples/sec: 121.60 - lr: 0.000781\n",
      "2022-01-13 10:11:39,878 epoch 105 - iter 10/22 - loss 0.05533363 - samples/sec: 122.48 - lr: 0.000781\n",
      "2022-01-13 10:11:40,423 epoch 105 - iter 12/22 - loss 0.05353385 - samples/sec: 117.45 - lr: 0.000781\n",
      "2022-01-13 10:11:40,841 epoch 105 - iter 14/22 - loss 0.05378712 - samples/sec: 154.24 - lr: 0.000781\n",
      "2022-01-13 10:11:41,283 epoch 105 - iter 16/22 - loss 0.05510527 - samples/sec: 144.78 - lr: 0.000781\n",
      "2022-01-13 10:11:41,711 epoch 105 - iter 18/22 - loss 0.05645640 - samples/sec: 149.46 - lr: 0.000781\n",
      "2022-01-13 10:11:42,153 epoch 105 - iter 20/22 - loss 0.05557377 - samples/sec: 144.80 - lr: 0.000781\n",
      "2022-01-13 10:11:42,597 epoch 105 - iter 22/22 - loss 0.05588664 - samples/sec: 144.13 - lr: 0.000781\n",
      "2022-01-13 10:11:42,597 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-13 10:11:42,597 EPOCH 105 done: loss 0.0559 - lr 0.0007813\n",
      "2022-01-13 10:11:44,026 DEV : loss 0.021451495587825775 - f1-score (micro avg)  0.9235\n",
      "2022-01-13 10:11:44,066 BAD EPOCHS (no improvement): 0\n",
      "2022-01-13 10:11:44,066 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-13 10:11:44,536 epoch 106 - iter 2/22 - loss 0.05249631 - samples/sec: 137.81 - lr: 0.000781\n",
      "2022-01-13 10:11:44,897 epoch 106 - iter 4/22 - loss 0.04708733 - samples/sec: 181.40 - lr: 0.000781\n",
      "2022-01-13 10:11:45,418 epoch 106 - iter 6/22 - loss 0.05462998 - samples/sec: 122.72 - lr: 0.000781\n",
      "2022-01-13 10:11:45,924 epoch 106 - iter 8/22 - loss 0.05968409 - samples/sec: 126.61 - lr: 0.000781\n",
      "2022-01-13 10:11:46,456 epoch 106 - iter 10/22 - loss 0.06013373 - samples/sec: 120.20 - lr: 0.000781\n",
      "2022-01-13 10:11:46,866 epoch 106 - iter 12/22 - loss 0.05960051 - samples/sec: 156.04 - lr: 0.000781\n",
      "2022-01-13 10:11:47,291 epoch 106 - iter 14/22 - loss 0.05945304 - samples/sec: 150.69 - lr: 0.000781\n",
      "2022-01-13 10:11:47,665 epoch 106 - iter 16/22 - loss 0.05890302 - samples/sec: 170.98 - lr: 0.000781\n",
      "2022-01-13 10:11:48,015 epoch 106 - iter 18/22 - loss 0.05985352 - samples/sec: 182.88 - lr: 0.000781\n",
      "2022-01-13 10:11:48,389 epoch 106 - iter 20/22 - loss 0.06101406 - samples/sec: 171.15 - lr: 0.000781\n",
      "2022-01-13 10:11:48,684 epoch 106 - iter 22/22 - loss 0.06003887 - samples/sec: 216.98 - lr: 0.000781\n",
      "2022-01-13 10:11:48,684 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-13 10:11:48,684 EPOCH 106 done: loss 0.0600 - lr 0.0007813\n",
      "2022-01-13 10:11:49,947 DEV : loss 0.021383697167038918 - f1-score (micro avg)  0.9221\n",
      "2022-01-13 10:11:49,980 BAD EPOCHS (no improvement): 1\n",
      "2022-01-13 10:11:49,980 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-13 10:11:50,313 epoch 107 - iter 2/22 - loss 0.05223150 - samples/sec: 194.54 - lr: 0.000781\n",
      "2022-01-13 10:11:50,675 epoch 107 - iter 4/22 - loss 0.06148938 - samples/sec: 176.84 - lr: 0.000781\n",
      "2022-01-13 10:11:51,138 epoch 107 - iter 6/22 - loss 0.05645548 - samples/sec: 138.28 - lr: 0.000781\n",
      "2022-01-13 10:11:51,618 epoch 107 - iter 8/22 - loss 0.06259740 - samples/sec: 133.19 - lr: 0.000781\n",
      "2022-01-13 10:11:52,038 epoch 107 - iter 10/22 - loss 0.06344057 - samples/sec: 152.50 - lr: 0.000781\n",
      "2022-01-13 10:11:52,405 epoch 107 - iter 12/22 - loss 0.06228751 - samples/sec: 174.34 - lr: 0.000781\n",
      "2022-01-13 10:11:52,830 epoch 107 - iter 14/22 - loss 0.06277997 - samples/sec: 150.60 - lr: 0.000781\n",
      "2022-01-13 10:11:53,253 epoch 107 - iter 16/22 - loss 0.06244012 - samples/sec: 151.39 - lr: 0.000781\n",
      "2022-01-13 10:11:53,622 epoch 107 - iter 18/22 - loss 0.06270902 - samples/sec: 173.18 - lr: 0.000781\n",
      "2022-01-13 10:11:53,987 epoch 107 - iter 20/22 - loss 0.06229991 - samples/sec: 175.36 - lr: 0.000781\n",
      "2022-01-13 10:11:54,357 epoch 107 - iter 22/22 - loss 0.06174696 - samples/sec: 180.75 - lr: 0.000781\n",
      "2022-01-13 10:11:54,357 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-13 10:11:54,357 EPOCH 107 done: loss 0.0617 - lr 0.0007813\n",
      "2022-01-13 10:11:55,537 DEV : loss 0.021398238837718964 - f1-score (micro avg)  0.9221\n",
      "2022-01-13 10:11:55,576 BAD EPOCHS (no improvement): 2\n",
      "2022-01-13 10:11:55,576 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-13 10:11:56,033 epoch 108 - iter 2/22 - loss 0.04463028 - samples/sec: 142.00 - lr: 0.000781\n",
      "2022-01-13 10:11:56,394 epoch 108 - iter 4/22 - loss 0.05441769 - samples/sec: 177.61 - lr: 0.000781\n",
      "2022-01-13 10:11:56,812 epoch 108 - iter 6/22 - loss 0.05680430 - samples/sec: 152.86 - lr: 0.000781\n",
      "2022-01-13 10:11:57,267 epoch 108 - iter 8/22 - loss 0.05723516 - samples/sec: 140.70 - lr: 0.000781\n",
      "2022-01-13 10:11:57,636 epoch 108 - iter 10/22 - loss 0.05481702 - samples/sec: 173.62 - lr: 0.000781\n",
      "2022-01-13 10:11:58,076 epoch 108 - iter 12/22 - loss 0.05731875 - samples/sec: 145.49 - lr: 0.000781\n",
      "2022-01-13 10:11:58,500 epoch 108 - iter 14/22 - loss 0.05531039 - samples/sec: 150.87 - lr: 0.000781\n",
      "2022-01-13 10:11:58,792 epoch 108 - iter 16/22 - loss 0.05303926 - samples/sec: 220.26 - lr: 0.000781\n",
      "2022-01-13 10:11:59,220 epoch 108 - iter 18/22 - loss 0.05361848 - samples/sec: 149.25 - lr: 0.000781\n",
      "2022-01-13 10:11:59,587 epoch 108 - iter 20/22 - loss 0.05347757 - samples/sec: 174.65 - lr: 0.000781\n",
      "2022-01-13 10:11:59,881 epoch 108 - iter 22/22 - loss 0.05284397 - samples/sec: 217.68 - lr: 0.000781\n",
      "2022-01-13 10:11:59,881 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-13 10:11:59,881 EPOCH 108 done: loss 0.0528 - lr 0.0007813\n",
      "2022-01-13 10:12:01,005 DEV : loss 0.021344196051359177 - f1-score (micro avg)  0.9221\n",
      "2022-01-13 10:12:01,037 BAD EPOCHS (no improvement): 3\n",
      "2022-01-13 10:12:01,037 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-13 10:12:01,419 epoch 109 - iter 2/22 - loss 0.06050962 - samples/sec: 170.81 - lr: 0.000781\n",
      "2022-01-13 10:12:01,729 epoch 109 - iter 4/22 - loss 0.06933924 - samples/sec: 206.74 - lr: 0.000781\n",
      "2022-01-13 10:12:02,152 epoch 109 - iter 6/22 - loss 0.06167074 - samples/sec: 151.29 - lr: 0.000781\n",
      "2022-01-13 10:12:02,531 epoch 109 - iter 8/22 - loss 0.06135061 - samples/sec: 168.45 - lr: 0.000781\n",
      "2022-01-13 10:12:02,909 epoch 109 - iter 10/22 - loss 0.06086881 - samples/sec: 169.47 - lr: 0.000781\n",
      "2022-01-13 10:12:03,315 epoch 109 - iter 12/22 - loss 0.05875175 - samples/sec: 157.63 - lr: 0.000781\n",
      "2022-01-13 10:12:03,722 epoch 109 - iter 14/22 - loss 0.05829239 - samples/sec: 157.42 - lr: 0.000781\n",
      "2022-01-13 10:12:04,033 epoch 109 - iter 16/22 - loss 0.05714305 - samples/sec: 205.72 - lr: 0.000781\n",
      "2022-01-13 10:12:04,493 epoch 109 - iter 18/22 - loss 0.05733027 - samples/sec: 139.18 - lr: 0.000781\n",
      "2022-01-13 10:12:04,924 epoch 109 - iter 20/22 - loss 0.05575343 - samples/sec: 148.48 - lr: 0.000781\n",
      "2022-01-13 10:12:05,290 epoch 109 - iter 22/22 - loss 0.05519003 - samples/sec: 174.71 - lr: 0.000781\n",
      "2022-01-13 10:12:05,306 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-13 10:12:05,306 EPOCH 109 done: loss 0.0552 - lr 0.0007813\n",
      "2022-01-13 10:12:06,440 DEV : loss 0.02128119207918644 - f1-score (micro avg)  0.9221\n",
      "Epoch   109: reducing learning rate of group 0 to 3.9063e-04.\n",
      "2022-01-13 10:12:06,452 BAD EPOCHS (no improvement): 4\n",
      "2022-01-13 10:12:06,468 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-13 10:12:06,844 epoch 110 - iter 2/22 - loss 0.05889811 - samples/sec: 171.11 - lr: 0.000391\n",
      "2022-01-13 10:12:07,261 epoch 110 - iter 4/22 - loss 0.05806823 - samples/sec: 153.25 - lr: 0.000391\n",
      "2022-01-13 10:12:07,655 epoch 110 - iter 6/22 - loss 0.05780332 - samples/sec: 169.39 - lr: 0.000391\n",
      "2022-01-13 10:12:08,048 epoch 110 - iter 8/22 - loss 0.05618879 - samples/sec: 162.75 - lr: 0.000391\n",
      "2022-01-13 10:12:08,397 epoch 110 - iter 10/22 - loss 0.05477518 - samples/sec: 183.63 - lr: 0.000391\n",
      "2022-01-13 10:12:08,803 epoch 110 - iter 12/22 - loss 0.05465063 - samples/sec: 157.38 - lr: 0.000391\n",
      "2022-01-13 10:12:09,213 epoch 110 - iter 14/22 - loss 0.05408306 - samples/sec: 156.37 - lr: 0.000391\n",
      "2022-01-13 10:12:09,584 epoch 110 - iter 16/22 - loss 0.05337827 - samples/sec: 172.50 - lr: 0.000391\n",
      "2022-01-13 10:12:09,990 epoch 110 - iter 18/22 - loss 0.05379620 - samples/sec: 157.52 - lr: 0.000391\n",
      "2022-01-13 10:12:10,385 epoch 110 - iter 20/22 - loss 0.05583207 - samples/sec: 161.97 - lr: 0.000391\n",
      "2022-01-13 10:12:10,715 epoch 110 - iter 22/22 - loss 0.05566991 - samples/sec: 197.33 - lr: 0.000391\n",
      "2022-01-13 10:12:10,715 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-13 10:12:10,715 EPOCH 110 done: loss 0.0557 - lr 0.0003906\n",
      "2022-01-13 10:12:12,013 DEV : loss 0.02133321575820446 - f1-score (micro avg)  0.9221\n",
      "2022-01-13 10:12:12,045 BAD EPOCHS (no improvement): 1\n",
      "2022-01-13 10:12:12,045 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-13 10:12:12,457 epoch 111 - iter 2/22 - loss 0.05559711 - samples/sec: 159.60 - lr: 0.000391\n",
      "2022-01-13 10:12:12,807 epoch 111 - iter 4/22 - loss 0.05496544 - samples/sec: 183.04 - lr: 0.000391\n",
      "2022-01-13 10:12:13,345 epoch 111 - iter 6/22 - loss 0.05193191 - samples/sec: 119.16 - lr: 0.000391\n",
      "2022-01-13 10:12:13,693 epoch 111 - iter 8/22 - loss 0.05615198 - samples/sec: 184.02 - lr: 0.000391\n",
      "2022-01-13 10:12:14,194 epoch 111 - iter 10/22 - loss 0.05692690 - samples/sec: 144.56 - lr: 0.000391\n",
      "2022-01-13 10:12:14,595 epoch 111 - iter 12/22 - loss 0.05934705 - samples/sec: 159.69 - lr: 0.000391\n",
      "2022-01-13 10:12:15,035 epoch 111 - iter 14/22 - loss 0.05966830 - samples/sec: 145.52 - lr: 0.000391\n",
      "2022-01-13 10:12:15,516 epoch 111 - iter 16/22 - loss 0.06103564 - samples/sec: 133.03 - lr: 0.000391\n",
      "2022-01-13 10:12:15,892 epoch 111 - iter 18/22 - loss 0.06062795 - samples/sec: 170.85 - lr: 0.000391\n",
      "2022-01-13 10:12:16,304 epoch 111 - iter 20/22 - loss 0.06039208 - samples/sec: 155.31 - lr: 0.000391\n",
      "2022-01-13 10:12:16,738 epoch 111 - iter 22/22 - loss 0.06031885 - samples/sec: 147.56 - lr: 0.000391\n",
      "2022-01-13 10:12:16,738 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-13 10:12:16,745 EPOCH 111 done: loss 0.0603 - lr 0.0003906\n",
      "2022-01-13 10:12:18,202 DEV : loss 0.02135380357503891 - f1-score (micro avg)  0.9221\n",
      "2022-01-13 10:12:18,237 BAD EPOCHS (no improvement): 2\n",
      "2022-01-13 10:12:18,238 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-13 10:12:18,796 epoch 112 - iter 2/22 - loss 0.06908709 - samples/sec: 115.28 - lr: 0.000391\n",
      "2022-01-13 10:12:19,389 epoch 112 - iter 4/22 - loss 0.05984412 - samples/sec: 108.23 - lr: 0.000391\n",
      "2022-01-13 10:12:19,891 epoch 112 - iter 6/22 - loss 0.05709941 - samples/sec: 128.00 - lr: 0.000391\n",
      "2022-01-13 10:12:20,400 epoch 112 - iter 8/22 - loss 0.05662413 - samples/sec: 126.20 - lr: 0.000391\n",
      "2022-01-13 10:12:20,848 epoch 112 - iter 10/22 - loss 0.05763486 - samples/sec: 143.32 - lr: 0.000391\n",
      "2022-01-13 10:12:21,504 epoch 112 - iter 12/22 - loss 0.05662264 - samples/sec: 97.82 - lr: 0.000391\n",
      "2022-01-13 10:12:21,969 epoch 112 - iter 14/22 - loss 0.05682767 - samples/sec: 138.59 - lr: 0.000391\n",
      "2022-01-13 10:12:22,408 epoch 112 - iter 16/22 - loss 0.05773552 - samples/sec: 145.80 - lr: 0.000391\n",
      "2022-01-13 10:12:22,861 epoch 112 - iter 18/22 - loss 0.05753686 - samples/sec: 141.32 - lr: 0.000391\n",
      "2022-01-13 10:12:23,328 epoch 112 - iter 20/22 - loss 0.05769554 - samples/sec: 137.55 - lr: 0.000391\n",
      "2022-01-13 10:12:23,738 epoch 112 - iter 22/22 - loss 0.05935956 - samples/sec: 156.02 - lr: 0.000391\n",
      "2022-01-13 10:12:23,738 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-13 10:12:23,738 EPOCH 112 done: loss 0.0594 - lr 0.0003906\n",
      "2022-01-13 10:12:25,237 DEV : loss 0.021347777917981148 - f1-score (micro avg)  0.9221\n",
      "2022-01-13 10:12:25,253 BAD EPOCHS (no improvement): 3\n",
      "2022-01-13 10:12:25,268 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-13 10:12:25,761 epoch 113 - iter 2/22 - loss 0.05968328 - samples/sec: 130.18 - lr: 0.000391\n",
      "2022-01-13 10:12:26,324 epoch 113 - iter 4/22 - loss 0.05341522 - samples/sec: 113.62 - lr: 0.000391\n",
      "2022-01-13 10:12:26,651 epoch 113 - iter 6/22 - loss 0.05220665 - samples/sec: 196.09 - lr: 0.000391\n",
      "2022-01-13 10:12:27,169 epoch 113 - iter 8/22 - loss 0.05178551 - samples/sec: 123.49 - lr: 0.000391\n",
      "2022-01-13 10:12:27,582 epoch 113 - iter 10/22 - loss 0.05599239 - samples/sec: 154.93 - lr: 0.000391\n",
      "2022-01-13 10:12:28,026 epoch 113 - iter 12/22 - loss 0.05389520 - samples/sec: 144.07 - lr: 0.000391\n",
      "2022-01-13 10:12:28,425 epoch 113 - iter 14/22 - loss 0.05437711 - samples/sec: 160.60 - lr: 0.000391\n",
      "2022-01-13 10:12:28,774 epoch 113 - iter 16/22 - loss 0.05588894 - samples/sec: 183.23 - lr: 0.000391\n",
      "2022-01-13 10:12:29,232 epoch 113 - iter 18/22 - loss 0.05407909 - samples/sec: 139.57 - lr: 0.000391\n",
      "2022-01-13 10:12:29,626 epoch 113 - iter 20/22 - loss 0.05378957 - samples/sec: 162.61 - lr: 0.000391\n",
      "2022-01-13 10:12:29,932 epoch 113 - iter 22/22 - loss 0.05394498 - samples/sec: 208.88 - lr: 0.000391\n",
      "2022-01-13 10:12:29,932 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-13 10:12:29,932 EPOCH 113 done: loss 0.0539 - lr 0.0003906\n",
      "2022-01-13 10:12:31,246 DEV : loss 0.021353481337428093 - f1-score (micro avg)  0.9221\n",
      "Epoch   113: reducing learning rate of group 0 to 1.9531e-04.\n",
      "2022-01-13 10:12:31,277 BAD EPOCHS (no improvement): 4\n",
      "2022-01-13 10:12:31,277 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-13 10:12:31,690 epoch 114 - iter 2/22 - loss 0.05326855 - samples/sec: 159.59 - lr: 0.000195\n",
      "2022-01-13 10:12:32,000 epoch 114 - iter 4/22 - loss 0.06321080 - samples/sec: 206.71 - lr: 0.000195\n",
      "2022-01-13 10:12:32,428 epoch 114 - iter 6/22 - loss 0.05744070 - samples/sec: 149.42 - lr: 0.000195\n",
      "2022-01-13 10:12:32,825 epoch 114 - iter 8/22 - loss 0.05823085 - samples/sec: 161.42 - lr: 0.000195\n",
      "2022-01-13 10:12:33,276 epoch 114 - iter 10/22 - loss 0.05900815 - samples/sec: 141.79 - lr: 0.000195\n",
      "2022-01-13 10:12:33,669 epoch 114 - iter 12/22 - loss 0.05869719 - samples/sec: 163.16 - lr: 0.000195\n",
      "2022-01-13 10:12:34,008 epoch 114 - iter 14/22 - loss 0.05733694 - samples/sec: 188.77 - lr: 0.000195\n",
      "2022-01-13 10:12:34,466 epoch 114 - iter 16/22 - loss 0.05524571 - samples/sec: 139.62 - lr: 0.000195\n",
      "2022-01-13 10:12:34,911 epoch 114 - iter 18/22 - loss 0.05514339 - samples/sec: 143.74 - lr: 0.000195\n",
      "2022-01-13 10:12:35,347 epoch 114 - iter 20/22 - loss 0.05660103 - samples/sec: 146.71 - lr: 0.000195\n",
      "2022-01-13 10:12:35,714 epoch 114 - iter 22/22 - loss 0.05681912 - samples/sec: 174.36 - lr: 0.000195\n",
      "2022-01-13 10:12:35,714 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-13 10:12:35,714 EPOCH 114 done: loss 0.0568 - lr 0.0001953\n",
      "2022-01-13 10:12:36,935 DEV : loss 0.02136887237429619 - f1-score (micro avg)  0.9221\n",
      "2022-01-13 10:12:36,966 BAD EPOCHS (no improvement): 1\n",
      "2022-01-13 10:12:36,966 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-13 10:12:37,368 epoch 115 - iter 2/22 - loss 0.04789945 - samples/sec: 163.20 - lr: 0.000195\n",
      "2022-01-13 10:12:37,783 epoch 115 - iter 4/22 - loss 0.04992550 - samples/sec: 154.28 - lr: 0.000195\n",
      "2022-01-13 10:12:38,094 epoch 115 - iter 6/22 - loss 0.04918303 - samples/sec: 205.89 - lr: 0.000195\n",
      "2022-01-13 10:12:38,547 epoch 115 - iter 8/22 - loss 0.05216281 - samples/sec: 141.07 - lr: 0.000195\n",
      "2022-01-13 10:12:38,936 epoch 115 - iter 10/22 - loss 0.05496562 - samples/sec: 164.59 - lr: 0.000195\n",
      "2022-01-13 10:12:39,337 epoch 115 - iter 12/22 - loss 0.05668864 - samples/sec: 159.66 - lr: 0.000195\n",
      "2022-01-13 10:12:39,687 epoch 115 - iter 14/22 - loss 0.05515303 - samples/sec: 183.03 - lr: 0.000195\n",
      "2022-01-13 10:12:40,097 epoch 115 - iter 16/22 - loss 0.05451522 - samples/sec: 162.11 - lr: 0.000195\n",
      "2022-01-13 10:12:40,444 epoch 115 - iter 18/22 - loss 0.05678577 - samples/sec: 184.36 - lr: 0.000195\n",
      "2022-01-13 10:12:40,918 epoch 115 - iter 20/22 - loss 0.05750656 - samples/sec: 135.24 - lr: 0.000195\n",
      "2022-01-13 10:12:41,285 epoch 115 - iter 22/22 - loss 0.05736457 - samples/sec: 174.27 - lr: 0.000195\n",
      "2022-01-13 10:12:41,285 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-13 10:12:41,285 EPOCH 115 done: loss 0.0574 - lr 0.0001953\n",
      "2022-01-13 10:12:42,545 DEV : loss 0.021359801292419434 - f1-score (micro avg)  0.9221\n",
      "2022-01-13 10:12:42,580 BAD EPOCHS (no improvement): 2\n",
      "2022-01-13 10:12:42,580 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-13 10:12:42,914 epoch 116 - iter 2/22 - loss 0.05645971 - samples/sec: 196.21 - lr: 0.000195\n",
      "2022-01-13 10:12:43,244 epoch 116 - iter 4/22 - loss 0.07098560 - samples/sec: 193.97 - lr: 0.000195\n",
      "2022-01-13 10:12:43,626 epoch 116 - iter 6/22 - loss 0.06199336 - samples/sec: 167.53 - lr: 0.000195\n",
      "2022-01-13 10:12:44,057 epoch 116 - iter 8/22 - loss 0.05975528 - samples/sec: 148.50 - lr: 0.000195\n",
      "2022-01-13 10:12:44,509 epoch 116 - iter 10/22 - loss 0.06441772 - samples/sec: 141.38 - lr: 0.000195\n",
      "2022-01-13 10:12:44,896 epoch 116 - iter 12/22 - loss 0.06330842 - samples/sec: 165.42 - lr: 0.000195\n",
      "2022-01-13 10:12:45,360 epoch 116 - iter 14/22 - loss 0.05988714 - samples/sec: 137.98 - lr: 0.000195\n",
      "2022-01-13 10:12:45,740 epoch 116 - iter 16/22 - loss 0.05937600 - samples/sec: 168.59 - lr: 0.000195\n",
      "2022-01-13 10:12:46,119 epoch 116 - iter 18/22 - loss 0.05892904 - samples/sec: 168.55 - lr: 0.000195\n",
      "2022-01-13 10:12:46,531 epoch 116 - iter 20/22 - loss 0.05898189 - samples/sec: 155.50 - lr: 0.000195\n",
      "2022-01-13 10:12:46,928 epoch 116 - iter 22/22 - loss 0.05780254 - samples/sec: 168.15 - lr: 0.000195\n",
      "2022-01-13 10:12:46,928 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-13 10:12:46,943 EPOCH 116 done: loss 0.0578 - lr 0.0001953\n",
      "2022-01-13 10:12:48,193 DEV : loss 0.021358206868171692 - f1-score (micro avg)  0.9221\n",
      "2022-01-13 10:12:48,225 BAD EPOCHS (no improvement): 3\n",
      "2022-01-13 10:12:48,225 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-13 10:12:48,587 epoch 117 - iter 2/22 - loss 0.05863221 - samples/sec: 180.53 - lr: 0.000195\n",
      "2022-01-13 10:12:48,917 epoch 117 - iter 4/22 - loss 0.04958984 - samples/sec: 194.21 - lr: 0.000195\n",
      "2022-01-13 10:12:49,391 epoch 117 - iter 6/22 - loss 0.05839923 - samples/sec: 135.11 - lr: 0.000195\n",
      "2022-01-13 10:12:49,800 epoch 117 - iter 8/22 - loss 0.05873866 - samples/sec: 156.27 - lr: 0.000195\n",
      "2022-01-13 10:12:50,292 epoch 117 - iter 10/22 - loss 0.05988215 - samples/sec: 130.18 - lr: 0.000195\n",
      "2022-01-13 10:12:50,720 epoch 117 - iter 12/22 - loss 0.05697248 - samples/sec: 149.52 - lr: 0.000195\n",
      "2022-01-13 10:12:51,059 epoch 117 - iter 14/22 - loss 0.05818806 - samples/sec: 188.85 - lr: 0.000195\n",
      "2022-01-13 10:12:51,459 epoch 117 - iter 16/22 - loss 0.05837000 - samples/sec: 160.07 - lr: 0.000195\n",
      "2022-01-13 10:12:51,855 epoch 117 - iter 18/22 - loss 0.05811971 - samples/sec: 161.26 - lr: 0.000195\n",
      "2022-01-13 10:12:52,238 epoch 117 - iter 20/22 - loss 0.05741060 - samples/sec: 167.49 - lr: 0.000195\n",
      "2022-01-13 10:12:52,693 epoch 117 - iter 22/22 - loss 0.05629584 - samples/sec: 140.39 - lr: 0.000195\n",
      "2022-01-13 10:12:52,693 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-13 10:12:52,693 EPOCH 117 done: loss 0.0563 - lr 0.0001953\n",
      "2022-01-13 10:12:53,907 DEV : loss 0.02133604884147644 - f1-score (micro avg)  0.9221\n",
      "Epoch   117: reducing learning rate of group 0 to 9.7656e-05.\n",
      "2022-01-13 10:12:53,938 BAD EPOCHS (no improvement): 4\n",
      "2022-01-13 10:12:53,938 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-13 10:12:53,944 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-13 10:12:53,944 learning rate too small - quitting training!\n",
      "2022-01-13 10:12:53,945 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-13 10:12:57,339 ----------------------------------------------------------------------------------------------------\n",
      "2022-01-13 10:12:57,339 loading file resources\\taggers\\example-ner\\best-model.pt\n",
      "2022-01-13 10:13:01,420 0.9242\t0.9229\t0.9235\t0.9229\n",
      "2022-01-13 10:13:01,420 \n",
      "Results:\n",
      "- F-score (micro) 0.9235\n",
      "- F-score (macro) 0.9242\n",
      "- Accuracy 0.9229\n",
      "\n",
      "By class:\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "           EMAIL     0.8224    0.8800    0.8502       100\n",
      "    PHONE_NUMBER     0.8286    0.8700    0.8488       100\n",
      "CREDITCARDNUMBER     0.8942    0.9300    0.9118       100\n",
      "         ADDRESS     1.0000    1.0000    1.0000       100\n",
      "            NAME     1.0000    1.0000    1.0000       100\n",
      "          PLATES     0.9677    0.9000    0.9326       100\n",
      "             SSN     0.9778    0.8800    0.9263       100\n",
      "\n",
      "       micro avg     0.9242    0.9229    0.9235       700\n",
      "       macro avg     0.9273    0.9229    0.9242       700\n",
      "    weighted avg     0.9273    0.9229    0.9242       700\n",
      "     samples avg     0.9229    0.9229    0.9229       700\n",
      "\n",
      "2022-01-13 10:13:01,420 ----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'test_score': 0.9235167977126518,\n",
       " 'dev_score_history': [0.045416316232127836,\n",
       "  0.2702205882352941,\n",
       "  0.4471698113207547,\n",
       "  0.48785046728971965,\n",
       "  0.5537583254043769,\n",
       "  0.6123222748815166,\n",
       "  0.6539923954372624,\n",
       "  0.6438746438746439,\n",
       "  0.6666666666666667,\n",
       "  0.6615969581749049,\n",
       "  0.6418527708850289,\n",
       "  0.6666666666666667,\n",
       "  0.6454248366013072,\n",
       "  0.6469740634005764,\n",
       "  0.6754221388367728,\n",
       "  0.6717267552182162,\n",
       "  0.6717267552182162,\n",
       "  0.6740331491712708,\n",
       "  0.6736045411542101,\n",
       "  0.714501510574018,\n",
       "  0.6935483870967742,\n",
       "  0.6796875,\n",
       "  0.7148120854826824,\n",
       "  0.68801191362621,\n",
       "  0.6730627306273063,\n",
       "  0.7473524962178516,\n",
       "  0.7481259370314842,\n",
       "  0.7350180505415163,\n",
       "  0.7538126361655774,\n",
       "  0.7680115273775217,\n",
       "  0.7546897546897546,\n",
       "  0.7164612037708484,\n",
       "  0.7385057471264368,\n",
       "  0.7823571945047,\n",
       "  0.7994248741912294,\n",
       "  0.800578034682081,\n",
       "  0.7706821480406386,\n",
       "  0.768790264853257,\n",
       "  0.7943786982248521,\n",
       "  0.8065902578796561,\n",
       "  0.7494521548575602,\n",
       "  0.8287769784172663,\n",
       "  0.8058608058608059,\n",
       "  0.8111273792093704,\n",
       "  0.7753303964757708,\n",
       "  0.8448275862068966,\n",
       "  0.8028880866425994,\n",
       "  0.861671469740634,\n",
       "  0.8430107526881719,\n",
       "  0.8403722261989978,\n",
       "  0.8589835361488903,\n",
       "  0.8457142857142858,\n",
       "  0.8763402430307361,\n",
       "  0.8817204301075269,\n",
       "  0.8534667619728378,\n",
       "  0.8726752503576538,\n",
       "  0.8892065761258041,\n",
       "  0.8791994281629736,\n",
       "  0.8777698355968548,\n",
       "  0.886021505376344,\n",
       "  0.8933428775948461,\n",
       "  0.8798283261802575,\n",
       "  0.9033643521832498,\n",
       "  0.886184681460272,\n",
       "  0.8749106504646176,\n",
       "  0.8841201716738198,\n",
       "  0.8868194842406876,\n",
       "  0.9049320943531094,\n",
       "  0.8955650929899857,\n",
       "  0.9006433166547535,\n",
       "  0.9141630901287554,\n",
       "  0.9170243204577969,\n",
       "  0.8992137240886348,\n",
       "  0.9055793991416309,\n",
       "  0.9077912794853467,\n",
       "  0.9113018597997139,\n",
       "  0.9141630901287554,\n",
       "  0.9157142857142857,\n",
       "  0.910650464617584,\n",
       "  0.9149392423159399,\n",
       "  0.9163688348820586,\n",
       "  0.9149392423159399,\n",
       "  0.9149392423159399,\n",
       "  0.9198855507868383,\n",
       "  0.9184549356223176,\n",
       "  0.9163688348820586,\n",
       "  0.9177984274481773,\n",
       "  0.9177984274481773,\n",
       "  0.9192280200142959,\n",
       "  0.9177984274481773,\n",
       "  0.9177984274481773,\n",
       "  0.9206576125804147,\n",
       "  0.9206576125804147,\n",
       "  0.9206576125804147,\n",
       "  0.9220872051465332,\n",
       "  0.9206576125804147,\n",
       "  0.9235167977126518,\n",
       "  0.9206576125804147,\n",
       "  0.9192280200142959,\n",
       "  0.9206576125804147,\n",
       "  0.9206576125804147,\n",
       "  0.9206576125804147,\n",
       "  0.9206576125804147,\n",
       "  0.9206576125804147,\n",
       "  0.9235167977126518,\n",
       "  0.9220872051465332,\n",
       "  0.9220872051465332,\n",
       "  0.9220872051465332,\n",
       "  0.9220872051465332,\n",
       "  0.9220872051465332,\n",
       "  0.9220872051465332,\n",
       "  0.9220872051465332,\n",
       "  0.9220872051465332,\n",
       "  0.9220872051465332,\n",
       "  0.9220872051465332,\n",
       "  0.9220872051465332,\n",
       "  0.9220872051465332],\n",
       " 'train_loss_history': [0.997413876232887,\n",
       "  0.35299449074129563,\n",
       "  0.252294370515092,\n",
       "  0.19731435508098039,\n",
       "  0.17292337354775877,\n",
       "  0.16234288569790245,\n",
       "  0.1487416547188829,\n",
       "  0.14015438905014865,\n",
       "  0.12992961406062284,\n",
       "  0.129966444255277,\n",
       "  0.12512382911480396,\n",
       "  0.126264870785363,\n",
       "  0.12087225520259023,\n",
       "  0.11634734881590145,\n",
       "  0.11562623952702596,\n",
       "  0.11135710100115943,\n",
       "  0.1050483122509682,\n",
       "  0.10063696927450246,\n",
       "  0.10002954491810746,\n",
       "  0.09002040377292335,\n",
       "  0.09187057153615547,\n",
       "  0.08954522764840912,\n",
       "  0.08739783916433799,\n",
       "  0.08799018099058213,\n",
       "  0.09551348900087107,\n",
       "  0.08723999483363097,\n",
       "  0.08493870064816571,\n",
       "  0.08479469711734719,\n",
       "  0.08338261674795434,\n",
       "  0.07985806024071328,\n",
       "  0.08176620582544075,\n",
       "  0.0849801263119832,\n",
       "  0.08134971737764811,\n",
       "  0.08021214408313375,\n",
       "  0.07881497721434952,\n",
       "  0.07971263183560702,\n",
       "  0.08035722616701907,\n",
       "  0.07606282684736525,\n",
       "  0.07561787912619909,\n",
       "  0.07914024134769239,\n",
       "  0.0806902811508103,\n",
       "  0.07572931985585722,\n",
       "  0.07539323645478906,\n",
       "  0.07758627943953456,\n",
       "  0.07129262038252326,\n",
       "  0.07808052044398474,\n",
       "  0.07332434893923861,\n",
       "  0.07106100102536418,\n",
       "  0.07190763523255689,\n",
       "  0.07041224016737459,\n",
       "  0.06770592744509006,\n",
       "  0.07253620962558288,\n",
       "  0.06618833683875588,\n",
       "  0.06723046662112284,\n",
       "  0.06546695421086506,\n",
       "  0.06854344369616172,\n",
       "  0.06351602625065833,\n",
       "  0.06448926909307034,\n",
       "  0.06341301737553741,\n",
       "  0.061248639184810466,\n",
       "  0.06805895407829632,\n",
       "  0.062331987389071454,\n",
       "  0.06742256542543526,\n",
       "  0.06296808101480893,\n",
       "  0.05628567087920325,\n",
       "  0.061329839462171486,\n",
       "  0.06082641543727545,\n",
       "  0.06528404427934517,\n",
       "  0.061053759138804245,\n",
       "  0.06219753798632918,\n",
       "  0.06255225594772477,\n",
       "  0.05796879338998677,\n",
       "  0.06148056389676375,\n",
       "  0.05974188953430985,\n",
       "  0.058929282936224944,\n",
       "  0.06574580303846568,\n",
       "  0.059114988912433164,\n",
       "  0.05789326816762293,\n",
       "  0.05351358259986125,\n",
       "  0.05609075606337266,\n",
       "  0.058971705564511465,\n",
       "  0.05944340608361159,\n",
       "  0.055507823433560616,\n",
       "  0.056576486722989025,\n",
       "  0.05970581601365531,\n",
       "  0.05604474697934087,\n",
       "  0.05913046595960111,\n",
       "  0.05558979174535893,\n",
       "  0.05444428094747361,\n",
       "  0.05489878382948307,\n",
       "  0.058188212325605025,\n",
       "  0.06127209229198628,\n",
       "  0.05734919030015923,\n",
       "  0.05594980401496209,\n",
       "  0.058048473434234804,\n",
       "  0.06094451992426331,\n",
       "  0.0544615532880483,\n",
       "  0.05763528020187638,\n",
       "  0.05824095918193921,\n",
       "  0.059188935378217254,\n",
       "  0.05533640131430983,\n",
       "  0.057908622659766366,\n",
       "  0.0563364675270716,\n",
       "  0.05830563277654318,\n",
       "  0.05588664480112442,\n",
       "  0.06003887444000341,\n",
       "  0.061746955832336886,\n",
       "  0.05284397017492111,\n",
       "  0.055190030337133066,\n",
       "  0.05566990796156216,\n",
       "  0.06031885173774542,\n",
       "  0.05935955710299619,\n",
       "  0.053944977318893084,\n",
       "  0.056819122606381944,\n",
       "  0.057364568890200744,\n",
       "  0.05780254267966335,\n",
       "  0.05629583915942477],\n",
       " 'dev_loss_history': [tensor(0.4338),\n",
       "  tensor(0.2163),\n",
       "  tensor(0.1646),\n",
       "  tensor(0.1346),\n",
       "  tensor(0.1427),\n",
       "  tensor(0.1086),\n",
       "  tensor(0.1152),\n",
       "  tensor(0.1084),\n",
       "  tensor(0.0795),\n",
       "  tensor(0.1052),\n",
       "  tensor(0.0713),\n",
       "  tensor(0.0718),\n",
       "  tensor(0.0635),\n",
       "  tensor(0.0599),\n",
       "  tensor(0.0685),\n",
       "  tensor(0.0810),\n",
       "  tensor(0.0745),\n",
       "  tensor(0.0654),\n",
       "  tensor(0.0698),\n",
       "  tensor(0.0543),\n",
       "  tensor(0.0525),\n",
       "  tensor(0.0554),\n",
       "  tensor(0.0524),\n",
       "  tensor(0.0521),\n",
       "  tensor(0.0517),\n",
       "  tensor(0.0522),\n",
       "  tensor(0.0507),\n",
       "  tensor(0.0485),\n",
       "  tensor(0.0470),\n",
       "  tensor(0.0459),\n",
       "  tensor(0.0474),\n",
       "  tensor(0.0465),\n",
       "  tensor(0.0459),\n",
       "  tensor(0.0449),\n",
       "  tensor(0.0430),\n",
       "  tensor(0.0423),\n",
       "  tensor(0.0440),\n",
       "  tensor(0.0403),\n",
       "  tensor(0.0431),\n",
       "  tensor(0.0397),\n",
       "  tensor(0.0432),\n",
       "  tensor(0.0384),\n",
       "  tensor(0.0403),\n",
       "  tensor(0.0392),\n",
       "  tensor(0.0405),\n",
       "  tensor(0.0364),\n",
       "  tensor(0.0375),\n",
       "  tensor(0.0343),\n",
       "  tensor(0.0339),\n",
       "  tensor(0.0333),\n",
       "  tensor(0.0324),\n",
       "  tensor(0.0322),\n",
       "  tensor(0.0306),\n",
       "  tensor(0.0307),\n",
       "  tensor(0.0306),\n",
       "  tensor(0.0300),\n",
       "  tensor(0.0291),\n",
       "  tensor(0.0291),\n",
       "  tensor(0.0276),\n",
       "  tensor(0.0275),\n",
       "  tensor(0.0274),\n",
       "  tensor(0.0276),\n",
       "  tensor(0.0264),\n",
       "  tensor(0.0268),\n",
       "  tensor(0.0263),\n",
       "  tensor(0.0258),\n",
       "  tensor(0.0261),\n",
       "  tensor(0.0243),\n",
       "  tensor(0.0247),\n",
       "  tensor(0.0242),\n",
       "  tensor(0.0238),\n",
       "  tensor(0.0240),\n",
       "  tensor(0.0239),\n",
       "  tensor(0.0234),\n",
       "  tensor(0.0228),\n",
       "  tensor(0.0231),\n",
       "  tensor(0.0231),\n",
       "  tensor(0.0223),\n",
       "  tensor(0.0227),\n",
       "  tensor(0.0224),\n",
       "  tensor(0.0222),\n",
       "  tensor(0.0222),\n",
       "  tensor(0.0221),\n",
       "  tensor(0.0222),\n",
       "  tensor(0.0221),\n",
       "  tensor(0.0219),\n",
       "  tensor(0.0220),\n",
       "  tensor(0.0219),\n",
       "  tensor(0.0219),\n",
       "  tensor(0.0217),\n",
       "  tensor(0.0218),\n",
       "  tensor(0.0217),\n",
       "  tensor(0.0216),\n",
       "  tensor(0.0216),\n",
       "  tensor(0.0215),\n",
       "  tensor(0.0214),\n",
       "  tensor(0.0215),\n",
       "  tensor(0.0217),\n",
       "  tensor(0.0218),\n",
       "  tensor(0.0217),\n",
       "  tensor(0.0215),\n",
       "  tensor(0.0215),\n",
       "  tensor(0.0215),\n",
       "  tensor(0.0215),\n",
       "  tensor(0.0215),\n",
       "  tensor(0.0214),\n",
       "  tensor(0.0214),\n",
       "  tensor(0.0213),\n",
       "  tensor(0.0213),\n",
       "  tensor(0.0213),\n",
       "  tensor(0.0214),\n",
       "  tensor(0.0213),\n",
       "  tensor(0.0214),\n",
       "  tensor(0.0214),\n",
       "  tensor(0.0214),\n",
       "  tensor(0.0214),\n",
       "  tensor(0.0213)]}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from flair.trainers import ModelTrainer\n",
    "trainer : ModelTrainer = ModelTrainer(tagger, corpus)\n",
    "trainer.train('resources/taggers/example-ner',\n",
    "              learning_rate=0.1,\n",
    "              mini_batch_size=32,\n",
    "              max_epochs=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a51e9a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3225047c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-01-13 10:34:42,082 loading file resources/taggers/example-ner/best-model.pt\n",
      "Candidate economic character present money daughter Apt <B-ADDRESS> . <I-ADDRESS> 026 <I-ADDRESS>\n"
     ]
    }
   ],
   "source": [
    "from flair.data import Sentence\n",
    "from flair.models import SequenceTagger\n",
    "from flair.trainers import ModelTrainer\n",
    "# load the trained model\n",
    "model = SequenceTagger.load('resources/taggers/example-ner/best-model.pt')\n",
    "#'resources/taggers/example-pos/final-model.pt'\n",
    "# create example sentence\n",
    "sentence = Sentence('Candidate economic character present money daughter Apt. 026')\n",
    "# predict the tags\n",
    "model.predict(sentence)\n",
    "print(sentence.to_tagged_string('ner'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "159ec556",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
